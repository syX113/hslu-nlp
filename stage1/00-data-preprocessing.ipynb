{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 13:37:47.958880: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-27 13:37:48.024038: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-27 13:37:48.691278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-27 13:37:49.502314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-27 13:37:49.503565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-27 13:37:49.503594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/tim/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to /home/tim/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/tim/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/tim/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/tim/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tim/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package words to /home/tim/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd  # library for data manipulation and analysis\n",
    "import re  # library for regular expressions\n",
    "import os  # library for operating system dependent functionality\n",
    "import string  # library for string operations\n",
    "import nltk  # library for natural language processing\n",
    "import spacy  # library for advanced natural language processing\n",
    "import requests  # library for making HTTP requests\n",
    "import contractions  # library for expanding contractions\n",
    "from langdetect import detect  # library for language detection\n",
    "from nltk.corpus import stopwords  # library for stop words\n",
    "from nltk.stem import WordNetLemmatizer  # library for lemmatizing words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # libraries for tokenizing text\n",
    "from unidecode import unidecode  # library for converting accented characters\n",
    "from bs4 import BeautifulSoup  # library for parsing HTML and XML documents\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # library for sentiment analysis\n",
    "nltk.download('vader_lexicon')  # download the VADER sentiment analysis lexicon\n",
    "nltk.download('wordnet')  # download WordNet for lemmatization\n",
    "nltk.download('stopwords')  # download stopwords for text preprocessing\n",
    "nltk.download('punkt')  # download the Punkt tokenizer for sentence segmentation\n",
    "nltk.download('maxent_ne_chunker')  # download the maximum entropy chunker for named entity recognition\n",
    "nltk.download('averaged_perceptron_tagger')  # download the averaged perceptron tagger for part-of-speech tagging\n",
    "nltk.download('words')  # download the NLTK corpus of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a spacy model, can also be adjusted (medium = en_core_web_sm, large = en_core_web_lg)\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = '../data/esg_documents_for_dax_companies.csv'\n",
    "\n",
    "# Check if the directory and file exist\n",
    "if os.path.exists(file_path):\n",
    "    # Read the dat\n",
    "    raw_data = pd.read_csv(file_path, delimiter='|', index_col=0)\n",
    "else:\n",
    "    print(f'The file {file_path} does not exist, please download the file and store it in \"/data\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beiersdorf AG</td>\n",
       "      <td>Sustainability Highlight Report CARE BEYOND SK...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'GHGEmission', 'ProductLiabilit...</td>\n",
       "      <td>1</td>\n",
       "      <td>BEI</td>\n",
       "      <td>BeiersdorfAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>Corporate Responsibility Report 2021 2 Content...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DTE</td>\n",
       "      <td>DeutscheTelekomAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vonovia SE</td>\n",
       "      <td>VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'DataSecurity', 'Vaccine', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>VNA</td>\n",
       "      <td>VonoviaSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merck KGaA</td>\n",
       "      <td>Sustainability Report 2021 TABLE OF CONTENTS S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'DataMisuse', 'DrugResistance...</td>\n",
       "      <td>1</td>\n",
       "      <td>MRK</td>\n",
       "      <td>MerckKGaA Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTU</td>\n",
       "      <td>Our ideas and concepts FOR A SUSTAINABLE FUTUR...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'AirQuality'...</td>\n",
       "      <td>1</td>\n",
       "      <td>MTX</td>\n",
       "      <td>MTUAeroEngines Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>#StandWithUkraine Sustainability Report 2021 C...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>E.ONSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RWE AG</td>\n",
       "      <td>Focus on tomorrow. Sustainability Report 2021 ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'Iso50001', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>RWE</td>\n",
       "      <td>RWEAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Annual Report 2021 HeidelbergCement at a glanc...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Vaccine', 'DataSecurity',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Annual Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Company Strategy &amp; Business &amp; Product &amp; Produc...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'Corruption', 'Whistleblowing',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>Sustainability 1 Siemens 2 Our 3 Governance – ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'EmployeeTurnover...</td>\n",
       "      <td>1</td>\n",
       "      <td>SIE</td>\n",
       "      <td>SiemensAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company                                            content  \\\n",
       "0         Beiersdorf AG  Sustainability Highlight Report CARE BEYOND SK...   \n",
       "1   Deutsche Telekom AG  Corporate Responsibility Report 2021 2 Content...   \n",
       "2            Vonovia SE  VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...   \n",
       "3            Merck KGaA  Sustainability Report 2021 TABLE OF CONTENTS S...   \n",
       "4                   MTU  Our ideas and concepts FOR A SUSTAINABLE FUTUR...   \n",
       "5                E ONSE  #StandWithUkraine Sustainability Report 2021 C...   \n",
       "6                RWE AG  Focus on tomorrow. Sustainability Report 2021 ...   \n",
       "7  Heidelberg Cement AG  Annual Report 2021 HeidelbergCement at a glanc...   \n",
       "8  Heidelberg Cement AG  Company Strategy & Business & Product & Produc...   \n",
       "9            Siemens AG  Sustainability 1 Siemens 2 Our 3 Governance – ...   \n",
       "\n",
       "                datatype        date domain  \\\n",
       "0  sustainability_report  2021-03-31    NaN   \n",
       "1  sustainability_report  2021-03-31    NaN   \n",
       "2  sustainability_report  2021-03-31    NaN   \n",
       "3  sustainability_report  2021-03-31    NaN   \n",
       "4  sustainability_report  2020-03-31    NaN   \n",
       "5  sustainability_report  2021-03-31    NaN   \n",
       "6  sustainability_report  2021-03-31    NaN   \n",
       "7          annual_report  2021-03-31    NaN   \n",
       "8  sustainability_report  2020-03-31    NaN   \n",
       "9  sustainability_report  2020-03-31    NaN   \n",
       "\n",
       "                                          esg_topics  internal symbol  \\\n",
       "0  ['CleanWater', 'GHGEmission', 'ProductLiabilit...         1    BEI   \n",
       "1  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1    DTE   \n",
       "2  ['Whistleblowing', 'DataSecurity', 'Vaccine', ...         1    VNA   \n",
       "3  ['DataSecurity', 'DataMisuse', 'DrugResistance...         1    MRK   \n",
       "4  ['WorkLifeBalance', 'Corruption', 'AirQuality'...         1    MTX   \n",
       "5  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1   EOAN   \n",
       "6  ['WorkLifeBalance', 'Corruption', 'Iso50001', ...         1    RWE   \n",
       "7  ['WorkLifeBalance', 'Vaccine', 'DataSecurity',...         1    HEI   \n",
       "8  ['CleanWater', 'Corruption', 'Whistleblowing',...         1    HEI   \n",
       "9  ['DataSecurity', 'Iso50001', 'EmployeeTurnover...         1    SIE   \n",
       "\n",
       "                                           title  url  \n",
       "0        BeiersdorfAG Sustainability Report 2021  NaN  \n",
       "1   DeutscheTelekomAG Sustainability Report 2021  NaN  \n",
       "2           VonoviaSE Sustainability Report 2021  NaN  \n",
       "3           MerckKGaA Sustainability Report 2021  NaN  \n",
       "4      MTUAeroEngines Sustainability Report 2020  NaN  \n",
       "5              E.ONSE Sustainability Report 2021  NaN  \n",
       "6               RWEAG Sustainability Report 2021  NaN  \n",
       "7          HeidelbergCementAG Annual Report 2021  NaN  \n",
       "8  HeidelbergCementAG Sustainability Report 2020  NaN  \n",
       "9           SiemensAG Sustainability Report 2020  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded data and reset index\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column descriptions**\n",
    "- symbol: stock symbol of the company\n",
    "- company: company name\n",
    "- date: publication date of document\n",
    "- title: document title\n",
    "- content: document content\n",
    "- datatype: document type\n",
    "- internal: is this a report by company (1) or a third-party document (0)\n",
    "- domain (optional): Web domain where the document was published\n",
    "- url (optional): URL where the document can be accessed\n",
    "- esg_topics (optional): ESG topics extracted from the data using our internal NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11188, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape (row and column amount)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company       object\n",
       "content       object\n",
       "datatype      object\n",
       "date          object\n",
       "domain        object\n",
       "esg_topics    object\n",
       "internal       int64\n",
       "symbol        object\n",
       "title         object\n",
       "url           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "raw_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to save intermediary steps in a file\n",
    "\n",
    "def csv_checkpoint(df, filename='checkpoint'):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a CSV file and loads it back into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to save and load.\n",
    "        filename (str): The name of the CSV file to save the DataFrame to (default: 'checkpoint').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../data/checkpoints/'):  # Check if the directory exists and create it if it doesn't\n",
    "        os.makedirs('../data/checkpoints/')\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(f'../data/checkpoints/{filename}.csv', index=False, sep='|')  # Save DataFrame to CSV with specified filename\n",
    "    print(f'Saved DataFrame to {filename}.csv')\n",
    "\n",
    "    # Load CSV back into DataFrame\n",
    "    df = pd.read_csv(f'../data/checkpoints/{filename}.csv', delimiter='|')  # Load CSV back into DataFrame\n",
    "    print(f'Loaded DataFrame from {filename}.csv')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As initial data cleaning steps, the following is conducted:\n",
    "- Rows with missing \"content\" were dropped to prevent any missing data-related issues. Missing data can create gaps in the data and lead to errors or distortions in the analysis.\n",
    "- The \"URL\" column was removed as the relevant information was available in the \"domain\" column. Removing redundant columns simplifies the data set and makes it easier to work with\n",
    "- Duplicate entries were identified and removed, resulting in a cleaner and more concise dataset. Duplicates can distort the data and lead to biased analysis. \n",
    "- Language checking was conducted and all rows with non-English content were dropped to ensure consistent language. Language inconsistencies can create bias in the data and lead to inaccurate conclusions. Therefore, it is important to ensure that the data is consistent in language to prevent linguistic biases.\n",
    "- \"Date\" is formatted as a date and wrong dates, e.g. \"bayer-03-31\" are replaced with a default date (2023-03-31).\n",
    "- Remove company name parts like \"AG\" for clarity\n",
    "- The \"sample\" method was used to check the data for representativeness and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_cleaned_data = raw_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"url\" column, since the most relevant information from an analysis perspective is already in the \"domain\" column (e.g. the source of the report)\n",
    "general_cleaned_data = general_cleaned_data.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 6\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and delete them\n",
    "duplicates = general_cleaned_data[general_cleaned_data.duplicated()]\n",
    "print(f'Duplicated rows: {len(duplicates)}')\n",
    "general_cleaned_data = general_cleaned_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted amount of rows with language other ehan English: 105\n"
     ]
    }
   ],
   "source": [
    "# Check for other languange than English\n",
    "general_cleaned_data['language'] = general_cleaned_data['content'].apply(lambda x: detect(x))\n",
    "not_english = len(general_cleaned_data) - len(general_cleaned_data.loc[general_cleaned_data['language'] == 'en'])\n",
    "\n",
    "# Drop rows with other languange, since other languanges influences to quality of the later analysis\n",
    "general_cleaned_data = general_cleaned_data.loc[general_cleaned_data['language'] == 'en']\n",
    "\n",
    "print(f'Deleted amount of rows with language other ehan English: {not_english}')\n",
    "general_cleaned_data.drop(['language'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly formatted dates:\n",
      "Row 13: p.DE-03-31\n",
      "Row 18: p.DE-03-31\n",
      "Row 20: bayer-03-31\n",
      "Row 22: p.DE-03-31\n",
      "Row 25: p.DE-03-31\n",
      "Row 26: p.DE-03-31\n",
      "Row 31: p.DE-03-31\n",
      "Row 32: p.DE-03-31\n",
      "Row 33: p.DE-03-31\n",
      "Row 37: p.DE-03-31\n",
      "Row 41: p.DE-03-31\n",
      "Row 50: p.DE-03-31\n",
      "Row 78: p.DE-03-31\n",
      "Row 80: p.DE-03-31\n",
      "Row 86: p.DE-03-31\n",
      "Row 87: p.DE-03-31\n",
      "Row 88: p.DE-03-31\n"
     ]
    }
   ],
   "source": [
    "# Correct the dates to ISO standard\n",
    "def find_incorrect_dates(data):\n",
    "    incorrect_dates = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            pd.to_datetime(row['date'], format='%Y-%m-%d', errors='raise')\n",
    "        except ValueError:\n",
    "            incorrect_dates.append((index, row['date']))\n",
    "\n",
    "    return incorrect_dates\n",
    "\n",
    "incorrect_date_rows = find_incorrect_dates(general_cleaned_data)\n",
    "print(\"Incorrectly formatted dates:\")\n",
    "for index, date in incorrect_date_rows:\n",
    "    print(f\"Row {index}: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the wrong formatted dates and set default date\n",
    "def correct_date_format(data):\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce').fillna('2022-03-31')\n",
    "    return data\n",
    "\n",
    "general_cleaned_data = correct_date_format(general_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace company name parts like \"AG\" to have a cleaner name\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' AG', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' SE', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' KGaA', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9345</th>\n",
       "      <td>Siemens Energy</td>\n",
       "      <td>SIEMENS GAMESA RENEWABLE ENERGY, S.A. OTHER RE...</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-01-24</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['RenewableEnergy']</td>\n",
       "      <td>0</td>\n",
       "      <td>ENR</td>\n",
       "      <td>Siemens Gamesa Renewable Energy S A: announces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>Merck</td>\n",
       "      <td>Sign up for our monthly newsletter R4D Insight...</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-01-12</td>\n",
       "      <td>r4d</td>\n",
       "      <td>['Social', 'GenderDiversity']</td>\n",
       "      <td>0</td>\n",
       "      <td>MRK</td>\n",
       "      <td>R4D Announces New Project to Strengthen Mixed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>Any discussion about disruption to traditional...</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>euromoney</td>\n",
       "      <td>['Environment']</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>Efficiency and sustainability top the post-pan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>Innovative technology enabling electric vehicl...</td>\n",
       "      <td>esg</td>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>businessgreen</td>\n",
       "      <td>['RenewableEnergy', 'CarbonDioxide', 'EMobility']</td>\n",
       "      <td>0</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>Back two-way EV charging technology to slash c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Hi, what are you looking for? By Published Thi...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>digitaljournal</td>\n",
       "      <td>['IslamicState']</td>\n",
       "      <td>0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>Jihadist attack kills 13 in northern Cameroon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             company                                            content  \\\n",
       "9345  Siemens Energy  SIEMENS GAMESA RENEWABLE ENERGY, S.A. OTHER RE...   \n",
       "6692           Merck  Sign up for our monthly newsletter R4D Insight...   \n",
       "4054   Deutsche Bank  Any discussion about disruption to traditional...   \n",
       "5417          E ONSE  Innovative technology enabling electric vehicl...   \n",
       "1879             BMW  Hi, what are you looking for? By Published Thi...   \n",
       "\n",
       "      datatype       date          domain  \\\n",
       "9345  business 2021-01-24  marketscreener   \n",
       "6692   general 2021-01-12             r4d   \n",
       "4054   general 2021-11-04       euromoney   \n",
       "5417       esg 2021-01-08   businessgreen   \n",
       "1879      tech 2021-01-08  digitaljournal   \n",
       "\n",
       "                                             esg_topics  internal symbol  \\\n",
       "9345                                ['RenewableEnergy']         0    ENR   \n",
       "6692                      ['Social', 'GenderDiversity']         0    MRK   \n",
       "4054                                    ['Environment']         0    DBK   \n",
       "5417  ['RenewableEnergy', 'CarbonDioxide', 'EMobility']         0   EOAN   \n",
       "1879                                   ['IslamicState']         0    BMW   \n",
       "\n",
       "                                                  title  \n",
       "9345  Siemens Gamesa Renewable Energy S A: announces...  \n",
       "6692  R4D Announces New Project to Strengthen Mixed ...  \n",
       "4054  Efficiency and sustainability top the post-pan...  \n",
       "5417  Back two-way EV charging technology to slash c...  \n",
       "1879      Jihadist attack kills 13 in northern Cameroon  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the date with some samples\n",
    "general_cleaned_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of \"Muenchener Rueckversicherungs Gesellschaft AGin Muenchen\" to something more readable\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].replace('Muenchener Rueckversicherungs Gesellschaftin Muenchen', 'Munich R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to general_cleaned_data.csv\n",
      "Loaded DataFrame from general_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "general_cleaned_data = csv_checkpoint(general_cleaned_data, 'general_cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning & Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"content\" column, containing the text of the reports, undergoes a series of cleaning, normalization, and preprocessing steps to ensure accurate and efficient analysis. These steps include:\n",
    "\n",
    "- **String conversion**: Converting the input to a string format ensures consistency and compatibility during subsequent processing tasks.\n",
    "- **Lowercase conversion**: Transforming all text to lowercase serves as a simple normalization step, reducing the complexity and variability of the input data.\n",
    "- **Unicode decoding**: Removing diacritics (e.g., accented characters) and normalizing the text encoding mitigates potential discrepancies arising from different encoding formats.\n",
    "- **URL and email address removal**: Eliminating URLs and email addresses reduces noise in the dataset, as these elements do not contribute valuable information for the analysis.\n",
    "- **Extra whitespace removal**: Eradicating extra whitespaces improves text analysis and tokenization by ensuring that only meaningful spaces are retained.\n",
    "- **Contact detail removal**: Excluding phone numbers, contact person strings, and social media references further minimizes noise in the dataset, honing the focus on relevant text.\n",
    "- **Table of contents removal**: Discarding the table of contents enhances the data quality by eliminating repetitive and non-essential information.\n",
    "- **Named entity removal**: Employing the spaCy model to remove human names and other named entities optimizes the text for analysis and modeling by concentrating on pertinent content.\n",
    "- **Abbreviation expansion**: Utilizing the contractions library and custom functions with regular expressions, common and uncommon abbreviations are expanded to improve text interpretation.\n",
    "- **Special character elimination**: Excluding all special characters, except punctuation, refines the input data. Retaining punctuation is necessary for accurate sentence tokenization and removed after sentence tokenization..\n",
    "- **Tokenization and lemmatization**: Tokenizing words and sentences, and subsequently lemmatizing words using the WordNetLemmatizer from nltk, streamlines the text and reduces morphological variations.\n",
    "- **Stopword removal**: Customizing the nltk stopwords list by adding or removing specific stopwords enables more precise and tailored text analysis.\n",
    "- **Part-of-speech (POS) tagging**: Assigning POS tags to words and sentences enhances the text representation by providing additional linguistic information, which may be beneficial for subsequent analysis and modeling tasks.\n",
    "- **Sentiment Analysis**: Basic sentiment value calculation on the tokenzued sentences to get first insights im terms of the sentiments in the reports within the EDA.\n",
    "\n",
    "Spellchecking was tested with TextBlob and PySpellChecker but deliverd not useful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = general_cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name entities removed: 94756\n"
     ]
    }
   ],
   "source": [
    "# Since the spacy model shows better results on the \"raw\" text, the named entity removal is conducted before all normalization and cleaning steps\n",
    "spacy_model = spacy.load('en_core_web_md')\n",
    "spacy_model.max_length = 1800000 # Increase max text length\n",
    "\n",
    "def remove_named_entities(text):\n",
    "    \"\"\"\n",
    "    Removes named entities from text and returns the modified text and the count of named entities removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to remove named entities from.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified text (str) and the count of named entities removed (int).\n",
    "    \"\"\"\n",
    "    doc = spacy_model(text)\n",
    "    \n",
    "    named_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\"]:\n",
    "            named_entities.add(ent.text)\n",
    "    \n",
    "    named_entities_count = len(named_entities)\n",
    "    \n",
    "    for named_entity in named_entities:\n",
    "        text = text.replace(named_entity, '')\n",
    "    \n",
    "    return text, named_entities_count\n",
    "\n",
    "# Assuming cleaned_data is a pandas DataFrame with a 'content' column\n",
    "cleaned_data['cleaned_content'], name_entity_count = zip(*cleaned_data['content'].apply(remove_named_entities))\n",
    "print(\"Name entities removed:\", sum(name_entity_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs removed: 7492\n",
      "Mail addresses removed: 435\n",
      "Extra whitespaces removed: 149006\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    urls = re.findall(r'http\\S+|www\\S+|https\\S+', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE), len(urls)\n",
    "\n",
    "def remove_emails(text):\n",
    "    mail_addresses = re.findall(r'\\S+@\\S+\\s?', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'\\S+@\\S+\\s?', '', text, flags=re.MULTILINE), len(mail_addresses)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    extra_spaces = re.findall(r'\\s{2,}', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip(), len(extra_spaces)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].astype(str) # Convert all texts to string\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: x.lower()) # Convert all texts to lower-case\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: unidecode(x, errors=\"preserve\")) # Remove diacritics / accented characters and unicode normalization\n",
    "cleaned_data['cleaned_content'], url_count = zip(*cleaned_data['cleaned_content'].apply(remove_urls)) # Remove URLs from texts\n",
    "cleaned_data['cleaned_content'], email_count = zip(*cleaned_data['cleaned_content'].apply(remove_emails)) # Remove e-mail addresses from texts\n",
    "cleaned_data['cleaned_content'], extra_space_count = zip(*cleaned_data['cleaned_content'].apply(remove_extra_whitespace)) # Remove extra whitespaces from texts\n",
    "\n",
    "print(\"URLs removed:\", sum(url_count))\n",
    "print(\"Mail addresses removed:\", sum(email_count))\n",
    "print(\"Extra whitespaces removed:\", sum(extra_space_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact information removed: 44244\n",
      "TOCs removed: 9609\n"
     ]
    }
   ],
   "source": [
    "def remove_contact_details(text):\n",
    "    # Remove phone numbers\n",
    "    phone_regex = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
    "    phone_count = len(re.findall(phone_regex, text))\n",
    "    text = re.sub(phone_regex, '', text)\n",
    "\n",
    "    # Remove common contact-related phrases\n",
    "    contact_phrases_regex = r'\\b(?:Contact Person|Phone|Tel|Fax|Mobile|E?mail|Skype|Twitter|Facebook|LinkedIn|Website):\\b'\n",
    "    contact_phrases_count = len(re.findall(contact_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(contact_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    total_count = phone_count + contact_phrases_count\n",
    "    return text, total_count\n",
    "\n",
    "def remove_table_of_contents(text):\n",
    "    # Remove common table of contents phrases\n",
    "    toc_phrases_regex = r'\\b(?:Table of Contents|Contents)\\b'\n",
    "    toc_phrases_count = len(re.findall(toc_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(toc_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove content with numbering like \"1. Introduction\", \"1.1. Background\", \"A. Overview\", etc.\n",
    "    toc_entries_regex = r'(^|\\n)\\s*\\w+(\\.\\w+)*\\s+\\w+([\\w\\s]+)?'\n",
    "    toc_entries_count = len(re.findall(toc_entries_regex, text))\n",
    "    text = re.sub(toc_entries_regex, '', text)\n",
    "\n",
    "    total_count = toc_phrases_count + toc_entries_count\n",
    "    return text, total_count\n",
    "\n",
    "cleaned_data['cleaned_content'], contact_count = zip(*cleaned_data['cleaned_content'].apply(remove_contact_details))\n",
    "cleaned_data['cleaned_content'], toc_count = zip(*cleaned_data['cleaned_content'].apply(remove_table_of_contents))\n",
    "print(\"Contact information removed:\", sum(contact_count))\n",
    "print(\"TOCs removed:\", sum(toc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = []\n",
    "    for word in text.split():\n",
    "        expanded_text.append(contractions.fix(word))\n",
    "    expanded_text = ' '.join(expanded_text)\n",
    "    return contractions.fix(expanded_text)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded custom abbreviations: 0\n"
     ]
    }
   ],
   "source": [
    "# Expand custom abbreviations which are not captured by \"contractions\"\n",
    "# Basic idea from: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# Compile the regular expressions only once for efficiency\n",
    "specific_patterns = [\n",
    "    (re.compile(r\"won['’]t\"), \"will not\"),\n",
    "    (re.compile(r\"can['’]t\"), \"can not\"),\n",
    "]\n",
    "\n",
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Expands contractions in a given phrase and returns the modified phrase and the count of contractions expanded.\n",
    "\n",
    "    Args:\n",
    "        phrase (str): The phrase to expand contractions in.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified phrase (str) and the count of contractions expanded (int).\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    # Replace specific patterns\n",
    "    for pattern, replacement in specific_patterns:\n",
    "        matches = len(pattern.findall(phrase))\n",
    "        count += matches\n",
    "        phrase = pattern.sub(replacement, phrase)\n",
    "\n",
    "    return phrase, count\n",
    "\n",
    "# Apply the function to expand abbreviations\n",
    "cleaned_data['cleaned_content'], abbreviation_counts = zip(*cleaned_data['cleaned_content'].apply(decontracted))\n",
    "print(\"Expanded custom abbreviations:\", sum(abbreviation_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters excl. punctuation removed: 1388815\n"
     ]
    }
   ],
   "source": [
    "# Remove special characters excl. punctuation since this is needed by the sentence tokenization\n",
    "def remove_non_alphanumeric(text, remove_punctuation=False):\n",
    "    if remove_punctuation:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s.,!?\\'\"]'\n",
    "    \n",
    "    special_chars = re.findall(pattern, text)\n",
    "    return re.sub(pattern, '', text), len(special_chars)\n",
    "\n",
    "cleaned_data['cleaned_content'], special_char_count = zip(*cleaned_data['cleaned_content'].apply(remove_non_alphanumeric, remove_punctuation=False))\n",
    "print(\"Special characters excl. punctuation removed:\", sum(special_char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word token amount: 16968105\n"
     ]
    }
   ],
   "source": [
    "def tokenize_words(text):\n",
    "    # Remove numbers, digits, and punctuation\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['word_tokens'], word_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_words))\n",
    "print(\"Generated word token amount:\", sum(word_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence token amount: 687961\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(text):\n",
    "    # Tokenize sentences\n",
    "    tokens = sent_tokenize(text)\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['sentence_tokens'], sentence_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_sentences))\n",
    "print(\"Generated sentence token amount:\", sum(sentence_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stopwords in word tokens 6853068\n",
      "Removed stopwords in sentence tokens 9663203\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_word_tokens(tokens, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Removes stopwords and one-character tokens from a list of word tokens and returns the modified list and the count of removed items.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): The list of word tokens to remove stopwords from.\n",
    "        custom_stopwords (list): A list of custom stopwords to remove from the word tokens.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified list of word tokens (list) and the count of removed items (int).\n",
    "    \"\"\"\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens\n",
    "        if token.lower() not in custom_stopwords and len(token) > 1\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens, len(tokens) - len(filtered_tokens)\n",
    "\n",
    "def remove_stopwords_from_sentence_tokens(sentences_list, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Removes stopwords, one-character tokens, digits, numbers, and special characters (excluding whitespace) from a list of sentence tokens and returns the modified list and the count of removed items.\n",
    "\n",
    "    Args:\n",
    "        sentences_list (list): The list of sentence tokens to remove stopwords from.\n",
    "        custom_stopwords (list): A list of custom stopwords to remove from the sentence tokens.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified list of sentence tokens (list) and the count of removed items (int).\n",
    "    \"\"\"\n",
    "    filtered_sentences_list = []\n",
    "    total_removed_items_count = 0\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        # Tokenize the sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords, one-character tokens, digits, numbers, and special characters (excluding whitespace) from word tokens\n",
    "        filtered_word_tokens = [\n",
    "            re.sub(rf\"[{re.escape(string.punctuation)}]\", '', token) for token in word_tokens\n",
    "            if token.lower() not in custom_stopwords\n",
    "            and len(token) > 1\n",
    "            and not re.search(r'\\d', token)\n",
    "            and not re.search(r'\\W', token)\n",
    "        ]\n",
    "\n",
    "        # Reconstruct the sentence without the removed words and special characters\n",
    "        filtered_sentence = ' '.join(filtered_word_tokens)\n",
    "        removed_items_count = len(word_tokens) - len(filtered_word_tokens)\n",
    "        filtered_sentences_list.append(filtered_sentence)\n",
    "        total_removed_items_count += removed_items_count\n",
    "\n",
    "    return filtered_sentences_list, total_removed_items_count\n",
    "\n",
    "# Define custom stopwords to add or remove (the extra stopwords were identified by the TFIDF based wordcloud)\n",
    "custom_stopwords = {\n",
    "    'add': ['said','company','companies','year','billion','million','siemens','linde','rwe','volkswagen','symrise','porsche','sap','adidas','puma','airbus','bmw','hannover','mtu','heiderbergcement','qiagen','benz','continental','bayer','fresenius','wa', 'ha', 'eur', 'allianz', 'board'],\n",
    "    'remove': [''] # Currently not needed\n",
    "}\n",
    "\n",
    "# Combine stopwords to filter the content of the reports\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "all_stopwords |= set(custom_stopwords['add'])\n",
    "all_stopwords -= set(custom_stopwords['remove'])\n",
    "\n",
    "cleaned_data['word_tokens'], stopword_count_words = zip(*cleaned_data['word_tokens'].apply(remove_stopwords_from_word_tokens, custom_stopwords=all_stopwords))\n",
    "cleaned_data['sentence_tokens'], stopword_count_sentences = zip(*cleaned_data['sentence_tokens'].apply(remove_stopwords_from_sentence_tokens, custom_stopwords=all_stopwords))\n",
    "\n",
    "print(\"Removed stopwords in word tokens\", sum(stopword_count_words))\n",
    "print(\"Removed stopwords in sentence tokens\", sum(stopword_count_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the cleaned content based on the cleaned word tokens\n",
    "cleaned_data['cleaned_content'] = cleaned_data['word_tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_tokens(word_tokens, sentence_list):\n",
    "    \"\"\"\n",
    "    Performs POS tagging on a given list of word tokens and a list of sentence tokens and returns the POS tagged word tokens and POS tagged sentence tokens.\n",
    "\n",
    "    Args:\n",
    "        word_tokens (list): The list of word tokens to perform POS tagging on.\n",
    "        sentence_list (list): The list of sentence tokens to perform POS tagging on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the POS tagged word tokens (list) and the POS tagged sentence tokens (list of lists).\n",
    "    \"\"\"\n",
    "    \n",
    "    # POS tagging for word tokens\n",
    "    pos_tagged_word_tokens = nltk.pos_tag(word_tokens)\n",
    "\n",
    "    # Create a dictionary to map word tokens to their POS tags, this reduces the effort to call nltk.pos_tag twice\n",
    "    pos_tags_dict = dict(pos_tagged_word_tokens)\n",
    "\n",
    "    # POS tagging for sentence tokens\n",
    "    pos_tagged_sentence_list = []\n",
    "    for sentence in sentence_list:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = [(token, pos_tags_dict[token]) for token in tokenized_sentence if token in pos_tags_dict]\n",
    "        pos_tagged_sentence_list.append(pos_tagged_sentence)\n",
    "\n",
    "    return pos_tagged_word_tokens, pos_tagged_sentence_list\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = cleaned_data.apply(lambda row: pos_tagging_tokens(row['word_tokens'], row['sentence_tokens']), axis=1)\n",
    "cleaned_data['pos_tagged_word_tokens'], cleaned_data['pos_tagged_sentence_tokens'] = zip(*pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint file\n",
    "cleaned_data = csv_checkpoint(cleaned_data, 'cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several additional information could be helpful in the further analysis, which are not included in the dataset. Therefore a small scraper is used to enrich the the dataset with the sector, industry and market capitalization of the DAX companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>symbol</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linde plc</td>\n",
       "      <td>LIN</td>\n",
       "      <td>156.93</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Specialty Chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAP SE</td>\n",
       "      <td>SAP</td>\n",
       "      <td>121.03</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software—Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>SIE</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>DTE</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Airbus SE</td>\n",
       "      <td>AIR</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company_name symbol  market_cap_in_usd_b         country  \\\n",
       "0            Linde plc    LIN               156.93  United Kingdom   \n",
       "1               SAP SE    SAP               121.03         Germany   \n",
       "2           Siemens AG    SIE               110.13         Germany   \n",
       "3  Deutsche Telekom AG    DTE               101.78         Germany   \n",
       "4            Airbus SE    AIR                96.87     Netherlands   \n",
       "\n",
       "                   sector                        industry  \n",
       "0         Basic Materials             Specialty Chemicals  \n",
       "1              Technology            Software—Application  \n",
       "2             Industrials  Specialty Industrial Machinery  \n",
       "3  Communication Services                Telecom Services  \n",
       "4             Industrials             Aerospace & Defense  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://disfold.com/stock-index/dax/companies/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "scraped_data = []\n",
    "for row in table.find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    scraped_data.append(cols)\n",
    "\n",
    "def clean_scraped_data(data):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in data:\n",
    "        # Remove empty rows\n",
    "        if len(row) > 0:\n",
    "            # Remove the '$' and ',' signs from the market cap and convert it to float\n",
    "            market_cap = float(row[3].replace('$', '').replace(',', '').replace('B', ''))\n",
    "            cleaned_data.append([row[1], row[2], market_cap, row[4], row[5], row[6]])\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_data, columns=['company_name', 'symbol', 'market_cap_in_usd_b', 'country', 'sector', 'industry'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "company_enrichments = clean_scraped_data(scraped_data)\n",
    "company_enrichments.to_csv('../data/dax_company_sectors.csv', index=False)\n",
    "company_enrichments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the ticker symbols to prevent NaN and ensure correct join conditions\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('SRT3', 'SRT')\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('HEN3', 'HNK')\n",
    "company_enrichments.loc[company_enrichments['company_name'] == 'Mercedes-Benz Group AG', 'symbol'] = 'DAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['symbol'] = cleaned_data['symbol'].astype(pd.StringDtype())\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].astype(pd.StringDtype())\n",
    "\n",
    "# Merge the cleaned data with the enrichment\n",
    "enriched_cleaned_data = pd.merge(cleaned_data, company_enrichments, how='left', on='symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>company_name</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Sustainability Report 2020 We face up to futur...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'Vaccine', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Sustainability Repo...</td>\n",
       "      <td>somewhat different approach purpose value refl...</td>\n",
       "      <td>[somewhat, different, approach, purpose, value...</td>\n",
       "      <td>[somewhat different, approach, purpose values ...</td>\n",
       "      <td>[(somewhat, RB), (different, JJ), (approach, N...</td>\n",
       "      <td>[[(somewhat, RB), (different, JJ)], [(approach...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Annual Report An overview Gross premium E 01 i...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Vaccine', 'Monopolization', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Annual Report 2021</td>\n",
       "      <td>group net income policyholder surplus book vue...</td>\n",
       "      <td>[group, net, income, policyholder, surplus, bo...</td>\n",
       "      <td>[group net income policyholders, surplus book ...</td>\n",
       "      <td>[(group, NN), (net, JJ), (income, NN), (policy...</td>\n",
       "      <td>[[(group, NN), (net, JJ), (income, NN)], [(sur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company                                            content  \\\n",
       "48  Hannover R  Sustainability Report 2020 We face up to futur...   \n",
       "76  Hannover R  Annual Report An overview Gross premium E 01 i...   \n",
       "\n",
       "                 datatype        date domain  \\\n",
       "48  sustainability_report  2020-03-31    NaN   \n",
       "76          annual_report  2021-03-31    NaN   \n",
       "\n",
       "                                           esg_topics  internal symbol  \\\n",
       "48  ['Whistleblowing', 'Vaccine', 'Corruption', 'G...         1   HNR1   \n",
       "76  ['Vaccine', 'Monopolization', 'Corruption', 'G...         1   HNR1   \n",
       "\n",
       "                                                title  \\\n",
       "48  HannoverRückversicherungAG Sustainability Repo...   \n",
       "76      HannoverRückversicherungAG Annual Report 2021   \n",
       "\n",
       "                                      cleaned_content  \\\n",
       "48  somewhat different approach purpose value refl...   \n",
       "76  group net income policyholder surplus book vue...   \n",
       "\n",
       "                                          word_tokens  \\\n",
       "48  [somewhat, different, approach, purpose, value...   \n",
       "76  [group, net, income, policyholder, surplus, bo...   \n",
       "\n",
       "                                      sentence_tokens  \\\n",
       "48  [somewhat different, approach, purpose values ...   \n",
       "76  [group net income policyholders, surplus book ...   \n",
       "\n",
       "                               pos_tagged_word_tokens  \\\n",
       "48  [(somewhat, RB), (different, JJ), (approach, N...   \n",
       "76  [(group, NN), (net, JJ), (income, NN), (policy...   \n",
       "\n",
       "                           pos_tagged_sentence_tokens company_name  \\\n",
       "48  [[(somewhat, RB), (different, JJ)], [(approach...          NaN   \n",
       "76  [[(group, NN), (net, JJ), (income, NN)], [(sur...          NaN   \n",
       "\n",
       "    market_cap_in_usd_b country sector industry  \n",
       "48                  NaN     NaN    NaN      NaN  \n",
       "76                  NaN     NaN    NaN      NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_cleaned_data[enriched_cleaned_data['industry'].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hannover R AG cannot be matched, since it is not present in the scraped data. Since there are only 2 records this is negligible and will be fixed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'sector'] = 'Financials'\n",
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'industry'] = 'Insurance—Reinsurance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns/data\n",
    "enriched_cleaned_data = enriched_cleaned_data.drop(columns=['content', 'company_name', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>st1_sentiment_continuous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7115</th>\n",
       "      <td>Porsche</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>morningstar</td>\n",
       "      <td>['Privacy', 'Transparency']</td>\n",
       "      <td>0</td>\n",
       "      <td>PAH3</td>\n",
       "      <td>Volkswagen Prepares Porsche IPO, Handelsblatt ...</td>\n",
       "      <td>preparing initial public offering sportscar br...</td>\n",
       "      <td>[preparing, initial, public, offering, sportsc...</td>\n",
       "      <td>[preparing initial public offering sportscar b...</td>\n",
       "      <td>[(preparing, VBG), (initial, JJ), (public, JJ)...</td>\n",
       "      <td>[[(preparing, VBG), (initial, JJ), (public, JJ...</td>\n",
       "      <td>16.65</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>0.306011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>Daimler</td>\n",
       "      <td>tech</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>informaconnect</td>\n",
       "      <td>['Diversity']</td>\n",
       "      <td>0</td>\n",
       "      <td>DAI</td>\n",
       "      <td>Charles Calloway - Chapman and Cutler LLP</td>\n",
       "      <td>informa plcs registered office howick place lo...</td>\n",
       "      <td>[informa, plcs, registered, office, howick, pl...</td>\n",
       "      <td>[, informa plc registered office howick place ...</td>\n",
       "      <td>[(informa, JJ), (plcs, NN), (registered, VBD),...</td>\n",
       "      <td>[[], [(informa, JJ), (registered, JJ), (office...</td>\n",
       "      <td>75.72</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>0.209733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>BMW</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>eenewsautomotive</td>\n",
       "      <td>['EMobility']</td>\n",
       "      <td>0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>GaN Systems raises $ 150m for EV push</td>\n",
       "      <td>150m existing new investor including ceo talk ...</td>\n",
       "      <td>[150m, existing, new, investor, including, ceo...</td>\n",
       "      <td>[existing new investors including, ceo talks p...</td>\n",
       "      <td>[(150m, CD), (existing, VBG), (new, JJ), (inve...</td>\n",
       "      <td>[[(existing, VBG), (new, JJ), (including, VBG)...</td>\n",
       "      <td>60.24</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>0.194147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9166</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['RenewableEnergy']</td>\n",
       "      <td>0</td>\n",
       "      <td>SIE</td>\n",
       "      <td>Siemens Gamesa Renewable Energy S A: Second qu...</td>\n",
       "      <td>disclaimer material prepared gamesa renewable ...</td>\n",
       "      <td>[disclaimer, material, prepared, gamesa, renew...</td>\n",
       "      <td>[disclaimer material prepared gamesa renewable...</td>\n",
       "      <td>[(disclaimer, JJ), (material, NN), (prepared, ...</td>\n",
       "      <td>[[(disclaimer, NN), (material, NN), (prepared,...</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "      <td>0.113462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>Infineon Technologies</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>cnbc</td>\n",
       "      <td>['Antitrust']</td>\n",
       "      <td>0</td>\n",
       "      <td>IFX</td>\n",
       "      <td>GlobalWafers bid for Siltronic fails amid tech...</td>\n",
       "      <td>taiwanese firm make silicon wafer computer chi...</td>\n",
       "      <td>[taiwanese, firm, make, silicon, wafer, comput...</td>\n",
       "      <td>[taiwanese firm makes silicon wafers computer ...</td>\n",
       "      <td>[(taiwanese, JJ), (firm, NN), (make, VBP), (si...</td>\n",
       "      <td>[[(taiwanese, JJ), (firm, NN), (silicon, NN), ...</td>\n",
       "      <td>41.33</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>0.063874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8286</th>\n",
       "      <td>SAP</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-09-20</td>\n",
       "      <td>supplychainbrain</td>\n",
       "      <td>['ValueChain', 'GHGEmission', 'Transparency']</td>\n",
       "      <td>0</td>\n",
       "      <td>SAP</td>\n",
       "      <td>SAP Carbon Footprint Solution Helps Companies ...</td>\n",
       "      <td>application software announced availability pr...</td>\n",
       "      <td>[application, software, announced, availabilit...</td>\n",
       "      <td>[application software announced availability p...</td>\n",
       "      <td>[(application, NN), (software, NN), (announced...</td>\n",
       "      <td>[[(application, NN), (software, NN), (announce...</td>\n",
       "      <td>121.03</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software—Application</td>\n",
       "      <td>0.377927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>marketwatch</td>\n",
       "      <td>['NaturalGas', 'RussianFederation', 'Renewable...</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>European gas futures up as Deutsche Bank analy...</td>\n",
       "      <td>thursday one day eu policymakers unveiled plan...</td>\n",
       "      <td>[thursday, one, day, eu, policymakers, unveile...</td>\n",
       "      <td>[thursday one day eu policymakers unveiled pla...</td>\n",
       "      <td>[(thursday, JJ), (one, CD), (day, NN), (eu, VB...</td>\n",
       "      <td>[[(thursday, NN), (one, CD), (day, NN), (eu, N...</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.084273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5854</th>\n",
       "      <td>Infineon Technologies</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-05-10</td>\n",
       "      <td>eenewsautomotive</td>\n",
       "      <td>['SolarEnergy', 'EMobility']</td>\n",
       "      <td>0</td>\n",
       "      <td>IFX</td>\n",
       "      <td>Infineon signs Showa Denko for silicon carbide...</td>\n",
       "      <td>technology signed key two supply contract japa...</td>\n",
       "      <td>[technology, signed, key, two, supply, contrac...</td>\n",
       "      <td>[technologies signed key two supply contract j...</td>\n",
       "      <td>[(technology, NN), (signed, VBD), (key, JJ), (...</td>\n",
       "      <td>[[(signed, VBD), (key, JJ), (two, CD), (supply...</td>\n",
       "      <td>41.33</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>0.300109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-08-19</td>\n",
       "      <td>businesswire</td>\n",
       "      <td>['Governance']</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>Mavenir Selected by Telekom Romania in the Del...</td>\n",
       "      <td>photo business wire telekom romania headquarte...</td>\n",
       "      <td>[photo, business, wire, telekom, romania, head...</td>\n",
       "      <td>[photo business wire telekom romania headquart...</td>\n",
       "      <td>[(photo, NN), (business, NN), (wire, NN), (tel...</td>\n",
       "      <td>[[(photo, NN), (business, NN), (wire, NN), (te...</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "      <td>0.512247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>Daimler</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>economist</td>\n",
       "      <td>['HumanCapital', 'Compliance', 'Renumeration']</td>\n",
       "      <td>0</td>\n",
       "      <td>DAI</td>\n",
       "      <td>Why Formula 1’ s overseer was right to penalis...</td>\n",
       "      <td>empty threat woefully ineffective child twig m...</td>\n",
       "      <td>[empty, threat, woefully, ineffective, child, ...</td>\n",
       "      <td>[empty threats woefully ineffective, children ...</td>\n",
       "      <td>[(empty, JJ), (threat, NN), (woefully, RB), (i...</td>\n",
       "      <td>[[(empty, JJ), (woefully, RB), (ineffective, J...</td>\n",
       "      <td>75.72</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>-0.100176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company  datatype        date            domain  \\\n",
       "7115                Porsche  business  2021-12-07       morningstar   \n",
       "3330                Daimler      tech  2022-01-31    informaconnect   \n",
       "2135                    BMW      tech  2021-11-22  eenewsautomotive   \n",
       "9166                Siemens  business  2021-04-30    marketscreener   \n",
       "6046  Infineon Technologies  business  2022-02-01              cnbc   \n",
       "8286                    SAP      tech  2021-09-20  supplychainbrain   \n",
       "4024          Deutsche Bank  business  2022-09-15       marketwatch   \n",
       "5854  Infineon Technologies      tech  2021-05-10  eenewsautomotive   \n",
       "4669       Deutsche Telekom   general  2021-08-19      businesswire   \n",
       "3685                Daimler  business  2022-11-03         economist   \n",
       "\n",
       "                                             esg_topics  internal symbol  \\\n",
       "7115                        ['Privacy', 'Transparency']         0   PAH3   \n",
       "3330                                      ['Diversity']         0    DAI   \n",
       "2135                                      ['EMobility']         0    BMW   \n",
       "9166                                ['RenewableEnergy']         0    SIE   \n",
       "6046                                      ['Antitrust']         0    IFX   \n",
       "8286      ['ValueChain', 'GHGEmission', 'Transparency']         0    SAP   \n",
       "4024  ['NaturalGas', 'RussianFederation', 'Renewable...         0    DBK   \n",
       "5854                       ['SolarEnergy', 'EMobility']         0    IFX   \n",
       "4669                                     ['Governance']         0    DTE   \n",
       "3685     ['HumanCapital', 'Compliance', 'Renumeration']         0    DAI   \n",
       "\n",
       "                                                  title  \\\n",
       "7115  Volkswagen Prepares Porsche IPO, Handelsblatt ...   \n",
       "3330          Charles Calloway - Chapman and Cutler LLP   \n",
       "2135              GaN Systems raises $ 150m for EV push   \n",
       "9166  Siemens Gamesa Renewable Energy S A: Second qu...   \n",
       "6046  GlobalWafers bid for Siltronic fails amid tech...   \n",
       "8286  SAP Carbon Footprint Solution Helps Companies ...   \n",
       "4024  European gas futures up as Deutsche Bank analy...   \n",
       "5854  Infineon signs Showa Denko for silicon carbide...   \n",
       "4669  Mavenir Selected by Telekom Romania in the Del...   \n",
       "3685  Why Formula 1’ s overseer was right to penalis...   \n",
       "\n",
       "                                        cleaned_content  \\\n",
       "7115  preparing initial public offering sportscar br...   \n",
       "3330  informa plcs registered office howick place lo...   \n",
       "2135  150m existing new investor including ceo talk ...   \n",
       "9166  disclaimer material prepared gamesa renewable ...   \n",
       "6046  taiwanese firm make silicon wafer computer chi...   \n",
       "8286  application software announced availability pr...   \n",
       "4024  thursday one day eu policymakers unveiled plan...   \n",
       "5854  technology signed key two supply contract japa...   \n",
       "4669  photo business wire telekom romania headquarte...   \n",
       "3685  empty threat woefully ineffective child twig m...   \n",
       "\n",
       "                                            word_tokens  \\\n",
       "7115  [preparing, initial, public, offering, sportsc...   \n",
       "3330  [informa, plcs, registered, office, howick, pl...   \n",
       "2135  [150m, existing, new, investor, including, ceo...   \n",
       "9166  [disclaimer, material, prepared, gamesa, renew...   \n",
       "6046  [taiwanese, firm, make, silicon, wafer, comput...   \n",
       "8286  [application, software, announced, availabilit...   \n",
       "4024  [thursday, one, day, eu, policymakers, unveile...   \n",
       "5854  [technology, signed, key, two, supply, contrac...   \n",
       "4669  [photo, business, wire, telekom, romania, head...   \n",
       "3685  [empty, threat, woefully, ineffective, child, ...   \n",
       "\n",
       "                                        sentence_tokens  \\\n",
       "7115  [preparing initial public offering sportscar b...   \n",
       "3330  [, informa plc registered office howick place ...   \n",
       "2135  [existing new investors including, ceo talks p...   \n",
       "9166  [disclaimer material prepared gamesa renewable...   \n",
       "6046  [taiwanese firm makes silicon wafers computer ...   \n",
       "8286  [application software announced availability p...   \n",
       "4024  [thursday one day eu policymakers unveiled pla...   \n",
       "5854  [technologies signed key two supply contract j...   \n",
       "4669  [photo business wire telekom romania headquart...   \n",
       "3685  [empty threats woefully ineffective, children ...   \n",
       "\n",
       "                                 pos_tagged_word_tokens  \\\n",
       "7115  [(preparing, VBG), (initial, JJ), (public, JJ)...   \n",
       "3330  [(informa, JJ), (plcs, NN), (registered, VBD),...   \n",
       "2135  [(150m, CD), (existing, VBG), (new, JJ), (inve...   \n",
       "9166  [(disclaimer, JJ), (material, NN), (prepared, ...   \n",
       "6046  [(taiwanese, JJ), (firm, NN), (make, VBP), (si...   \n",
       "8286  [(application, NN), (software, NN), (announced...   \n",
       "4024  [(thursday, JJ), (one, CD), (day, NN), (eu, VB...   \n",
       "5854  [(technology, NN), (signed, VBD), (key, JJ), (...   \n",
       "4669  [(photo, NN), (business, NN), (wire, NN), (tel...   \n",
       "3685  [(empty, JJ), (threat, NN), (woefully, RB), (i...   \n",
       "\n",
       "                             pos_tagged_sentence_tokens  market_cap_in_usd_b  \\\n",
       "7115  [[(preparing, VBG), (initial, JJ), (public, JJ...                16.65   \n",
       "3330  [[], [(informa, JJ), (registered, JJ), (office...                75.72   \n",
       "2135  [[(existing, VBG), (new, JJ), (including, VBG)...                60.24   \n",
       "9166  [[(disclaimer, NN), (material, NN), (prepared,...               110.13   \n",
       "6046  [[(taiwanese, JJ), (firm, NN), (silicon, NN), ...                41.33   \n",
       "8286  [[(application, NN), (software, NN), (announce...               121.03   \n",
       "4024  [[(thursday, NN), (one, CD), (day, NN), (eu, N...                24.97   \n",
       "5854  [[(signed, VBD), (key, JJ), (two, CD), (supply...                41.33   \n",
       "4669  [[(photo, NN), (business, NN), (wire, NN), (te...               101.78   \n",
       "3685  [[(empty, JJ), (woefully, RB), (ineffective, J...                75.72   \n",
       "\n",
       "                      sector                        industry  \\\n",
       "7115  Consumer Discretionary              Auto Manufacturers   \n",
       "3330  Consumer Discretionary              Auto Manufacturers   \n",
       "2135  Consumer Discretionary              Auto Manufacturers   \n",
       "9166             Industrials  Specialty Industrial Machinery   \n",
       "6046              Technology                  Semiconductors   \n",
       "8286              Technology            Software—Application   \n",
       "4024              Financials                           Banks   \n",
       "5854              Technology                  Semiconductors   \n",
       "4669  Communication Services                Telecom Services   \n",
       "3685  Consumer Discretionary              Auto Manufacturers   \n",
       "\n",
       "      st1_sentiment_continuous  \n",
       "7115                  0.306011  \n",
       "3330                  0.209733  \n",
       "2135                  0.194147  \n",
       "9166                  0.113462  \n",
       "6046                  0.063874  \n",
       "8286                  0.377927  \n",
       "4024                  0.084273  \n",
       "5854                  0.300109  \n",
       "4669                  0.512247  \n",
       "3685                 -0.100176  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataframe\n",
    "enriched_cleaned_data.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Sentiment Value with Polarity Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last preprocessing step, the sentiment is calculated with the (quite basic) SentimentIntensityAnalyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def get_sentiment_score(sentences):\n",
    "    \"\"\"\n",
    "    Computes the sentiment score for a given list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): The sentences to compute the sentiment score for.\n",
    "\n",
    "    Returns:\n",
    "        float: The sentiment score of the text as a float between -1 and 1.\n",
    "    \"\"\"\n",
    "    # Compute sentiment scores for each sentence and store them in a list\n",
    "    sentiment_scores = [sia.polarity_scores(sentence)['compound'] for sentence in sentences]\n",
    "\n",
    "    # Compute the average sentiment score\n",
    "    avg_sentiment_score = np.mean(sentiment_scores) if sentiment_scores else 0\n",
    "\n",
    "    return avg_sentiment_score\n",
    "\n",
    "# Sentiment score calculation provided most \"balanced\" results with averaged sentence tokens. Therefore the sentiment is calculated on these texts.\n",
    "enriched_cleaned_data['st1_sentiment_continuous'] = enriched_cleaned_data['sentence_tokens'].apply(get_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>st1_sentiment_continuous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [company, datatype, date, domain, esg_topics, internal, symbol, title, cleaned_content, word_tokens, sentence_tokens, pos_tagged_word_tokens, pos_tagged_sentence_tokens, market_cap_in_usd_b, sector, industry, st1_sentiment_continuous]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NaN rows\n",
    "enriched_cleaned_data[enriched_cleaned_data['cleaned_content'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN rows\n",
    "enriched_cleaned_data = enriched_cleaned_data.dropna(subset=['cleaned_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to stage1_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the dataframe as output\n",
    "if not os.path.exists('./output/'):\n",
    "    os.makedirs('./output/')\n",
    "\n",
    "filename = 'stage1_output'\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "enriched_cleaned_data.to_csv(f'./output/{filename}.csv', index=False, sep='|')\n",
    "print(f'Saved DataFrame to {filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>st1_sentiment_continuous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>Deutsche Boerse</td>\n",
       "      <td>tech</td>\n",
       "      <td>2023-03-26</td>\n",
       "      <td>finextra</td>\n",
       "      <td>['terrorism', 'Environment', 'CarbonOffsetting...</td>\n",
       "      <td>0</td>\n",
       "      <td>DB1</td>\n",
       "      <td>How to Build an Institutional-Grade Digital As...</td>\n",
       "      <td>tokenization traditional crypto trading beginn...</td>\n",
       "      <td>[tokenization, traditional, crypto, trading, b...</td>\n",
       "      <td>[, tokenization traditional crypto trading beg...</td>\n",
       "      <td>[(tokenization, NN), (traditional, JJ), (crypt...</td>\n",
       "      <td>[[], [(tokenization, NN), (traditional, JJ), (...</td>\n",
       "      <td>31.43</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Financial Data &amp; Stock Exchanges</td>\n",
       "      <td>0.318784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>Beiersdorf</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>road</td>\n",
       "      <td>['GenderDiversity']</td>\n",
       "      <td>0</td>\n",
       "      <td>BEI</td>\n",
       "      <td>Gammons roasted on Twitter over anti-LTN video</td>\n",
       "      <td>help make better ukip candidate next month lon...</td>\n",
       "      <td>[help, make, better, ukip, candidate, next, mo...</td>\n",
       "      <td>[, help us make better, ukip candidate next mo...</td>\n",
       "      <td>[(help, NN), (make, VB), (better, JJR), (ukip,...</td>\n",
       "      <td>[[], [(help, NN), (make, VB), (better, JJR)], ...</td>\n",
       "      <td>25.99</td>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>Household &amp; Personal Products</td>\n",
       "      <td>0.154850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>finsmes</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>Kinexon Raises $ 130M in Series A Funding</td>\n",
       "      <td>munich germanybased provider sensing software ...</td>\n",
       "      <td>[munich, germanybased, provider, sensing, soft...</td>\n",
       "      <td>[munich germanybased provider sensing software...</td>\n",
       "      <td>[(munich, NNS), (germanybased, VBD), (provider...</td>\n",
       "      <td>[[(munich, NN), (germanybased, VBD), (provider...</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "      <td>0.310410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-06-21</td>\n",
       "      <td>autonews</td>\n",
       "      <td>['Cybersecurity', 'DataSecurity', 'Recruiting']</td>\n",
       "      <td>0</td>\n",
       "      <td>VOW3</td>\n",
       "      <td>Car dealers, vendors need to protect data</td>\n",
       "      <td>data breach ransomware attack car dealer softw...</td>\n",
       "      <td>[data, breach, ransomware, attack, car, dealer...</td>\n",
       "      <td>[data breaches ransomware attacks car dealers ...</td>\n",
       "      <td>[(data, NNS), (breach, NN), (ransomware, NN), ...</td>\n",
       "      <td>[[(data, NNS), (ransomware, NN), (car, NN), (s...</td>\n",
       "      <td>75.05</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>0.342875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10945</th>\n",
       "      <td>Zalando</td>\n",
       "      <td>esg</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>greenbiz</td>\n",
       "      <td>['RenewableEnergy', 'Transparency', 'ValueChai...</td>\n",
       "      <td>0</td>\n",
       "      <td>ZAL</td>\n",
       "      <td>Fashion retailers need to know where their cot...</td>\n",
       "      <td>simply know buy middleman fiber get blended me...</td>\n",
       "      <td>[simply, know, buy, middleman, fiber, get, ble...</td>\n",
       "      <td>[simply know, buy, fibers get, mechanisms trac...</td>\n",
       "      <td>[(simply, RB), (know, VB), (buy, VB), (middlem...</td>\n",
       "      <td>[[(simply, RB), (know, VB)], [(buy, VB)], [(ge...</td>\n",
       "      <td>10.41</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Internet Retail</td>\n",
       "      <td>0.231551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5127</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-06-17</td>\n",
       "      <td>zdnet</td>\n",
       "      <td>['Privacy', 'DataSecurity', 'Compliance', 'Soc...</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>The biggest investment in database history, th...</td>\n",
       "      <td>series funding round bringing neo4js valuation...</td>\n",
       "      <td>[series, funding, round, bringing, neo4js, val...</td>\n",
       "      <td>[series funding round bringing valuation, soci...</td>\n",
       "      <td>[(series, NN), (funding, NN), (round, IN), (br...</td>\n",
       "      <td>[[(series, NN), (funding, NN), (round, IN), (b...</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "      <td>0.158088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8462</th>\n",
       "      <td>SAP</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>globalbankingandfinance</td>\n",
       "      <td>['Environment', 'Transparency']</td>\n",
       "      <td>0</td>\n",
       "      <td>SAP</td>\n",
       "      <td>Accounting Software Market Is Projected To Exp...</td>\n",
       "      <td>description according new market report publis...</td>\n",
       "      <td>[description, according, new, market, report, ...</td>\n",
       "      <td>[description according new market report publi...</td>\n",
       "      <td>[(description, NN), (according, VBG), (new, JJ...</td>\n",
       "      <td>[[(description, NN), (according, VBG), (new, J...</td>\n",
       "      <td>121.03</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software—Application</td>\n",
       "      <td>0.331017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8732</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>designnews</td>\n",
       "      <td>['Compliance', 'Environment']</td>\n",
       "      <td>0</td>\n",
       "      <td>SIE</td>\n",
       "      <td>Build a Better Food &amp; Beverage System with Non...</td>\n",
       "      <td>beverage system noncorrosive bearing designnew...</td>\n",
       "      <td>[beverage, system, noncorrosive, bearing, desi...</td>\n",
       "      <td>[beverage system noncorrosive bearings subthem...</td>\n",
       "      <td>[(beverage, NN), (system, NN), (noncorrosive, ...</td>\n",
       "      <td>[[(beverage, VBP), (system, NN), (noncorrosive...</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "      <td>0.106493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>business</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['Compliance', 'Governance']</td>\n",
       "      <td>0</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>E ON: Declaration of Compliance December 2022</td>\n",
       "      <td>se pursuant section german stock corporation a...</td>\n",
       "      <td>[se, pursuant, section, german, stock, corpora...</td>\n",
       "      <td>[se pursuant section german stock corporation ...</td>\n",
       "      <td>[(se, NN), (pursuant, NN), (section, NN), (ger...</td>\n",
       "      <td>[[(se, NN), (pursuant, NN), (section, NN), (ge...</td>\n",
       "      <td>27.70</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Utilities—Diversified</td>\n",
       "      <td>0.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11009</th>\n",
       "      <td>Zalando</td>\n",
       "      <td>thinktank</td>\n",
       "      <td>2021-05-13</td>\n",
       "      <td>sloanreview.mit</td>\n",
       "      <td>['Privacy', 'ValueChain', 'CustomerService', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>ZAL</td>\n",
       "      <td>How COVID-19 Will Change the Geography of Comp...</td>\n",
       "      <td>three trend reshaping global strategy operatio...</td>\n",
       "      <td>[three, trend, reshaping, global, strategy, op...</td>\n",
       "      <td>[, three trends reshaping global strategy oper...</td>\n",
       "      <td>[(three, CD), (trend, NN), (reshaping, VBG), (...</td>\n",
       "      <td>[[], [(three, CD), (reshaping, VBG), (global, ...</td>\n",
       "      <td>10.41</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Internet Retail</td>\n",
       "      <td>0.158508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company   datatype        date                   domain  \\\n",
       "4442    Deutsche Boerse       tech  2023-03-26                 finextra   \n",
       "2758         Beiersdorf    general  2021-04-16                     road   \n",
       "5092   Deutsche Telekom   business  2022-04-26                  finsmes   \n",
       "10683        Volkswagen    general  2021-06-21                 autonews   \n",
       "10945           Zalando        esg  2022-05-06                 greenbiz   \n",
       "5127   Deutsche Telekom       tech  2021-06-17                    zdnet   \n",
       "8462                SAP   business  2021-11-03  globalbankingandfinance   \n",
       "8732            Siemens   business  2021-11-30               designnews   \n",
       "5194             E ONSE   business  2023-01-30           marketscreener   \n",
       "11009           Zalando  thinktank  2021-05-13          sloanreview.mit   \n",
       "\n",
       "                                              esg_topics  internal symbol  \\\n",
       "4442   ['terrorism', 'Environment', 'CarbonOffsetting...         0    DB1   \n",
       "2758                                 ['GenderDiversity']         0    BEI   \n",
       "5092                                                  []         0    DTE   \n",
       "10683    ['Cybersecurity', 'DataSecurity', 'Recruiting']         0   VOW3   \n",
       "10945  ['RenewableEnergy', 'Transparency', 'ValueChai...         0    ZAL   \n",
       "5127   ['Privacy', 'DataSecurity', 'Compliance', 'Soc...         0    DTE   \n",
       "8462                     ['Environment', 'Transparency']         0    SAP   \n",
       "8732                       ['Compliance', 'Environment']         0    SIE   \n",
       "5194                        ['Compliance', 'Governance']         0   EOAN   \n",
       "11009  ['Privacy', 'ValueChain', 'CustomerService', '...         0    ZAL   \n",
       "\n",
       "                                                   title  \\\n",
       "4442   How to Build an Institutional-Grade Digital As...   \n",
       "2758      Gammons roasted on Twitter over anti-LTN video   \n",
       "5092           Kinexon Raises $ 130M in Series A Funding   \n",
       "10683          Car dealers, vendors need to protect data   \n",
       "10945  Fashion retailers need to know where their cot...   \n",
       "5127   The biggest investment in database history, th...   \n",
       "8462   Accounting Software Market Is Projected To Exp...   \n",
       "8732   Build a Better Food & Beverage System with Non...   \n",
       "5194       E ON: Declaration of Compliance December 2022   \n",
       "11009  How COVID-19 Will Change the Geography of Comp...   \n",
       "\n",
       "                                         cleaned_content  \\\n",
       "4442   tokenization traditional crypto trading beginn...   \n",
       "2758   help make better ukip candidate next month lon...   \n",
       "5092   munich germanybased provider sensing software ...   \n",
       "10683  data breach ransomware attack car dealer softw...   \n",
       "10945  simply know buy middleman fiber get blended me...   \n",
       "5127   series funding round bringing neo4js valuation...   \n",
       "8462   description according new market report publis...   \n",
       "8732   beverage system noncorrosive bearing designnew...   \n",
       "5194   se pursuant section german stock corporation a...   \n",
       "11009  three trend reshaping global strategy operatio...   \n",
       "\n",
       "                                             word_tokens  \\\n",
       "4442   [tokenization, traditional, crypto, trading, b...   \n",
       "2758   [help, make, better, ukip, candidate, next, mo...   \n",
       "5092   [munich, germanybased, provider, sensing, soft...   \n",
       "10683  [data, breach, ransomware, attack, car, dealer...   \n",
       "10945  [simply, know, buy, middleman, fiber, get, ble...   \n",
       "5127   [series, funding, round, bringing, neo4js, val...   \n",
       "8462   [description, according, new, market, report, ...   \n",
       "8732   [beverage, system, noncorrosive, bearing, desi...   \n",
       "5194   [se, pursuant, section, german, stock, corpora...   \n",
       "11009  [three, trend, reshaping, global, strategy, op...   \n",
       "\n",
       "                                         sentence_tokens  \\\n",
       "4442   [, tokenization traditional crypto trading beg...   \n",
       "2758   [, help us make better, ukip candidate next mo...   \n",
       "5092   [munich germanybased provider sensing software...   \n",
       "10683  [data breaches ransomware attacks car dealers ...   \n",
       "10945  [simply know, buy, fibers get, mechanisms trac...   \n",
       "5127   [series funding round bringing valuation, soci...   \n",
       "8462   [description according new market report publi...   \n",
       "8732   [beverage system noncorrosive bearings subthem...   \n",
       "5194   [se pursuant section german stock corporation ...   \n",
       "11009  [, three trends reshaping global strategy oper...   \n",
       "\n",
       "                                  pos_tagged_word_tokens  \\\n",
       "4442   [(tokenization, NN), (traditional, JJ), (crypt...   \n",
       "2758   [(help, NN), (make, VB), (better, JJR), (ukip,...   \n",
       "5092   [(munich, NNS), (germanybased, VBD), (provider...   \n",
       "10683  [(data, NNS), (breach, NN), (ransomware, NN), ...   \n",
       "10945  [(simply, RB), (know, VB), (buy, VB), (middlem...   \n",
       "5127   [(series, NN), (funding, NN), (round, IN), (br...   \n",
       "8462   [(description, NN), (according, VBG), (new, JJ...   \n",
       "8732   [(beverage, NN), (system, NN), (noncorrosive, ...   \n",
       "5194   [(se, NN), (pursuant, NN), (section, NN), (ger...   \n",
       "11009  [(three, CD), (trend, NN), (reshaping, VBG), (...   \n",
       "\n",
       "                              pos_tagged_sentence_tokens  market_cap_in_usd_b  \\\n",
       "4442   [[], [(tokenization, NN), (traditional, JJ), (...                31.43   \n",
       "2758   [[], [(help, NN), (make, VB), (better, JJR)], ...                25.99   \n",
       "5092   [[(munich, NN), (germanybased, VBD), (provider...               101.78   \n",
       "10683  [[(data, NNS), (ransomware, NN), (car, NN), (s...                75.05   \n",
       "10945  [[(simply, RB), (know, VB)], [(buy, VB)], [(ge...                10.41   \n",
       "5127   [[(series, NN), (funding, NN), (round, IN), (b...               101.78   \n",
       "8462   [[(description, NN), (according, VBG), (new, J...               121.03   \n",
       "8732   [[(beverage, VBP), (system, NN), (noncorrosive...               110.13   \n",
       "5194   [[(se, NN), (pursuant, NN), (section, NN), (ge...                27.70   \n",
       "11009  [[], [(three, CD), (reshaping, VBG), (global, ...                10.41   \n",
       "\n",
       "                       sector                          industry  \\\n",
       "4442               Financials  Financial Data & Stock Exchanges   \n",
       "2758         Consumer Staples     Household & Personal Products   \n",
       "5092   Communication Services                  Telecom Services   \n",
       "10683  Consumer Discretionary                Auto Manufacturers   \n",
       "10945  Consumer Discretionary                   Internet Retail   \n",
       "5127   Communication Services                  Telecom Services   \n",
       "8462               Technology              Software—Application   \n",
       "8732              Industrials    Specialty Industrial Machinery   \n",
       "5194                Utilities             Utilities—Diversified   \n",
       "11009  Consumer Discretionary                   Internet Retail   \n",
       "\n",
       "       st1_sentiment_continuous  \n",
       "4442                   0.318784  \n",
       "2758                   0.154850  \n",
       "5092                   0.310410  \n",
       "10683                  0.342875  \n",
       "10945                  0.231551  \n",
       "5127                   0.158088  \n",
       "8462                   0.331017  \n",
       "8732                   0.106493  \n",
       "5194                   0.435000  \n",
       "11009                  0.158508  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data after full preprocessing\n",
    "enriched_cleaned_data.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
