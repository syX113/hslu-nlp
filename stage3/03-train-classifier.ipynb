{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syX113/hslu-nlp/blob/train-colab/stage3/03-train-classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UyvrKk4ZBcNI"
      },
      "source": [
        "# CLT Project - Stage III\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpbTSOZBcNP"
      },
      "source": [
        "- **Author:**             Arian Contessotto, Tim Giger, Levin Reichmuth\n",
        "- **Submission Date:**    1 June 2023"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHGNKKFbBcNR"
      },
      "source": [
        "## 1. Prerequisites and Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMbz9g1GhM8h",
        "outputId": "f4ee390e-7a82-456f-a575-f8376cb53567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'hslu-nlp'...\n",
            "remote: Enumerating objects: 212, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 212 (delta 18), reused 28 (delta 8), pack-reused 122\u001b[K\n",
            "Receiving objects: 100% (212/212), 40.85 MiB | 7.05 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "Updating files: 100% (12/12), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone repo with dataset\n",
        "!git clone https://github.com/syX113/hslu-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-su7FTvhjtW",
        "outputId": "c142039c-b845-4820-9950-76895875e69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "full_llm_annotated.csv\tgold_standard.csv\n"
          ]
        }
      ],
      "source": [
        "# Check if files are loaded\n",
        "!ls hslu-nlp/stage2/annotated/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ATGDdAK4NloL"
      },
      "source": [
        "If necessary, install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBNiqJjdBcNT",
        "outputId": "d19fa007-38e6-401f-ff62-375663f9d095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Required package installation\n",
        "!transformers==4.28.0\n",
        "!pip install torch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ppL6jWrIBcNV"
      },
      "source": [
        "### 1.1 Import Packages and Make Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mikPBmLaBcNV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-26 14:57:52.372680: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-26 14:57:52.525578: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-26 14:57:53.159311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score,  mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TsgyJBEWBcNW"
      },
      "source": [
        "### 1.2 Load Annotated Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zzhuW_gcN404"
      },
      "source": [
        "The final dataframe from stage one is loaded. These data are the basis for stage two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "xzPR79XdBcNX",
        "outputId": "2e5e8a18-6c4e-4b11-f648-e989ef8b0e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11071, 17)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company</th>\n",
              "      <th>datatype</th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>domain</th>\n",
              "      <th>esg_topics</th>\n",
              "      <th>internal</th>\n",
              "      <th>symbol</th>\n",
              "      <th>sentence_tokens</th>\n",
              "      <th>market_cap_in_usd_b</th>\n",
              "      <th>sector</th>\n",
              "      <th>industry</th>\n",
              "      <th>year_month</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>sentiment_llm_continuous</th>\n",
              "      <th>sentiment_llm_categorial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beiersdorf</td>\n",
              "      <td>sustainability_report</td>\n",
              "      <td>BeiersdorfAG Sustainability Report 2021</td>\n",
              "      <td>2021-03-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[CleanWater, GHGEmission, ProductLiability, Va...</td>\n",
              "      <td>1</td>\n",
              "      <td>BEI</td>\n",
              "      <td>[brands strategy sustainability agenda care be...</td>\n",
              "      <td>25.99</td>\n",
              "      <td>Consumer Staples</td>\n",
              "      <td>Household &amp; Personal Products</td>\n",
              "      <td>2021-03</td>\n",
              "      <td>2021</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.4510161280632019, 0.6138720512390137, 0.226...</td>\n",
              "      <td>[0.5, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, 1.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Deutsche Telekom</td>\n",
              "      <td>sustainability_report</td>\n",
              "      <td>DeutscheTelekomAG Sustainability Report 2021</td>\n",
              "      <td>2021-03-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[DataSecurity, Iso50001, GlobalWarming, Produc...</td>\n",
              "      <td>1</td>\n",
              "      <td>DTE</td>\n",
              "      <td>[management facts, deutsche telekom cr report,...</td>\n",
              "      <td>101.78</td>\n",
              "      <td>Communication Services</td>\n",
              "      <td>Telecom Services</td>\n",
              "      <td>2021-03</td>\n",
              "      <td>2021</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.35756340622901917, 0.29088783264160156, 0.3...</td>\n",
              "      <td>[0.5, 0.0, 0.5, 0.5, 0.0, 0.5, 0.5, 0.5, 0.5, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vonovia</td>\n",
              "      <td>sustainability_report</td>\n",
              "      <td>VonoviaSE Sustainability Report 2021</td>\n",
              "      <td>2021-03-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Whistleblowing, DataSecurity, Vaccine, GHGEmi...</td>\n",
              "      <td>1</td>\n",
              "      <td>VNA</td>\n",
              "      <td>[sustainable future, sustainability report dea...</td>\n",
              "      <td>20.35</td>\n",
              "      <td>Real Estate</td>\n",
              "      <td>Real Estate Services</td>\n",
              "      <td>2021-03</td>\n",
              "      <td>2021</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.4570336639881134, 0.45287153124809265, 0.26...</td>\n",
              "      <td>[0.5, 0.5, 0.0, 0.5, 0.5, 0.5, 0.0, 0.5, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Merck</td>\n",
              "      <td>sustainability_report</td>\n",
              "      <td>MerckKGaA Sustainability Report 2021</td>\n",
              "      <td>2021-03-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[DataSecurity, DataMisuse, DrugResistance, Iso...</td>\n",
              "      <td>1</td>\n",
              "      <td>MRK</td>\n",
              "      <td>[management employees profile attractive emplo...</td>\n",
              "      <td>87.64</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>Drug Manufacturers—Specialty &amp; Generic</td>\n",
              "      <td>2021-03</td>\n",
              "      <td>2021</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.36378589272499084, 0.6118267178535461, 0.48...</td>\n",
              "      <td>[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MTU</td>\n",
              "      <td>sustainability_report</td>\n",
              "      <td>MTUAeroEngines Sustainability Report 2020</td>\n",
              "      <td>2020-03-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[WorkLifeBalance, Corruption, AirQuality, Data...</td>\n",
              "      <td>1</td>\n",
              "      <td>MTX</td>\n",
              "      <td>[sustainability goes far beyond climate action...</td>\n",
              "      <td>12.24</td>\n",
              "      <td>Industrials</td>\n",
              "      <td>Aerospace &amp; Defense</td>\n",
              "      <td>2020-03</td>\n",
              "      <td>2020</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.46082836389541626, 0.46208637952804565, 0.4...</td>\n",
              "      <td>[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 0.5, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            company               datatype  \\\n",
              "0        Beiersdorf  sustainability_report   \n",
              "1  Deutsche Telekom  sustainability_report   \n",
              "2           Vonovia  sustainability_report   \n",
              "3             Merck  sustainability_report   \n",
              "4               MTU  sustainability_report   \n",
              "\n",
              "                                          title        date domain  \\\n",
              "0       BeiersdorfAG Sustainability Report 2021  2021-03-31    NaN   \n",
              "1  DeutscheTelekomAG Sustainability Report 2021  2021-03-31    NaN   \n",
              "2          VonoviaSE Sustainability Report 2021  2021-03-31    NaN   \n",
              "3          MerckKGaA Sustainability Report 2021  2021-03-31    NaN   \n",
              "4     MTUAeroEngines Sustainability Report 2020  2020-03-31    NaN   \n",
              "\n",
              "                                          esg_topics  internal symbol  \\\n",
              "0  [CleanWater, GHGEmission, ProductLiability, Va...         1    BEI   \n",
              "1  [DataSecurity, Iso50001, GlobalWarming, Produc...         1    DTE   \n",
              "2  [Whistleblowing, DataSecurity, Vaccine, GHGEmi...         1    VNA   \n",
              "3  [DataSecurity, DataMisuse, DrugResistance, Iso...         1    MRK   \n",
              "4  [WorkLifeBalance, Corruption, AirQuality, Data...         1    MTX   \n",
              "\n",
              "                                     sentence_tokens  market_cap_in_usd_b  \\\n",
              "0  [brands strategy sustainability agenda care be...                25.99   \n",
              "1  [management facts, deutsche telekom cr report,...               101.78   \n",
              "2  [sustainable future, sustainability report dea...                20.35   \n",
              "3  [management employees profile attractive emplo...                87.64   \n",
              "4  [sustainability goes far beyond climate action...                12.24   \n",
              "\n",
              "                   sector                                industry year_month  \\\n",
              "0        Consumer Staples           Household & Personal Products    2021-03   \n",
              "1  Communication Services                        Telecom Services    2021-03   \n",
              "2             Real Estate                    Real Estate Services    2021-03   \n",
              "3              Healthcare  Drug Manufacturers—Specialty & Generic    2021-03   \n",
              "4             Industrials                     Aerospace & Defense    2020-03   \n",
              "\n",
              "   year  month                           sentiment_llm_continuous  \\\n",
              "0  2021      3  [0.4510161280632019, 0.6138720512390137, 0.226...   \n",
              "1  2021      3  [0.35756340622901917, 0.29088783264160156, 0.3...   \n",
              "2  2021      3  [0.4570336639881134, 0.45287153124809265, 0.26...   \n",
              "3  2021      3  [0.36378589272499084, 0.6118267178535461, 0.48...   \n",
              "4  2020      3  [0.46082836389541626, 0.46208637952804565, 0.4...   \n",
              "\n",
              "                            sentiment_llm_categorial  \n",
              "0  [0.5, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, 1.0, ...  \n",
              "1  [0.5, 0.0, 0.5, 0.5, 0.0, 0.5, 0.5, 0.5, 0.5, ...  \n",
              "2  [0.5, 0.5, 0.0, 0.5, 0.5, 0.5, 0.0, 0.5, 0.0, ...  \n",
              "3  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, ...  \n",
              "4  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 0.5, ...  "
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define file name\n",
        "esg_file = '../stage2/annotated/full_llm_annotated.csv' # Local filepath\n",
        "#esg_file = 'hslu-nlp/stage2/annotated/full_llm_annotated.csv' # Filepath on Colab\n",
        "\n",
        "# Define function to load and merge data\n",
        "def load_data(file):\n",
        "\n",
        "    # Load the data\n",
        "    df = pd.read_csv(file, delimiter = '|')\n",
        "\n",
        "    # Apply eval function\n",
        "    df['esg_topics'] = df['esg_topics'].apply(eval)\n",
        "    df['sentence_tokens'] = df['sentence_tokens'].apply(eval)\n",
        "    df['sentiment_llm_continuous'] = df['sentiment_llm_continuous'].apply(eval)\n",
        "    df['sentiment_llm_categorial'] = df['sentiment_llm_categorial'].apply(eval)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = load_data(esg_file)\n",
        "\n",
        "# Print shape and diyplay header\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ukHZpK87esiJ"
      },
      "source": [
        "### 1.3 Create different Dataframes (Sentences & full Document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "kas2z-8wsNbe",
        "outputId": "5a7333be-d438-40bd-eb24-c384451ef041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(678529, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>internal</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>brands strategy sustainability agenda care bey...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>successfully reduced carbon footprint absolute...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>end consumer business returned levels reduced ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>decoupling human economic activity natural res...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>inspired beiersdorf ambitious sustainability a...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   internal                                           sentence  sentiment\n",
              "0         1  brands strategy sustainability agenda care bey...        0.5\n",
              "1         1  successfully reduced carbon footprint absolute...        0.5\n",
              "2         1  end consumer business returned levels reduced ...        0.0\n",
              "3         1  decoupling human economic activity natural res...        0.0\n",
              "4         1  inspired beiersdorf ambitious sustainability a...        0.5"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_sentence_df(data):\n",
        "\n",
        "    # Select relevant columns\n",
        "    data = data[['internal','sentence_tokens','sentiment_llm_categorial']]\n",
        "\n",
        "    # Explode the tokens, so each sentence is a row\n",
        "    data = data.set_index(['internal']).apply(pd.Series.explode).reset_index()\n",
        "\n",
        "    # Rename the columns and change order\n",
        "    data.rename(columns={'sentence_tokens': 'sentence', 'sentiment_llm_categorial': 'sentiment'}, inplace=True)\n",
        "    data = data[['internal', 'sentence', 'sentiment']]\n",
        "\n",
        "    # Convert types\n",
        "    data['internal'] = data['internal'].astype(int)\n",
        "    data['sentence'] = data['sentence'].astype(str)\n",
        "    data['sentiment'] = data['sentiment'].astype(float)\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Create sentence data\n",
        "sentence_df = create_sentence_df(df)\n",
        "\n",
        "# Display header and shape\n",
        "print(sentence_df.shape)\n",
        "sentence_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "vf-rmV_PesiK",
        "outputId": "9aa43b1b-bd42-490c-ead3-a9d0bffff230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11071, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>internal</th>\n",
              "      <th>document</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>brands strategy sustainability agenda care bey...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>management facts deutsche telekom cr report th...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>sustainable future sustainability report dear ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>management employees profile attractive employ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>sustainability goes far beyond climate action ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   internal                                           document  sentiment\n",
              "0         1  brands strategy sustainability agenda care bey...        0.5\n",
              "1         1  management facts deutsche telekom cr report th...        0.5\n",
              "2         1  sustainable future sustainability report dear ...        0.5\n",
              "3         1  management employees profile attractive employ...        0.5\n",
              "4         1  sustainability goes far beyond climate action ...        0.5"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to create document data\n",
        "def create_document_df(data):\n",
        "\n",
        "    # Join tokens\n",
        "    data['document'] = data['sentence_tokens'].apply(' '.join)  # Convert tokens to strings\n",
        "\n",
        "    # Compute the mean of the computed sentiment and discretize it\n",
        "    def discretize_sentiment(value):\n",
        "        if value <= 0.33:\n",
        "            return 0.0\n",
        "        elif value <= 0.66:\n",
        "            return 0.5\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    data['sentiment'] = data['sentiment_llm_continuous'].apply(np.mean).apply(discretize_sentiment)\n",
        "\n",
        "    # Convert types\n",
        "    data['internal'] = data['internal'].astype(int)\n",
        "    data['sentence'] = data['document'].astype(str)\n",
        "    data['sentiment'] = data['sentiment'].astype(float)\n",
        "\n",
        "    # Return needed columns and discretized mean of the sentiment\n",
        "    return data[['internal', 'document', 'sentiment']]\n",
        "\n",
        "# Create sentence data\n",
        "document_df = create_document_df(df)\n",
        "\n",
        "# Display header and shape\n",
        "print(document_df.shape)\n",
        "document_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The subsets for the training should have equally distributed classes. In addition, external and internal documents should be represented.  \n",
        "One of these conditions needs to be more \"loose\", we decide class equality is more important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "4IEirqQJmfLm",
        "outputId": "0a1940c5-c50a-452b-948c-361714ed5d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(103812, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>internal</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>482121</th>\n",
              "      <td>0</td>\n",
              "      <td>july incyte eli lilly announced fda would meet...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459567</th>\n",
              "      <td>0</td>\n",
              "      <td>still cautious highly contagious delta strain ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110344</th>\n",
              "      <td>1</td>\n",
              "      <td>march first publicprivate peer employees tied ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267939</th>\n",
              "      <td>0</td>\n",
              "      <td>content plans featuring presentation followed ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315677</th>\n",
              "      <td>0</td>\n",
              "      <td>pubmed scopus google scholar however link nets...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        internal                                           sentence  sentiment\n",
              "482121         0  july incyte eli lilly announced fda would meet...        0.5\n",
              "459567         0  still cautious highly contagious delta strain ...        0.5\n",
              "110344         1  march first publicprivate peer employees tied ...        0.5\n",
              "267939         0  content plans featuring presentation followed ...        0.5\n",
              "315677         0  pubmed scopus google scholar however link nets...        0.5"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def balance_sentiment_and_internal(df):\n",
        "    # Get minimum number of observations across sentiment classes\n",
        "    min_internal_count = df['internal'].value_counts().min()\n",
        "    \n",
        "    # Get the minimum number of observations between internal == 0 and internal == 1\n",
        "    min_sentiment_count = min(df[df['sentiment'] == 0].shape[0], df[df['sentiment'] == 1].shape[0], min_internal_count)\n",
        "\n",
        "    # Create \"balanced\" dataframe\n",
        "    balanced_df = pd.concat([df[df['sentiment'] == i].sample(min_sentiment_count, random_state=1) for i in df['sentiment'].unique()])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "# Sample the sentence dataframe\n",
        "sub_sentence_df = balance_sentiment_and_internal(sentence_df)\n",
        "\n",
        "# Display header and shape\n",
        "print(sub_sentence_df.shape)\n",
        "sub_sentence_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(186, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>internal</th>\n",
              "      <th>document</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10179</th>\n",
              "      <td>0</td>\n",
              "      <td>dgapnews ag key word quarterly interim stateme...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2292</th>\n",
              "      <td>0</td>\n",
              "      <td>president fraunhofer institute ceramic technol...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10338</th>\n",
              "      <td>0</td>\n",
              "      <td>yet father granted biopic still waiting twenty...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5472</th>\n",
              "      <td>0</td>\n",
              "      <td>stepped gear dedicated taskforce held inaugura...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8299</th>\n",
              "      <td>0</td>\n",
              "      <td>business technology platform critical piece la...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       internal                                           document  sentiment\n",
              "10179         0  dgapnews ag key word quarterly interim stateme...        0.5\n",
              "2292          0  president fraunhofer institute ceramic technol...        0.5\n",
              "10338         0  yet father granted biopic still waiting twenty...        0.5\n",
              "5472          0  stepped gear dedicated taskforce held inaugura...        0.5\n",
              "8299          0  business technology platform critical piece la...        0.5"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample the document dataframe\n",
        "sub_document_df = balance_sentiment_and_internal(document_df)\n",
        "\n",
        "# Display header and shape\n",
        "print(sub_document_df.shape)\n",
        "sub_document_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence subset:\n",
            "0    72896\n",
            "1    30916\n",
            "Name: internal, dtype: int64\n",
            "0.5    34604\n",
            "0.0    34604\n",
            "1.0    34604\n",
            "Name: sentiment, dtype: int64\n",
            "\n",
            "\n",
            "Document subset:\n",
            "0    186\n",
            "Name: internal, dtype: int64\n",
            "0.5    62\n",
            "1.0    62\n",
            "0.0    62\n",
            "Name: sentiment, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Inspect sampling results\n",
        "print('Sentence subset:')\n",
        "print(sub_sentence_df['internal'].value_counts())\n",
        "print(sub_sentence_df['sentiment'].value_counts())\n",
        "print('\\n')\n",
        "print('Document subset:')\n",
        "print(sub_document_df['internal'].value_counts())\n",
        "print(sub_document_df['sentiment'].value_counts())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o1UJ8F9FesiK"
      },
      "source": [
        "As a result, the models can be evaluated and trained with 2 approaches:  \n",
        "- A dataframe containing the full document and a discretized mean sentiment of all included sentences\n",
        "- A dataframe containing each sentence with the corresponding discretized sentiment  \n",
        "- Two sampled subset dataframes for moel evaluation\n",
        "\n",
        "\"Discretized\" corresponds to the labels 0.0 (negative), 0.5 (neutral) and 1.0 (positive)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2iVzZBesiL"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The evaluation for the model is based on the following conceptual approach:\n",
        "1. Select multiple pretrained (Huggingface) models, based on previous stages\n",
        "2. Train the selected models on a small subset of the full documents and the single sentences to keep the training time short\n",
        "3. Compare the training outcomes of the different models on the two subsets and select the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to compute the comparison metrics\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    \n",
        "    # Use the appropriate metrics, since we don't have discrete classes but a continous score \n",
        "    mse = mean_squared_error(y_true=labels, y_pred=pred)\n",
        "    mae = mean_absolute_error(y_true=labels, y_pred=pred)\n",
        "    r2 = r2_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"MSE\": mse, \"MAE\": mae, \"R2\": r2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "fzglNdb0esiM",
        "outputId": "3cc02fbb-4a45-4320-c9b0-bb78d703c4c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "# Load Tensorboard for training monitoring\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "# Kill potential Tensorboard process, so it don't block the port\n",
        "!pkill -f \"tensorboard\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "D-2Tp63XesiM",
        "outputId": "05ed8c73-846e-44a1-e12d-4e5b5134484e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-e3e70682c2094cac\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-e3e70682c2094cac\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6010;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start Tensorboard to monitor training process\n",
        "%tensorboard --logdir ./evaluation/ --port 6010"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model 1: *distilbert-base-uncased*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kwjEdd_oesiL"
      },
      "source": [
        "As a first test, we use the lightweight \"distilbert-base-uncased\" model and fine-tune it on the full documents and the sentences, since the finetuned *\"nlptown/bert-base-multilingual-uncased-sentiment\"* demonstrated high alignment with gold standard in stage 2.  \n",
        "Since BERT only accepts 512 input word tokens, the full documents are heavyily truncated.  \n",
        "\n",
        "Model page: https://huggingface.co/distilbert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DWYx-J7esiL",
        "outputId": "8307f8d4-dfe9-4999-9aa5-83f9f32507be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'classifier.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'classifier.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Define pretrained tokenizer and model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1) # 1 label to get a continuous score between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the torch dataset to use dataset in PyTorch and override necessary methods\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DM6ziqUCesiL"
      },
      "source": [
        "#### Train *distilbert-base-uncased* on sentence subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data with a 70%, 15% and 15% ratio (train, valid, test)\n",
        "X = list(sub_sentence_df[\"sentence\"])\n",
        "y = list(sub_sentence_df[\"sentiment\"])\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3) # Split 70% train data\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5) # Split the other 30% in 50% each to get the correct ratio\n",
        "\n",
        "# Tokenize the datasets\n",
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
        "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
        "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Create the train, validation and test dataset as PyTorch datasets\n",
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(X_val_tokenized, y_val)\n",
        "test_dataset = Dataset(X_test_tokenized, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "LySMEgQbesiL"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./evaluation/distilbert_sentences\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    gradient_accumulation_steps=4,\n",
        "    seed=0,\n",
        "    optim=\"adamw_torch\", # Use newer PyTorch optimizer\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    report_to='tensorboard')\n",
        "\n",
        "# Define Huggingface Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "gkzspgEResiM",
        "outputId": "544b3439-c9c8-4b11-bfc4-574588c30ef6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1001' max='6813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1001/6813 08:03 < 46:52, 2.07 it/s, Epoch 0.44/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1946' max='1947' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1946/1947 01:13 < 00:00, 26.30 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Delete GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "# Train pre-trained model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./models/distilbert_sentences\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train *distilbert-base-uncased* on document subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data with a 70%, 15% and 15% ratio (train, valid, test)\n",
        "X = list(sub_document_df[\"document\"])\n",
        "y = list(sub_document_df[\"sentiment\"])\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3) # Split 70% train data\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5) # Split the other 30% in 50% each to get the correct ratio\n",
        "\n",
        "# Tokenize the datasets\n",
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
        "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
        "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Create the train, validation and test dataset as PyTorch datasets\n",
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(X_val_tokenized, y_val)\n",
        "test_dataset = Dataset(X_test_tokenized, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./evaluation/distilbert_documents\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=100,\n",
        "    gradient_accumulation_steps=4,\n",
        "    seed=0,\n",
        "    optim=\"adamw_torch\", # Use newer PyTorch optimizer\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=1,\n",
        "    fp16=True,\n",
        "    report_to='tensorboard')\n",
        "\n",
        "# Define Huggingface Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "# Train pre-trained model\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model 2: *roberta-base*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset # Load default methods, since a few are overwritten for destillbert\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31a09d951cb544c3947ef31dec6fb96c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/83050 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81e1888f78034e6081e40dab69fb5489",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/20762 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "def tokenize_and_format_sentence(examples):\n",
        "    # Tokenize the text\n",
        "    tokenized_inputs = tokenizer(examples['sentence'], truncation=True, padding='max_length')\n",
        "    # Map the sentiment score to the label\n",
        "    labels = examples['sentiment']\n",
        "    # Return both the tokenized inputs and labels\n",
        "    return {**tokenized_inputs, 'labels': labels}\n",
        "\n",
        "# Train, validation and test split (70%, 15% and 15%)\n",
        "train_dataset, temp_df = train_test_split(sentence_df, test_size=0.3, random_state=42)\n",
        "val_dataset, test_dataset = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert pandas DataFrame to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_dataset)\n",
        "val_dataset = Dataset.from_pandas(val_dataset)\n",
        "\n",
        "# Tokenizing the datasets\n",
        "train_dataset = train_dataset.map(tokenize_and_format_sentence, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_and_format_sentence, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4989c9bdaed74052b57cdbd3267124ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Set the format for PyTorch\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Load the base RoBERTa model\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1001' max='7785' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1001/7785 08:21 < 56:44, 1.99 it/s, Epoch 0.39/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2595' max='2596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2595/2596 01:41 < 00:00, 25.48 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Prepare to train the model\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./evaluation/roberta_sentences\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    gradient_accumulation_steps=4,\n",
        "    seed=0,\n",
        "    optim=\"adamw_torch\", # Use newer PyTorch optimizer\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    report_to='tensorboard')\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./models/roberta_sentences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c_FFqFrresiO"
      },
      "source": [
        "### Compare the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "labMxVVuesiO"
      },
      "outputs": [],
      "source": [
        "# Create torch dataset\n",
        "test_dataset = Dataset(X_test_tokenized)\n",
        "\n",
        "# Load trained model\n",
        "model_path =\"TBD\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
        "\n",
        "# Define test trainer\n",
        "test_trainer = Trainer(model)\n",
        "\n",
        "# Make predictions\n",
        "predictions = test_trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYcomVCfesiP"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9UZ_caPYX3jC"
      },
      "source": [
        "## Train, Dev, Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nw2PKk89psR"
      },
      "outputs": [],
      "source": [
        "# Split the DataFrame into train, dev, and test sets (70%, 15% and 15%)\n",
        "train_df, temp_df = train_test_split(sentence_df, test_size=0.3, random_state=42)\n",
        "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NIwomxvRY2Vm"
      },
      "source": [
        "## Tokenize Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBKy8NGC9pv6"
      },
      "outputs": [],
      "source": [
        "# Define the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize the sentences and convert them to input features\n",
        "def tokenize_sentences(sentences):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=100,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    \n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Tokenize the sentences in the train, dev, and test sets\n",
        "train_sentences = train_df['sentence'].tolist()\n",
        "dev_sentences = dev_df['sentence'].tolist()\n",
        "test_sentences = test_df['sentence'].tolist()\n",
        "train_inputs, train_masks = tokenize_sentences(train_sentences)\n",
        "dev_inputs, dev_masks = tokenize_sentences(dev_sentences)\n",
        "test_inputs, test_masks = tokenize_sentences(test_sentences)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lmwbJNB9Y6je"
      },
      "source": [
        "## Convert sentiment scores to tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osu0DVQk9py-"
      },
      "outputs": [],
      "source": [
        "# Convert the sentiment scores to tensors\n",
        "train_labels = torch.tensor(train_df['sentiment'].tolist())\n",
        "dev_labels = torch.tensor(dev_df['sentiment'].tolist())\n",
        "test_labels = torch.tensor(test_df['sentiment'].tolist())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FVN5NUY9Y98g"
      },
      "source": [
        "## Create DataLoader and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0z7gXieYrUr"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader for each set\n",
        "batch_size = 64\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "dev_data = TensorDataset(dev_inputs, dev_masks, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AF38wZp4ZFNG"
      },
      "source": [
        "## Train base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X_FceeT4Gtk"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "# Set the device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "print(f'\\n Selected device to run: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N25T9roqesiR"
      },
      "outputs": [],
      "source": [
        "# Small evaluation function\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        true_labels = labels.cpu().numpy()\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_accuracy = total_accuracy / len(dataloader)\n",
        "\n",
        "    return avg_loss, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFgSmwQpesiR"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNfe5S14esiR"
      },
      "outputs": [],
      "source": [
        "# Set the optimizer and parameter\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 2\n",
        "\n",
        "# Create the SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):        \n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch: {epoch+1}, Step: {step+1}, Loss: {loss.item()}')\n",
        "\n",
        "        # Log the loss and learning rate to TensorBoard\n",
        "        writer.add_scalar(\"Loss/train\", loss, step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            writer.add_scalar(\"Learning rate\", param_group['lr'], step)\n",
        "\n",
        "        # Log histograms of all model parameters\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                writer.add_histogram(name, param.data, step)\n",
        "       \n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    writer.add_scalar(\"Average Loss/train\", avg_train_loss, epoch)\n",
        "\n",
        "    # Evaluate on the validation set and log metrics to TensorBoard\n",
        "    avg_val_loss, avg_val_accuracy = evaluate(model, dev_dataloader)\n",
        "    writer.add_scalar(\"Average Loss/validation\", avg_val_loss, epoch)\n",
        "    writer.add_scalar(\"Average Accuracy/validation\", avg_val_accuracy, epoch)\n",
        "\n",
        "# After training\n",
        "trained_model = model\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hIe_QOR5eJw"
      },
      "outputs": [],
      "source": [
        "print(trained_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BKZ-EEVYHSMA"
      },
      "source": [
        "## Evaluate base model on dev set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHzx5Juv6Cek"
      },
      "outputs": [],
      "source": [
        "# Evaluation on the dev set\n",
        "trained_model.eval()\n",
        "dev_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dev_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, labels = batch\n",
        "\n",
        "        outputs = trained_model(inputs, attention_mask=masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.sigmoid(logits).squeeze().tolist()\n",
        "        dev_predictions.extend(predictions)\n",
        "\n",
        "    # Handle the remaining instances\n",
        "    if len(dev_predictions) < len(dev_labels):\n",
        "        remaining_instances = len(dev_labels) - len(dev_predictions)\n",
        "        last_batch_inputs = dev_inputs[-remaining_instances:]\n",
        "        last_batch_masks = dev_masks[-remaining_instances:]\n",
        "        last_batch = (last_batch_inputs, last_batch_masks)\n",
        "\n",
        "        last_batch = tuple(t.to(device) for t in last_batch)\n",
        "\n",
        "        outputs = trained_model(*last_batch, attention_mask=last_batch[1])\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.sigmoid(logits).squeeze().tolist()\n",
        "        dev_predictions.extend(predictions)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "# Define the thresholds for discretization\n",
        "thresholds = [1/3, 2/3]\n",
        "\n",
        "# Discretize the predicted probabilities\n",
        "discretized_predictions = np.digitize(dev_predictions, thresholds)\n",
        "\n",
        "# Convert continuous labels to integers\n",
        "dev_labels_int = np.digitize(dev_labels, thresholds)\n",
        "\n",
        "# Create the confusion matrix-like representation\n",
        "num_classes = len(thresholds) + 1  # Number of classes: below threshold, between thresholds, above threshold\n",
        "cm = np.zeros((num_classes, num_classes))\n",
        "\n",
        "for true_label, predicted_label in zip(dev_labels_int, discretized_predictions):\n",
        "    cm[true_label, predicted_label] += 1\n",
        "\n",
        "# Print the confusion matrix-like representation\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Get the values from the confusion matrix\n",
        "TP = cm[1, 1]\n",
        "FP = cm[0, 1] + cm[2, 1]\n",
        "FN = cm[1, 0] + cm[1, 2]\n",
        "TN = cm[0, 0] + cm[0, 2] + cm[2, 0] + cm[2, 2]\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Compute precision\n",
        "precision = TP / (TP + FP)\n",
        "\n",
        "# Compute recall\n",
        "recall = TP / (TP + FN)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy:\", accuracy)\n",
        "print(f\"Precision:\", precision)\n",
        "print(f\"Recall:\", recall)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MbEvy82OHW1B"
      },
      "source": [
        "## Parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVOT0rJt6CGi"
      },
      "outputs": [],
      "source": [
        "# Define list of parameters\n",
        "batch_sizes = [16]\n",
        "learning_rates = [5e-5, 1e-5]\n",
        "num_epochs_list = [1, 2]\n",
        "\n",
        "# Set results dict\n",
        "results = {\n",
        "    'batch_size': [],\n",
        "    'learning_rate': [],\n",
        "    'num_epochs': [],\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'confusion_matrix': []\n",
        "}\n",
        "\n",
        "# Iterate over parameter combinations\n",
        "for batch_size in batch_sizes:\n",
        "    for learning_rate in learning_rates:\n",
        "        for num_epochs in num_epochs_list:\n",
        "            # Train the model with the current hyperparameters\n",
        "           \n",
        "            # Load the pre-trained BERT model for sequence classification\n",
        "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "            # Set the device (GPU if available, else CPU)\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            print(device)\n",
        "            model = model.to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                for batch in train_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    inputs, masks, labels = batch\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    # Forward pass\n",
        "                    outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                    logits = outputs.logits\n",
        "                    \n",
        "                    # Backward pass and optimization\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluation on dev set\n",
        "            model.eval()\n",
        "            dev_predictions = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in dev_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    inputs, masks, labels = batch\n",
        "\n",
        "                    outputs = trained_model(inputs, attention_mask=masks)\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                    predictions = torch.sigmoid(logits).squeeze().tolist()\n",
        "                    dev_predictions.extend(predictions)\n",
        "\n",
        "                # Handle the remaining instances\n",
        "                if len(dev_predictions) < len(dev_labels):\n",
        "                    remaining_instances = len(dev_labels) - len(dev_predictions)\n",
        "                    last_batch_inputs = dev_inputs[-remaining_instances:]\n",
        "                    last_batch_masks = dev_masks[-remaining_instances:]\n",
        "                    last_batch = (last_batch_inputs, last_batch_masks)\n",
        "\n",
        "                    last_batch = tuple(t.to(device) for t in last_batch)\n",
        "\n",
        "                    outputs = trained_model(*last_batch, attention_mask=last_batch[1])\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                    predictions = torch.sigmoid(logits).squeeze().tolist()\n",
        "                    dev_predictions.extend(predictions)\n",
        "\n",
        "            # Calculate evaluation metrics\n",
        "            # Define the thresholds for discretization\n",
        "            thresholds = [1/3, 2/3]\n",
        "\n",
        "            # Discretize the predicted probabilities\n",
        "            discretized_predictions = np.digitize(dev_predictions, thresholds)\n",
        "\n",
        "            # Convert continuous labels to integers\n",
        "            dev_labels_int = np.digitize(dev_labels, thresholds)\n",
        "\n",
        "            # Create the confusion matrix-like representation\n",
        "            num_classes = len(thresholds) + 1  # Number of classes: below threshold, between thresholds, above threshold\n",
        "            cm = np.zeros((num_classes, num_classes))\n",
        "\n",
        "            for true_label, predicted_label in zip(dev_labels_int, discretized_predictions):\n",
        "                cm[true_label, predicted_label] += 1\n",
        "\n",
        "            # Print the confusion matrix-like representation\n",
        "            print(\"Confusion Matrix:\")\n",
        "            print(cm)\n",
        "\n",
        "            # Get the values from the confusion matrix\n",
        "            TP = cm[1, 1]\n",
        "            FP = cm[0, 1] + cm[2, 1]\n",
        "            FN = cm[1, 0] + cm[1, 2]\n",
        "            TN = cm[0, 0] + cm[0, 2] + cm[2, 0] + cm[2, 2]\n",
        "\n",
        "            # Compute accuracy\n",
        "            accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "            # Compute precision\n",
        "            precision = TP / (TP + FP)\n",
        "\n",
        "            # Compute recall\n",
        "            recall = TP / (TP + FN)\n",
        "\n",
        "            # Store the results in the dictionary\n",
        "            results['batch_size'].append(batch_size)\n",
        "            results['learning_rate'].append(learning_rate)\n",
        "            results['num_epochs'].append(num_epochs)\n",
        "            results['accuracy'].append(accuracy)\n",
        "            results['precision'].append(precision)\n",
        "            results['recall'].append(recall)\n",
        "            results['confusion_matrix'].append(cm)\n",
        "\n",
        "# Create results df out of results dictionary\n",
        "results_df = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj2tCoW1UONv"
      },
      "outputs": [],
      "source": [
        "# Display results dataframe\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5B7qsOqWl7b"
      },
      "outputs": [],
      "source": [
        "# Create subplots for each metric\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "metrics = ['accuracy', 'precision', 'recall']\n",
        "num_epochs = results_df['num_epochs'].unique().tolist()\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    \n",
        "    # Group the dataframe by batch_size and learning_rate\n",
        "    grouped_df = results_df.groupby(['batch_size', 'learning_rate'])\n",
        "    \n",
        "    # Iterate over the unique combinations\n",
        "    for (bs, lr), group in grouped_df:\n",
        "        # Get the metric values for the current combination\n",
        "        metric_values = group[metric].values\n",
        "        \n",
        "        # Plot the metric values as a line\n",
        "        ax.plot(num_epochs, metric_values, marker='o', label=f\"Batch sizes={bs}, LR={lr}\")\n",
        "\n",
        "    ax.set_xticks(num_epochs)\n",
        "    ax.set_xticklabels(num_epochs)\n",
        "    ax.set_xlabel(\"Number of epochs\")\n",
        "    ax.set_ylabel(metric.capitalize())\n",
        "    ax.set_title(metric.capitalize())\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t5s0kz7fRb1u"
      },
      "source": [
        "## Train final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJERVi3zHY03"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "# Set the device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the optimizer and parameter\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "num_epochs = 3\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKgkWEmmZX3_"
      },
      "outputs": [],
      "source": [
        "train_loss_values = []  # List to store training loss values\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0  # Variable to accumulate the loss for each epoch\n",
        "    \n",
        "    for batch in train_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, labels = batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()  # Accumulate the loss for the current batch\n",
        "        \n",
        "    # Calculate the average training loss for the epoch\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    \n",
        "    train_loss_values.append(avg_epoch_loss)  # Store the training loss value for the epoch\n",
        "    \n",
        "    # Print the training loss for the epoch\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "# After training\n",
        "final_model = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-c1raqeKG0M"
      },
      "outputs": [],
      "source": [
        "print(final_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2HgRBEyWRxBJ"
      },
      "source": [
        "## Evaluate final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoo9jrMgKGuf"
      },
      "outputs": [],
      "source": [
        "# Evaluation on the test set\n",
        "final_model.eval()\n",
        "test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, labels = batch\n",
        "\n",
        "        outputs = final_model(inputs, attention_mask=masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.sigmoid(logits).squeeze().tolist()\n",
        "        test_predictions.extend(predictions)\n",
        "\n",
        "    # Handle the remaining instances\n",
        "    if len(test_predictions) < len(test_labels):\n",
        "        remaining_instances = len(test_labels) - len(test_predictions)\n",
        "        last_batch_inputs = test_inputs[-remaining_instances:]\n",
        "        last_batch_masks = test_masks[-remaining_instances:]\n",
        "        last_batch = (last_batch_inputs, last_batch_masks)\n",
        "\n",
        "        last_batch = tuple(t.to(device) for t in last_batch)\n",
        "\n",
        "        outputs = final_model(*last_batch, attention_mask=last_batch[1])\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.sigmoid(logits).squeeze().tolist()\n",
        "        test_predictions.extend(predictions)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "# Define the thresholds for discretization\n",
        "thresholds = [1/3, 2/3]\n",
        "\n",
        "# Discretize the predicted probabilities\n",
        "discretized_predictions = np.digitize(test_predictions, thresholds)\n",
        "\n",
        "# Convert continuous labels to integers\n",
        "test_labels_int = np.digitize(test_labels, thresholds)\n",
        "\n",
        "# Create the confusion matrix-like representation\n",
        "num_classes = len(thresholds) + 1  # Number of classes: below threshold, between thresholds, above threshold\n",
        "cm = np.zeros((num_classes, num_classes))\n",
        "\n",
        "for true_label, predicted_label in zip(test_labels_int, discretized_predictions):\n",
        "    cm[true_label, predicted_label] += 1\n",
        "\n",
        "# Print the confusion matrix-like representation\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Get the values from the confusion matrix\n",
        "TP = cm[1, 1]\n",
        "FP = cm[0, 1] + cm[2, 1]\n",
        "FN = cm[1, 0] + cm[1, 2]\n",
        "TN = cm[0, 0] + cm[0, 2] + cm[2, 0] + cm[2, 2]\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Compute precision\n",
        "precision = TP / (TP + FP)\n",
        "\n",
        "# Compute recall\n",
        "recall = TP / (TP + FN)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy:\", accuracy)\n",
        "print(f\"Precision:\", precision)\n",
        "print(f\"Recall:\", recall)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rZAVy-vjSl-c"
      },
      "source": [
        "## Annotate sentiments with final prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Yd7jyUcBH5"
      },
      "outputs": [],
      "source": [
        "# Define the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize the sentences and convert them to input features\n",
        "def tokenize_sentences(sentences):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=100,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    \n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
        "\n",
        "def make_predictions(tokenized_sentences):\n",
        "  inputs, masks = tokenized_sentences\n",
        "\n",
        "  # Prepare the data for model input\n",
        "  inputs = inputs.to(device)\n",
        "  masks = masks.to(device)\n",
        "\n",
        "  # Evaluate the model on the dataframe\n",
        "  final_model.eval()\n",
        "  sentiment_predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for i in range(len(inputs)):\n",
        "          input_ids = inputs[i].unsqueeze(0)\n",
        "          attention_mask = masks[i].unsqueeze(0)\n",
        "          \n",
        "          outputs = final_model(input_ids, attention_mask=attention_mask)\n",
        "          logits = outputs.logits\n",
        "          \n",
        "          predictions = torch.sigmoid(logits).squeeze().tolist()\n",
        "          sentiment_predictions.append(predictions)\n",
        "  return sentiment_predictions\n",
        "\n",
        "subset_df['tokenized_sentences'] = subset_df['sentence_tokens'].apply(lambda x: tokenize_sentences(x))\n",
        "subset_df['sentiments'] = subset_df['tokenized_sentences'].apply(lambda x: make_predictions(x))\n",
        "subset_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rK7BZAfyx_rY"
      },
      "source": [
        "## Compare internal vs. external"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsYNhfC7qqSl"
      },
      "outputs": [],
      "source": [
        "# Compute the average of each list in the 'sentiments' column\n",
        "subset_df['sentiments_avg'] = subset_df['sentiments'].apply(lambda x: np.mean(x))\n",
        "\n",
        "# Create two separate dataframes for internal values 0 and 1\n",
        "subset_internal_0 = subset_df[subset_df['internal'] == 0]\n",
        "subset_internal_1 = subset_df[subset_df['internal'] == 1]\n",
        "\n",
        "# Create boxplots for average sentiments grouped by internal values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='internal', y='sentiments_avg', data=subset_df)\n",
        "plt.xlabel('Internal')\n",
        "plt.ylabel('Average Sentiments')\n",
        "plt.title('Boxplot of Average Sentiments by Internal')\n",
        "plt.show()\n",
        "\n",
        "# Create histograms for average sentiments grouped by internal values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data=subset_df, x='sentiments_avg', hue='internal', element='step', bins=10, alpha=0.5, legend=True)\n",
        "plt.xlabel('Average Sentiments')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Average Sentiments by Internal')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
