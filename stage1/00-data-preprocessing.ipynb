{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import contractions\n",
    "from langdetect import detect \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# Download a spacy model, can also be adjusted (medium = en_core_web_md, large = en_core_web_lg)\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_csv('../data/esg_documents_for_dax_companies.csv', delimiter = '|', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beiersdorf AG</td>\n",
       "      <td>Sustainability Highlight Report CARE BEYOND SK...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'GHGEmission', 'ProductLiabilit...</td>\n",
       "      <td>1</td>\n",
       "      <td>BEI</td>\n",
       "      <td>BeiersdorfAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>Corporate Responsibility Report 2021 2 Content...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DTE</td>\n",
       "      <td>DeutscheTelekomAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vonovia SE</td>\n",
       "      <td>VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'DataSecurity', 'Vaccine', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>VNA</td>\n",
       "      <td>VonoviaSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merck KGaA</td>\n",
       "      <td>Sustainability Report 2021 TABLE OF CONTENTS S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'DataMisuse', 'DrugResistance...</td>\n",
       "      <td>1</td>\n",
       "      <td>MRK</td>\n",
       "      <td>MerckKGaA Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTU</td>\n",
       "      <td>Our ideas and concepts FOR A SUSTAINABLE FUTUR...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'AirQuality'...</td>\n",
       "      <td>1</td>\n",
       "      <td>MTX</td>\n",
       "      <td>MTUAeroEngines Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>#StandWithUkraine Sustainability Report 2021 C...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>E.ONSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RWE AG</td>\n",
       "      <td>Focus on tomorrow. Sustainability Report 2021 ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'Iso50001', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>RWE</td>\n",
       "      <td>RWEAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Annual Report 2021 HeidelbergCement at a glanc...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Vaccine', 'DataSecurity',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Annual Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Company Strategy &amp; Business &amp; Product &amp; Produc...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'Corruption', 'Whistleblowing',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>Sustainability 1 Siemens 2 Our 3 Governance – ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'EmployeeTurnover...</td>\n",
       "      <td>1</td>\n",
       "      <td>SIE</td>\n",
       "      <td>SiemensAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company                                            content   \n",
       "0         Beiersdorf AG  Sustainability Highlight Report CARE BEYOND SK...  \\\n",
       "1   Deutsche Telekom AG  Corporate Responsibility Report 2021 2 Content...   \n",
       "2            Vonovia SE  VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...   \n",
       "3            Merck KGaA  Sustainability Report 2021 TABLE OF CONTENTS S...   \n",
       "4                   MTU  Our ideas and concepts FOR A SUSTAINABLE FUTUR...   \n",
       "5                E ONSE  #StandWithUkraine Sustainability Report 2021 C...   \n",
       "6                RWE AG  Focus on tomorrow. Sustainability Report 2021 ...   \n",
       "7  Heidelberg Cement AG  Annual Report 2021 HeidelbergCement at a glanc...   \n",
       "8  Heidelberg Cement AG  Company Strategy & Business & Product & Produc...   \n",
       "9            Siemens AG  Sustainability 1 Siemens 2 Our 3 Governance – ...   \n",
       "\n",
       "                datatype        date domain   \n",
       "0  sustainability_report  2021-03-31    NaN  \\\n",
       "1  sustainability_report  2021-03-31    NaN   \n",
       "2  sustainability_report  2021-03-31    NaN   \n",
       "3  sustainability_report  2021-03-31    NaN   \n",
       "4  sustainability_report  2020-03-31    NaN   \n",
       "5  sustainability_report  2021-03-31    NaN   \n",
       "6  sustainability_report  2021-03-31    NaN   \n",
       "7          annual_report  2021-03-31    NaN   \n",
       "8  sustainability_report  2020-03-31    NaN   \n",
       "9  sustainability_report  2020-03-31    NaN   \n",
       "\n",
       "                                          esg_topics  internal symbol   \n",
       "0  ['CleanWater', 'GHGEmission', 'ProductLiabilit...         1    BEI  \\\n",
       "1  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1    DTE   \n",
       "2  ['Whistleblowing', 'DataSecurity', 'Vaccine', ...         1    VNA   \n",
       "3  ['DataSecurity', 'DataMisuse', 'DrugResistance...         1    MRK   \n",
       "4  ['WorkLifeBalance', 'Corruption', 'AirQuality'...         1    MTX   \n",
       "5  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1   EOAN   \n",
       "6  ['WorkLifeBalance', 'Corruption', 'Iso50001', ...         1    RWE   \n",
       "7  ['WorkLifeBalance', 'Vaccine', 'DataSecurity',...         1    HEI   \n",
       "8  ['CleanWater', 'Corruption', 'Whistleblowing',...         1    HEI   \n",
       "9  ['DataSecurity', 'Iso50001', 'EmployeeTurnover...         1    SIE   \n",
       "\n",
       "                                           title  url  \n",
       "0        BeiersdorfAG Sustainability Report 2021  NaN  \n",
       "1   DeutscheTelekomAG Sustainability Report 2021  NaN  \n",
       "2           VonoviaSE Sustainability Report 2021  NaN  \n",
       "3           MerckKGaA Sustainability Report 2021  NaN  \n",
       "4      MTUAeroEngines Sustainability Report 2020  NaN  \n",
       "5              E.ONSE Sustainability Report 2021  NaN  \n",
       "6               RWEAG Sustainability Report 2021  NaN  \n",
       "7          HeidelbergCementAG Annual Report 2021  NaN  \n",
       "8  HeidelbergCementAG Sustainability Report 2020  NaN  \n",
       "9           SiemensAG Sustainability Report 2020  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded data and reset index\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column descriptions**\n",
    "- symbol: stock symbol of the company\n",
    "- company: company name\n",
    "- date: publication date of document\n",
    "- title: document title\n",
    "- content: document content\n",
    "- datatype: document type\n",
    "- internal: is this a report by company (1) or a third-party document (0)\n",
    "- domain (optional): Web domain where the document was published\n",
    "- url (optional): URL where the document can be accessed\n",
    "- esg_topics (optional): ESG topics extracted from the data using our internal NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11188, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape (row and column amount)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company       object\n",
       "content       object\n",
       "datatype      object\n",
       "date          object\n",
       "domain        object\n",
       "esg_topics    object\n",
       "internal       int64\n",
       "symbol        object\n",
       "title         object\n",
       "url           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "raw_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small checkpoint function to save intermediary processing steps and enhance development\n",
    "def csv_checkpoint(df, filename='checkpoint'):\n",
    "\n",
    "    if not os.path.exists('../data/checkpoints/'):\n",
    "        os.makedirs('../data/checkpoints/')\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(f'../data/checkpoints/{filename}.csv', index=False, sep = '|')\n",
    "    print(f'Saved DataFrame to {filename}.csv')\n",
    "    \n",
    "    # Load CSV back into DataFrame\n",
    "    df = pd.read_csv(f'../data/checkpoints/{filename}.csv', delimiter = '|')\n",
    "    print(f'Loaded DataFrame from {filename}.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As initial data cleaning steps, the following is conducted:\n",
    "- Rows with missing \"content\" were dropped to prevent any missing data-related issues. Missing data can create gaps in the data and lead to errors or distortions in the analysis.\n",
    "- The \"URL\" column was removed as the relevant information was available in the \"domain\" column. Removing redundant columns simplifies the data set and makes it easier to work with\n",
    "- Duplicate entries were identified and removed, resulting in a cleaner and more concise dataset. Duplicates can distort the data and lead to biased analysis. \n",
    "- Language checking was conducted and all rows with non-English content were dropped to ensure consistent language. Language inconsistencies can create bias in the data and lead to inaccurate conclusions. Therefore, it is important to ensure that the data is consistent in language to prevent linguistic biases.\n",
    "- \"Date\" is formatted as a date and wrong dates, e.g. \"bayer-03-31\" are replaced with a default date (2023-03-31).\n",
    "- Remove company name parts like \"AG\" for clarity\n",
    "- The \"sample\" method was used to check the data for representativeness and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_cleaned_data = raw_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"url\" column, since the most relevant information from an analysis perspective is already in the \"domain\" column (e.g. the source of the report)\n",
    "general_cleaned_data = general_cleaned_data.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 6\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and delete them\n",
    "duplicates = general_cleaned_data[general_cleaned_data.duplicated()]\n",
    "print(f'Duplicated rows: {len(duplicates)}')\n",
    "general_cleaned_data = general_cleaned_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted amount of rows with language other ehan English: 107\n"
     ]
    }
   ],
   "source": [
    "# Check for other languange than English\n",
    "general_cleaned_data['language'] = general_cleaned_data['content'].apply(lambda x: detect(x))\n",
    "not_english = len(general_cleaned_data) - len(general_cleaned_data.loc[general_cleaned_data['language'] == 'en'])\n",
    "\n",
    "# Drop rows with other languange, since other languanges influences to quality of the later analysis\n",
    "general_cleaned_data = general_cleaned_data.loc[general_cleaned_data['language'] == 'en']\n",
    "\n",
    "print(f'Deleted amount of rows with language other ehan English: {not_english}')\n",
    "general_cleaned_data.drop(['language'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly formatted dates:\n",
      "Row 13: p.DE-03-31\n",
      "Row 18: p.DE-03-31\n",
      "Row 20: bayer-03-31\n",
      "Row 22: p.DE-03-31\n",
      "Row 25: p.DE-03-31\n",
      "Row 26: p.DE-03-31\n",
      "Row 31: p.DE-03-31\n",
      "Row 32: p.DE-03-31\n",
      "Row 33: p.DE-03-31\n",
      "Row 37: p.DE-03-31\n",
      "Row 41: p.DE-03-31\n",
      "Row 50: p.DE-03-31\n",
      "Row 78: p.DE-03-31\n",
      "Row 80: p.DE-03-31\n",
      "Row 86: p.DE-03-31\n",
      "Row 87: p.DE-03-31\n",
      "Row 88: p.DE-03-31\n"
     ]
    }
   ],
   "source": [
    "# Correct the dates to ISO standard\n",
    "def find_incorrect_dates(data):\n",
    "    incorrect_dates = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            pd.to_datetime(row['date'], format='%Y-%m-%d', errors='raise')\n",
    "        except ValueError:\n",
    "            incorrect_dates.append((index, row['date']))\n",
    "\n",
    "    return incorrect_dates\n",
    "\n",
    "incorrect_date_rows = find_incorrect_dates(general_cleaned_data)\n",
    "print(\"Incorrectly formatted dates:\")\n",
    "for index, date in incorrect_date_rows:\n",
    "    print(f\"Row {index}: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the wrong formatted dates and set default date\n",
    "def correct_date_format(data):\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce').fillna('2022-03-31')\n",
    "    return data\n",
    "\n",
    "general_cleaned_data = correct_date_format(general_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace company name parts like \"AG\" to have a cleaner name\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' AG', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' SE', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' KGaA', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>Beiersdorf</td>\n",
       "      <td>A group of 36 major personal care and cosmetic...</td>\n",
       "      <td>esg</td>\n",
       "      <td>2022-02-23</td>\n",
       "      <td>esgtoday</td>\n",
       "      <td>['Transparency']</td>\n",
       "      <td>0</td>\n",
       "      <td>BEI</td>\n",
       "      <td>Consumer Products Giants Partner to Launch Imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Adidas</td>\n",
       "      <td>Experience this story and others in the new is...</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>highsnobiety</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>ADS</td>\n",
       "      <td>The Shade Is Real: Gear Testing the Best Cycli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>Bayer</td>\n",
       "      <td>CHICAGO, Nov. 29, 2021 ( GLOBE NEWSWIRE) -- Mo...</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-11-29</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['HumanCapital']</td>\n",
       "      <td>0</td>\n",
       "      <td>BAYN</td>\n",
       "      <td>Mondelēz International Appoints Ertharin Cousi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6024</th>\n",
       "      <td>Infineon Technologies</td>\n",
       "      <td>Instilling a sense of confidence and trust amo...</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-05-16</td>\n",
       "      <td>thefintechtimes</td>\n",
       "      <td>['Fraud', 'DataSecurity', 'UnbankedPopulation'...</td>\n",
       "      <td>0</td>\n",
       "      <td>IFX</td>\n",
       "      <td>Financial Inclusion Is Nothing Without Securit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5053</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>The information you requested is not available...</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-01-27</td>\n",
       "      <td>bnnbloomberg</td>\n",
       "      <td>['Privacy']</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>Deutsche Telekom Weighs Tower Tie-Up With Voda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company   \n",
       "2837             Beiersdorf  \\\n",
       "230                  Adidas   \n",
       "2457                  Bayer   \n",
       "6024  Infineon Technologies   \n",
       "5053       Deutsche Telekom   \n",
       "\n",
       "                                                content  datatype       date   \n",
       "2837  A group of 36 major personal care and cosmetic...       esg 2022-02-23  \\\n",
       "230   Experience this story and others in the new is...   general 2022-03-25   \n",
       "2457  CHICAGO, Nov. 29, 2021 ( GLOBE NEWSWIRE) -- Mo...  business 2021-11-29   \n",
       "6024  Instilling a sense of confidence and trust amo...  business 2021-05-16   \n",
       "5053  The information you requested is not available...  business 2022-01-27   \n",
       "\n",
       "               domain                                         esg_topics   \n",
       "2837         esgtoday                                   ['Transparency']  \\\n",
       "230      highsnobiety                                                 []   \n",
       "2457   marketscreener                                   ['HumanCapital']   \n",
       "6024  thefintechtimes  ['Fraud', 'DataSecurity', 'UnbankedPopulation'...   \n",
       "5053     bnnbloomberg                                        ['Privacy']   \n",
       "\n",
       "      internal symbol                                              title  \n",
       "2837         0    BEI  Consumer Products Giants Partner to Launch Imp...  \n",
       "230          0    ADS  The Shade Is Real: Gear Testing the Best Cycli...  \n",
       "2457         0   BAYN  Mondelēz International Appoints Ertharin Cousi...  \n",
       "6024         0    IFX  Financial Inclusion Is Nothing Without Securit...  \n",
       "5053         0    DTE  Deutsche Telekom Weighs Tower Tie-Up With Voda...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the date with some samples\n",
    "general_cleaned_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of \"Muenchener Rueckversicherungs Gesellschaft AGin Muenchen\" to something more readable\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].replace('Muenchener Rueckversicherungs Gesellschaftin Muenchen', 'Munich RE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to general_cleaned_data.csv\n",
      "Loaded DataFrame from general_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "general_cleaned_data = csv_checkpoint(general_cleaned_data, 'general_cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning & Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"content\" column, containing the text of the reports, undergoes a series of cleaning, normalization, and preprocessing steps to ensure accurate and efficient analysis. These steps include:\n",
    "\n",
    "- **String conversion**: Converting the input to a string format ensures consistency and compatibility during subsequent processing tasks.\n",
    "- **Lowercase conversion**: Transforming all text to lowercase serves as a simple normalization step, reducing the complexity and variability of the input data.\n",
    "- **Unicode decoding**: Removing diacritics (e.g., accented characters) and normalizing the text encoding mitigates potential discrepancies arising from different encoding formats.\n",
    "- **URL and email address removal**: Eliminating URLs and email addresses reduces noise in the dataset, as these elements do not contribute valuable information for the analysis.\n",
    "- **Extra whitespace removal**: Eradicating extra whitespaces improves text analysis and tokenization by ensuring that only meaningful spaces are retained.\n",
    "- **Contact detail removal**: Excluding phone numbers, contact person strings, and social media references further minimizes noise in the dataset, honing the focus on relevant text.\n",
    "- **Table of contents removal**: Discarding the table of contents enhances the data quality by eliminating repetitive and non-essential information.\n",
    "- **Named entity removal**: Employing the spaCy model to remove human names and other named entities optimizes the text for analysis and modeling by concentrating on pertinent content.\n",
    "- **Abbreviation expansion**: Utilizing the contractions library and custom functions with regular expressions, common and uncommon abbreviations are expanded to improve text interpretation.\n",
    "- **Special character elimination**: Excluding all special characters, except punctuation, refines the input data. Retaining punctuation is necessary for accurate sentence tokenization and removed after sentence tokenization..\n",
    "- **Tokenization and lemmatization**: Tokenizing words and sentences, and subsequently lemmatizing words using the WordNetLemmatizer from nltk, streamlines the text and reduces morphological variations.\n",
    "- **Stopword removal**: Customizing the nltk stopwords list by adding or removing specific stopwords enables more precise and tailored text analysis.\n",
    "- **Part-of-speech (POS) tagging**: Assigning POS tags to words and sentences enhances the text representation by providing additional linguistic information, which may be beneficial for subsequent analysis and modeling tasks.  \n",
    "\n",
    "Spellchecking was tested with TextBlob and PySpellChecker but deliverd not useful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = general_cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name entities removed: 94694\n"
     ]
    }
   ],
   "source": [
    "# Since the spacy model shows better results on the \"raw\" text, the named entity removal is conducted before all normalization and cleaning steps\n",
    "spacy_model = spacy.load('en_core_web_md')\n",
    "spacy_model.max_length = 1800000 # Increase max text length\n",
    "\n",
    "def remove_named_entities(text):\n",
    "    doc = spacy_model(text)\n",
    "    \n",
    "    named_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\"]:\n",
    "            named_entities.add(ent.text)\n",
    "    \n",
    "    named_entities_count = len(named_entities)\n",
    "    \n",
    "    for named_entity in named_entities:\n",
    "        text = text.replace(named_entity, '')\n",
    "    \n",
    "    return text, named_entities_count\n",
    "\n",
    "# Assuming cleaned_data is a pandas DataFrame with a 'content' column\n",
    "cleaned_data['cleaned_content'], name_entity_count = zip(*cleaned_data['content'].apply(remove_named_entities))\n",
    "print(\"Name entities removed:\", sum(name_entity_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs removed: 7487\n",
      "Mail addresses removed: 434\n",
      "Extra whitespaces removed: 148978\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    urls = re.findall(r'http\\S+|www\\S+|https\\S+', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE), len(urls)\n",
    "\n",
    "def remove_emails(text):\n",
    "    mail_addresses = re.findall(r'\\S+@\\S+\\s?', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'\\S+@\\S+\\s?', '', text, flags=re.MULTILINE), len(mail_addresses)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    extra_spaces = re.findall(r'\\s{2,}', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip(), len(extra_spaces)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].astype(str) # Convert all texts to string\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: x.lower()) # Convert all texts to lower-case\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: unidecode(x, errors=\"preserve\")) # Remove diacritics / accented characters and unicode normalization\n",
    "cleaned_data['cleaned_content'], url_count = zip(*cleaned_data['cleaned_content'].apply(remove_urls)) # Remove URLs from texts\n",
    "cleaned_data['cleaned_content'], email_count = zip(*cleaned_data['cleaned_content'].apply(remove_emails)) # Remove e-mail addresses from texts\n",
    "cleaned_data['cleaned_content'], extra_space_count = zip(*cleaned_data['cleaned_content'].apply(remove_extra_whitespace)) # Remove extra whitespaces from texts\n",
    "\n",
    "print(\"URLs removed:\", sum(url_count))\n",
    "print(\"Mail addresses removed:\", sum(email_count))\n",
    "print(\"Extra whitespaces removed:\", sum(extra_space_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact information removed: 44160\n",
      "TOCs removed: 9603\n"
     ]
    }
   ],
   "source": [
    "def remove_contact_details(text):\n",
    "    # Remove phone numbers\n",
    "    phone_regex = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
    "    phone_count = len(re.findall(phone_regex, text))\n",
    "    text = re.sub(phone_regex, '', text)\n",
    "\n",
    "    # Remove common contact-related phrases\n",
    "    contact_phrases_regex = r'\\b(?:Contact Person|Phone|Tel|Fax|Mobile|E?mail|Skype|Twitter|Facebook|LinkedIn|Website):\\b'\n",
    "    contact_phrases_count = len(re.findall(contact_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(contact_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    total_count = phone_count + contact_phrases_count\n",
    "    return text, total_count\n",
    "\n",
    "def remove_table_of_contents(text):\n",
    "    # Remove common table of contents phrases\n",
    "    toc_phrases_regex = r'\\b(?:Table of Contents|Contents)\\b'\n",
    "    toc_phrases_count = len(re.findall(toc_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(toc_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove content with numbering like \"1. Introduction\", \"1.1. Background\", \"A. Overview\", etc.\n",
    "    toc_entries_regex = r'(^|\\n)\\s*\\w+(\\.\\w+)*\\s+\\w+([\\w\\s]+)?'\n",
    "    toc_entries_count = len(re.findall(toc_entries_regex, text))\n",
    "    text = re.sub(toc_entries_regex, '', text)\n",
    "\n",
    "    total_count = toc_phrases_count + toc_entries_count\n",
    "    return text, total_count\n",
    "\n",
    "cleaned_data['cleaned_content'], contact_count = zip(*cleaned_data['cleaned_content'].apply(remove_contact_details))\n",
    "cleaned_data['cleaned_content'], toc_count = zip(*cleaned_data['cleaned_content'].apply(remove_table_of_contents))\n",
    "print(\"Contact information removed:\", sum(contact_count))\n",
    "print(\"TOCs removed:\", sum(toc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = []\n",
    "    for word in text.split():\n",
    "        expanded_text.append(contractions.fix(word))\n",
    "    expanded_text = ' '.join(expanded_text)\n",
    "    return contractions.fix(expanded_text)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded custom abbreviations: 0\n"
     ]
    }
   ],
   "source": [
    "# Expand custom abbreviations which are not captured by \"contractions\"\n",
    "# Basic idea from: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# Compile the regular expressions only once for efficiency\n",
    "specific_patterns = [\n",
    "    (re.compile(r\"won['’]t\"), \"will not\"),\n",
    "    (re.compile(r\"can['’]t\"), \"can not\"),\n",
    "]\n",
    "\n",
    "def decontracted(phrase):\n",
    "    count = 0\n",
    "\n",
    "    # Replace specific patterns\n",
    "    for pattern, replacement in specific_patterns:\n",
    "        matches = len(pattern.findall(phrase))\n",
    "        count += matches\n",
    "        phrase = pattern.sub(replacement, phrase)\n",
    "\n",
    "    return phrase, count\n",
    "\n",
    "# Apply the function to expand abbreviations\n",
    "cleaned_data['cleaned_content'], abbreviation_counts = zip(*cleaned_data['cleaned_content'].apply(decontracted))\n",
    "print(\"Expanded custom abbreviations:\", sum(abbreviation_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters excl. punctuation removed: 1387193\n"
     ]
    }
   ],
   "source": [
    "# Remove special characters excl. punctuation since this is needed by the sentence tokenization\n",
    "def remove_non_alphanumeric(text):\n",
    "    special_chars = re.findall(r'[^a-zA-Z0-9\\s.,!?\\'\"]', text)\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"]', ' ', text), len(special_chars)\n",
    "\n",
    "cleaned_data['cleaned_content'], special_char_count = zip(*cleaned_data['cleaned_content'].apply(remove_non_alphanumeric))\n",
    "print(\"Special characters excl. punctuation removed:\", sum(special_char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word token amount: 17274365\n"
     ]
    }
   ],
   "source": [
    "def tokenize_words(text):\n",
    "    # Remove numbers, digits, and punctuation\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['word_tokens'], word_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_words))\n",
    "print(\"Generated word token amount:\", sum(word_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence token amount: 712214\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(text):\n",
    "    # Tokenize sentences\n",
    "    tokens = sent_tokenize(text)\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['sentence_tokens'], sentence_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_sentences))\n",
    "print(\"Generated sentence token amount:\", sum(sentence_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stopwords in word tokens 6687477\n",
      "Removed stopwords in sentence tokens 7701752\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_word_tokens(tokens, custom_stopwords):\n",
    "\n",
    "    # Remove stopwords from tokens\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in custom_stopwords]\n",
    "    \n",
    "    return filtered_tokens, len(tokens) - len(filtered_tokens)\n",
    "\n",
    "def remove_stopwords_from_sentence_tokens(sentences_list, custom_stopwords):\n",
    "    filtered_sentences_list = []\n",
    "    total_removed_items_count = 0\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        # Tokenize the sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords, digits, numbers, dates, and punctuation from word tokens\n",
    "        filtered_word_tokens = [\n",
    "            token for token in word_tokens\n",
    "            if token.lower() not in custom_stopwords\n",
    "            and not re.search(r'\\d', token)  # Remove tokens containing digits\n",
    "            ]\n",
    "\n",
    "        # Remove remaining special characters from sentences, i.e. punctuation\n",
    "        filtered_word_tokens = [\n",
    "            re.sub(rf\"[{re.escape(string.punctuation)}]\", '', token) for token in filtered_word_tokens\n",
    "            ]\n",
    "\n",
    "        # Reconstruct the sentence without the removed words and special characters\n",
    "        filtered_sentence = ' '.join(filtered_word_tokens)\n",
    "        removed_items_count = len(word_tokens) - len(filtered_word_tokens)\n",
    "\n",
    "        filtered_sentences_list.append(filtered_sentence)\n",
    "        total_removed_items_count += removed_items_count\n",
    "\n",
    "    return filtered_sentences_list, total_removed_items_count\n",
    "\n",
    "# Define custom stopwords to add or remove\n",
    "custom_stopwords = {\n",
    "    'add': ['said','company','companies','year','billion','million','siemens','linde','rwe','volkswagen','symrise','porsche','sap','adidas','puma','airbus','bmw','hannover','mtu','heiderbergcement','qiagen','benz','continental','bayer','fresenius'],\n",
    "    'remove': [''] # Currently not needed anymore\n",
    "}\n",
    "\n",
    "# Combine stopwords to filter the content of the reports\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "all_stopwords |= set(custom_stopwords['add'])\n",
    "all_stopwords -= set(custom_stopwords['remove'])\n",
    "\n",
    "cleaned_data['word_tokens'], stopword_count_words = zip(*cleaned_data['word_tokens'].apply(remove_stopwords_from_word_tokens, custom_stopwords=all_stopwords))\n",
    "cleaned_data['sentence_tokens'], stopword_count_sentences = zip(*cleaned_data['sentence_tokens'].apply(remove_stopwords_from_sentence_tokens, custom_stopwords=all_stopwords))\n",
    "\n",
    "print(\"Removed stopwords in word tokens\", sum(stopword_count_words))\n",
    "print(\"Removed stopwords in sentence tokens\", sum(stopword_count_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_tokens(word_tokens, sentence_list):\n",
    "    # POS tagging for word tokens\n",
    "    pos_tagged_word_tokens = nltk.pos_tag(word_tokens)\n",
    "\n",
    "    # Create a dictionary to map word tokens to their POS tags, this reduces the effort to call nltk.pos_tag twice\n",
    "    pos_tags_dict = dict(pos_tagged_word_tokens)\n",
    "\n",
    "    # POS tagging for sentence tokens\n",
    "    pos_tagged_sentence_list = []\n",
    "    for sentence in sentence_list:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = [(token, pos_tags_dict[token]) for token in tokenized_sentence if token in pos_tags_dict]\n",
    "        pos_tagged_sentence_list.append(pos_tagged_sentence)\n",
    "\n",
    "    return pos_tagged_word_tokens, pos_tagged_sentence_list\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = cleaned_data.apply(lambda row: pos_tagging_tokens(row['word_tokens'], row['sentence_tokens']), axis=1)\n",
    "cleaned_data['pos_tagged_word_tokens'], cleaned_data['pos_tagged_sentence_tokens'] = zip(*pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>Continental</td>\n",
       "      <td>CHICAGO, Aug. 25, 2022 /PRNewswire/ -- The glo...</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-08-25</td>\n",
       "      <td>prnewswire.co</td>\n",
       "      <td>['EMobility', 'ValueChain']</td>\n",
       "      <td>0</td>\n",
       "      <td>CON</td>\n",
       "      <td>Automotive Valves Market worth $ 28.2 billion ...</td>\n",
       "      <td>chicago, august 25, 2022  prnewswire     the g...</td>\n",
       "      <td>[chicago, august, prnewswire, global, automoti...</td>\n",
       "      <td>[chicago  august  prnewswire global automotive...</td>\n",
       "      <td>[(chicago, RB), (august, JJ), (prnewswire, NN)...</td>\n",
       "      <td>[[(chicago, RB), (august, JJ), (prnewswire, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>Infineon Technologies</td>\n",
       "      <td>The Czech koruna rises as the country's centra...</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['RenewableEnergy', 'RussianFederation']</td>\n",
       "      <td>0</td>\n",
       "      <td>IFX</td>\n",
       "      <td>EUROPEAN MIDDAY BRIEFING - Stocks Lower Ahead ...</td>\n",
       "      <td>'s central bank is expected to raise interest ...</td>\n",
       "      <td>[central, bank, expected, raise, interest, rat...</td>\n",
       "      <td>[s central bank expected raise interest rates ...</td>\n",
       "      <td>[(central, JJ), (bank, NN), (expected, VBD), (...</td>\n",
       "      <td>[[(central, JJ), (bank, NN), (expected, VBN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Beiersdorf</td>\n",
       "      <td>CARE BEYOND SKIN03 Foreword05 Interview07 | Ou...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['RoundtableOnSustainablePalmOil', 'CleanWater...</td>\n",
       "      <td>1</td>\n",
       "      <td>BEI</td>\n",
       "      <td>BeiersdorfAG Sustainability Report 2020</td>\n",
       "      <td>our commitments08 overview of the consumer b...</td>\n",
       "      <td>[commitments08, overview, consumer, business, ...</td>\n",
       "      <td>[overview consumer business sustainability age...</td>\n",
       "      <td>[(commitments08, NN), (overview, JJ), (consume...</td>\n",
       "      <td>[[(overview, VBZ), (consumer, NN), (business, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>Bayer</td>\n",
       "      <td>Request Here Sample Report Buy This Complete B...</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>ecochunk</td>\n",
       "      <td>['ValueChain']</td>\n",
       "      <td>0</td>\n",
       "      <td>BAYN</td>\n",
       "      <td>Fungal Otitis Externa Market Will Watch a Sens...</td>\n",
       "      <td>. the research covers a valuable source of per...</td>\n",
       "      <td>[research, cover, valuable, source, perceptive...</td>\n",
       "      <td>[, research covers valuable source perceptive ...</td>\n",
       "      <td>[(research, NN), (cover, NN), (valuable, JJ), ...</td>\n",
       "      <td>[[], [(research, NN), (valuable, JJ), (source,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Qiagen</td>\n",
       "      <td>BackgroundWe aimed to examine the immunogenici...</td>\n",
       "      <td>thinktank</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>thelancet</td>\n",
       "      <td>['Social', 'GenderDiversity', 'PatientSafety',...</td>\n",
       "      <td>0</td>\n",
       "      <td>QIA</td>\n",
       "      <td>Immunogenicity and safety of two doses of the ...</td>\n",
       "      <td>cov 2 vaccine coronavac   sinovac life scienc...</td>\n",
       "      <td>[cov, vaccine, coronavac, sinovac, life, scien...</td>\n",
       "      <td>[cov vaccine coronavac sinovac life sciences  ...</td>\n",
       "      <td>[(cov, NN), (vaccine, NN), (coronavac, NN), (s...</td>\n",
       "      <td>[[(cov, VBP), (vaccine, JJ), (coronavac, NN), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company   \n",
       "3004            Continental  \\\n",
       "6173  Infineon Technologies   \n",
       "89               Beiersdorf   \n",
       "2473                  Bayer   \n",
       "7611                 Qiagen   \n",
       "\n",
       "                                                content   \n",
       "3004  CHICAGO, Aug. 25, 2022 /PRNewswire/ -- The glo...  \\\n",
       "6173  The Czech koruna rises as the country's centra...   \n",
       "89    CARE BEYOND SKIN03 Foreword05 Interview07 | Ou...   \n",
       "2473  Request Here Sample Report Buy This Complete B...   \n",
       "7611  BackgroundWe aimed to examine the immunogenici...   \n",
       "\n",
       "                   datatype        date          domain   \n",
       "3004                general  2022-08-25   prnewswire.co  \\\n",
       "6173               business  2022-02-03  marketscreener   \n",
       "89    sustainability_report  2020-03-31             NaN   \n",
       "2473                general  2021-10-22        ecochunk   \n",
       "7611              thinktank  2021-12-03       thelancet   \n",
       "\n",
       "                                             esg_topics  internal symbol   \n",
       "3004                        ['EMobility', 'ValueChain']         0    CON  \\\n",
       "6173           ['RenewableEnergy', 'RussianFederation']         0    IFX   \n",
       "89    ['RoundtableOnSustainablePalmOil', 'CleanWater...         1    BEI   \n",
       "2473                                     ['ValueChain']         0   BAYN   \n",
       "7611  ['Social', 'GenderDiversity', 'PatientSafety',...         0    QIA   \n",
       "\n",
       "                                                  title   \n",
       "3004  Automotive Valves Market worth $ 28.2 billion ...  \\\n",
       "6173  EUROPEAN MIDDAY BRIEFING - Stocks Lower Ahead ...   \n",
       "89              BeiersdorfAG Sustainability Report 2020   \n",
       "2473  Fungal Otitis Externa Market Will Watch a Sens...   \n",
       "7611  Immunogenicity and safety of two doses of the ...   \n",
       "\n",
       "                                        cleaned_content   \n",
       "3004  chicago, august 25, 2022  prnewswire     the g...  \\\n",
       "6173  's central bank is expected to raise interest ...   \n",
       "89      our commitments08 overview of the consumer b...   \n",
       "2473  . the research covers a valuable source of per...   \n",
       "7611   cov 2 vaccine coronavac   sinovac life scienc...   \n",
       "\n",
       "                                            word_tokens   \n",
       "3004  [chicago, august, prnewswire, global, automoti...  \\\n",
       "6173  [central, bank, expected, raise, interest, rat...   \n",
       "89    [commitments08, overview, consumer, business, ...   \n",
       "2473  [research, cover, valuable, source, perceptive...   \n",
       "7611  [cov, vaccine, coronavac, sinovac, life, scien...   \n",
       "\n",
       "                                        sentence_tokens   \n",
       "3004  [chicago  august  prnewswire global automotive...  \\\n",
       "6173  [s central bank expected raise interest rates ...   \n",
       "89    [overview consumer business sustainability age...   \n",
       "2473  [, research covers valuable source perceptive ...   \n",
       "7611  [cov vaccine coronavac sinovac life sciences  ...   \n",
       "\n",
       "                                 pos_tagged_word_tokens   \n",
       "3004  [(chicago, RB), (august, JJ), (prnewswire, NN)...  \\\n",
       "6173  [(central, JJ), (bank, NN), (expected, VBD), (...   \n",
       "89    [(commitments08, NN), (overview, JJ), (consume...   \n",
       "2473  [(research, NN), (cover, NN), (valuable, JJ), ...   \n",
       "7611  [(cov, NN), (vaccine, NN), (coronavac, NN), (s...   \n",
       "\n",
       "                             pos_tagged_sentence_tokens  \n",
       "3004  [[(chicago, RB), (august, JJ), (prnewswire, NN...  \n",
       "6173  [[(central, JJ), (bank, NN), (expected, VBN), ...  \n",
       "89    [[(overview, VBZ), (consumer, NN), (business, ...  \n",
       "2473  [[], [(research, NN), (valuable, JJ), (source,...  \n",
       "7611  [[(cov, VBP), (vaccine, JJ), (coronavac, NN), ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to cleaned_data.csv\n",
      "Loaded DataFrame from cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "cleaned_data = csv_checkpoint(cleaned_data, 'cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several additional information could be helpful in the further analysis, which are not included in the dataset. Therefore a small scraper is used to enrich the the dataset with the sector, industry and market capitalization of the DAX companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>symbol</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linde plc</td>\n",
       "      <td>LIN</td>\n",
       "      <td>156.93</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Specialty Chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAP SE</td>\n",
       "      <td>SAP</td>\n",
       "      <td>121.03</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software—Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>SIE</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>DTE</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Airbus SE</td>\n",
       "      <td>AIR</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company_name symbol  market_cap_in_usd_b         country   \n",
       "0            Linde plc    LIN               156.93  United Kingdom  \\\n",
       "1               SAP SE    SAP               121.03         Germany   \n",
       "2           Siemens AG    SIE               110.13         Germany   \n",
       "3  Deutsche Telekom AG    DTE               101.78         Germany   \n",
       "4            Airbus SE    AIR                96.87     Netherlands   \n",
       "\n",
       "                   sector                        industry  \n",
       "0         Basic Materials             Specialty Chemicals  \n",
       "1              Technology            Software—Application  \n",
       "2             Industrials  Specialty Industrial Machinery  \n",
       "3  Communication Services                Telecom Services  \n",
       "4             Industrials             Aerospace & Defense  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://disfold.com/stock-index/dax/companies/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "scraped_data = []\n",
    "for row in table.find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    scraped_data.append(cols)\n",
    "\n",
    "def clean_scraped_data(data):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in data:\n",
    "        # Remove empty rows\n",
    "        if len(row) > 0:\n",
    "            # Remove the '$' and ',' signs from the market cap and convert it to float\n",
    "            market_cap = float(row[3].replace('$', '').replace(',', '').replace('B', ''))\n",
    "            cleaned_data.append([row[1], row[2], market_cap, row[4], row[5], row[6]])\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_data, columns=['company_name', 'symbol', 'market_cap_in_usd_b', 'country', 'sector', 'industry'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "company_enrichments = clean_scraped_data(scraped_data)\n",
    "company_enrichments.to_csv('../data/dax_company_sectors.csv', index=False)\n",
    "company_enrichments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the ticker symbols to prevent NaN and ensure correct join conditions\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('SRT3', 'SRT')\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('HEN3', 'HNK')\n",
    "company_enrichments.loc[company_enrichments['company_name'] == 'Mercedes-Benz Group AG', 'symbol'] = 'DAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['symbol'] = cleaned_data['symbol'].astype(pd.StringDtype())\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].astype(pd.StringDtype())\n",
    "\n",
    "# Merge the cleaned data with the enrichment\n",
    "enriched_cleaned_data = pd.merge(cleaned_data, company_enrichments, how='left', on='symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>company_name</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Sustainability Report 2020 We face up to futur...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'Vaccine', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Sustainability Repo...</td>\n",
       "      <td>!somewhat different!   approach. our purpose a...</td>\n",
       "      <td>['somewhat', 'different', 'approach', 'purpose...</td>\n",
       "      <td>[' somewhat different ', 'approach ', 'purpose...</td>\n",
       "      <td>[('somewhat', 'RB'), ('different', 'JJ'), ('ap...</td>\n",
       "      <td>[[('somewhat', 'RB'), ('different', 'JJ')], [(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Annual Report An overview Gross premium E 01 i...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Vaccine', 'Monopolization', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Annual Report 2021</td>\n",
       "      <td>,762.3 30,000 13,774.2 13,963.4 14,361.8 17,06...</td>\n",
       "      <td>['group', 'net', 'income', 'e', 'eur', 'policy...</td>\n",
       "      <td>[' group net income e eur policyholders ', 'su...</td>\n",
       "      <td>[('group', 'NN'), ('net', 'JJ'), ('income', 'N...</td>\n",
       "      <td>[[('group', 'NN'), ('net', 'JJ'), ('income', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company                                            content   \n",
       "48  Hannover R  Sustainability Report 2020 We face up to futur...  \\\n",
       "76  Hannover R  Annual Report An overview Gross premium E 01 i...   \n",
       "\n",
       "                 datatype        date domain   \n",
       "48  sustainability_report  2020-03-31    NaN  \\\n",
       "76          annual_report  2021-03-31    NaN   \n",
       "\n",
       "                                           esg_topics  internal symbol   \n",
       "48  ['Whistleblowing', 'Vaccine', 'Corruption', 'G...         1   HNR1  \\\n",
       "76  ['Vaccine', 'Monopolization', 'Corruption', 'G...         1   HNR1   \n",
       "\n",
       "                                                title   \n",
       "48  HannoverRückversicherungAG Sustainability Repo...  \\\n",
       "76      HannoverRückversicherungAG Annual Report 2021   \n",
       "\n",
       "                                      cleaned_content   \n",
       "48  !somewhat different!   approach. our purpose a...  \\\n",
       "76  ,762.3 30,000 13,774.2 13,963.4 14,361.8 17,06...   \n",
       "\n",
       "                                          word_tokens   \n",
       "48  ['somewhat', 'different', 'approach', 'purpose...  \\\n",
       "76  ['group', 'net', 'income', 'e', 'eur', 'policy...   \n",
       "\n",
       "                                      sentence_tokens   \n",
       "48  [' somewhat different ', 'approach ', 'purpose...  \\\n",
       "76  [' group net income e eur policyholders ', 'su...   \n",
       "\n",
       "                               pos_tagged_word_tokens   \n",
       "48  [('somewhat', 'RB'), ('different', 'JJ'), ('ap...  \\\n",
       "76  [('group', 'NN'), ('net', 'JJ'), ('income', 'N...   \n",
       "\n",
       "                           pos_tagged_sentence_tokens company_name   \n",
       "48  [[('somewhat', 'RB'), ('different', 'JJ')], [(...          NaN  \\\n",
       "76  [[('group', 'NN'), ('net', 'JJ'), ('income', '...          NaN   \n",
       "\n",
       "    market_cap_in_usd_b country sector industry  \n",
       "48                  NaN     NaN    NaN      NaN  \n",
       "76                  NaN     NaN    NaN      NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_cleaned_data[enriched_cleaned_data['industry'].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hannover R AG cannot be matched, since it is not present in the scraped data. Since there are only 2 records this is negligible and will be fixed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'sector'] = 'Financials'\n",
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'industry'] = 'Insurance—Reinsurance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns/data\n",
    "enriched_cleaned_data = enriched_cleaned_data.drop(columns=['content', 'company_name', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>BMW</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-09-06</td>\n",
       "      <td>autonews</td>\n",
       "      <td>['Recycling', 'EMobility']</td>\n",
       "      <td>0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>BMW concept shows'sustainable car of the futur...</td>\n",
       "      <td>bmw's i vision circular concept previews    a ...</td>\n",
       "      <td>['bmws', 'vision', 'circular', 'concept', 'pre...</td>\n",
       "      <td>['s vision circular concept previews sustainab...</td>\n",
       "      <td>[('bmws', 'NN'), ('vision', 'NN'), ('circular'...</td>\n",
       "      <td>[[('vision', 'NN'), ('circular', 'JJ'), ('conc...</td>\n",
       "      <td>60.24</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>Deutsche Post</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['Compliance']</td>\n",
       "      <td>0</td>\n",
       "      <td>DPW</td>\n",
       "      <td>Deutsche Post AG: Announcement pursuant to Art...</td>\n",
       "      <td>dgap news  deutsche post ag   key word   s   s...</td>\n",
       "      <td>['dgap', 'news', 'deutsche', 'post', 'ag', 'ke...</td>\n",
       "      <td>['dgap news deutsche post ag key word share bu...</td>\n",
       "      <td>[('dgap', 'NN'), ('news', 'NN'), ('deutsche', ...</td>\n",
       "      <td>[[('dgap', 'NN'), ('news', 'NN'), ('deutsche',...</td>\n",
       "      <td>45.40</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Integrated Freight &amp; Logistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>Deutsche Boerse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-03-30</td>\n",
       "      <td>draeger</td>\n",
       "      <td>['Diversity', 'StakeholderEngagement', 'Collus...</td>\n",
       "      <td>0</td>\n",
       "      <td>DB1</td>\n",
       "      <td>Dräger Sustainability Report 2021</td>\n",
       "      <td>foreword 3 about the report 6 sustainability...</td>\n",
       "      <td>['foreword', 'report', 'sustainability', 'stak...</td>\n",
       "      <td>['foreword report sustainability stakeholders ...</td>\n",
       "      <td>[('foreword', 'JJ'), ('report', 'NN'), ('susta...</td>\n",
       "      <td>[[('foreword', 'NN'), ('report', 'NN'), ('sust...</td>\n",
       "      <td>31.43</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Financial Data &amp; Stock Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>Siemens Energy</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-02-18</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['Environment']</td>\n",
       "      <td>0</td>\n",
       "      <td>ENR</td>\n",
       "      <td>Siemens Energy's CEO increases pressure on Sie...</td>\n",
       "      <td>the market environment in the onshore busin...</td>\n",
       "      <td>['market', 'environment', 'onshore', 'business...</td>\n",
       "      <td>['market environment onshore business remain d...</td>\n",
       "      <td>[('market', 'NN'), ('environment', 'NN'), ('on...</td>\n",
       "      <td>[[('market', 'NN'), ('environment', 'NN'), ('o...</td>\n",
       "      <td>13.70</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Utilities—Independent Power Producers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>Allianz</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-10-09</td>\n",
       "      <td>bnnbloomberg</td>\n",
       "      <td>['RussianFederation', 'Petroleum']</td>\n",
       "      <td>0</td>\n",
       "      <td>ALV</td>\n",
       "      <td>Oil Steadies After Weekly Jump on Concern Fed ...</td>\n",
       "      <td>, please check back again soon. a worker drill...</td>\n",
       "      <td>['please', 'check', 'back', 'soon', 'worker', ...</td>\n",
       "      <td>[' please check back soon ', 'worker drills oi...</td>\n",
       "      <td>[('please', 'VB'), ('check', 'VB'), ('back', '...</td>\n",
       "      <td>[[('please', 'VB'), ('check', 'VB'), ('back', ...</td>\n",
       "      <td>88.44</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Insurance—Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-11-07</td>\n",
       "      <td>uniglobalunion</td>\n",
       "      <td>['Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>UNI stands in solidarity with Crnogorski Telek...</td>\n",
       "      <td>ct , a deutsche telekom subsidiary, in monte...</td>\n",
       "      <td>['ct', 'deutsche', 'telekom', 'subsidiary', 'm...</td>\n",
       "      <td>['ct  deutsche telekom subsidiary  montenegro ...</td>\n",
       "      <td>[('ct', 'NN'), ('deutsche', 'NN'), ('telekom',...</td>\n",
       "      <td>[[('ct', 'NN'), ('deutsche', 'NN'), ('telekom'...</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5587</th>\n",
       "      <td>Heidelberg Cement</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>worldcement</td>\n",
       "      <td>['CarbonCaptureAndStorage', 'Recarbonation', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCement pioneers new carbon capture t...</td>\n",
       "      <td>. here are the instructions how to enable java...</td>\n",
       "      <td>['instruction', 'enable', 'javascript', 'web',...</td>\n",
       "      <td>['', 'instructions enable javascript web brows...</td>\n",
       "      <td>[('instruction', 'NN'), ('enable', 'JJ'), ('ja...</td>\n",
       "      <td>[[], [('enable', 'JJ'), ('javascript', 'NN'), ...</td>\n",
       "      <td>11.91</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Building Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8563</th>\n",
       "      <td>SAP</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>businesswire</td>\n",
       "      <td>['ValueChain', 'EnergyManagement', 'WasteManag...</td>\n",
       "      <td>0</td>\n",
       "      <td>SAP</td>\n",
       "      <td>Global Smart Cities Markets Report 2021: Smart...</td>\n",
       "      <td>dublin      business wire     the    smart cit...</td>\n",
       "      <td>['dublin', 'business', 'wire', 'smart', 'city'...</td>\n",
       "      <td>['dublin business wire smart cities market sha...</td>\n",
       "      <td>[('dublin', 'NN'), ('business', 'NN'), ('wire'...</td>\n",
       "      <td>[[('dublin', 'NN'), ('business', 'NN'), ('wire...</td>\n",
       "      <td>121.03</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software—Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9762</th>\n",
       "      <td>Siemens Healthineers</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>SHL</td>\n",
       "      <td>Blue Earth Diagnostics Announces Results on Cl...</td>\n",
       "      <td>18f rhpsma 7.3 pet image showing prostate canc...</td>\n",
       "      <td>['18f', 'rhpsma', 'pet', 'image', 'showing', '...</td>\n",
       "      <td>['rhpsma pet image showing prostate cancer spr...</td>\n",
       "      <td>[('18f', 'CD'), ('rhpsma', 'NN'), ('pet', 'JJ'...</td>\n",
       "      <td>[[('rhpsma', 'NN'), ('pet', 'NN'), ('image', '...</td>\n",
       "      <td>55.11</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10055</th>\n",
       "      <td>Siemens Healthineers</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Environment', 'Social', 'Toxicity']</td>\n",
       "      <td>0</td>\n",
       "      <td>SHL</td>\n",
       "      <td>Rice Refines Analysis of MRI Contrast Agents</td>\n",
       "      <td>. in this model, green gadolinium is surrounde...</td>\n",
       "      <td>['model', 'green', 'gadolinium', 'surrounded',...</td>\n",
       "      <td>['', 'model  green gadolinium surrounded blue ...</td>\n",
       "      <td>[('model', 'NN'), ('green', 'JJ'), ('gadoliniu...</td>\n",
       "      <td>[[], [('model', 'NN'), ('green', 'JJ'), ('gado...</td>\n",
       "      <td>55.11</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company  datatype        date          domain   \n",
       "1794                    BMW   general  2021-09-06        autonews  \\\n",
       "4484          Deutsche Post  business  2022-03-09  marketscreener   \n",
       "4420        Deutsche Boerse       NaN  2022-03-30         draeger   \n",
       "9242         Siemens Energy  business  2022-02-18  marketscreener   \n",
       "1106                Allianz  business  2022-10-09    bnnbloomberg   \n",
       "4738       Deutsche Telekom   general  2022-11-07  uniglobalunion   \n",
       "5587      Heidelberg Cement  business  2021-03-15     worldcement   \n",
       "8563                    SAP   general  2021-11-12    businesswire   \n",
       "9762   Siemens Healthineers   general  2022-11-02       itnonline   \n",
       "10055  Siemens Healthineers   general  2022-11-21       itnonline   \n",
       "\n",
       "                                              esg_topics  internal symbol   \n",
       "1794                          ['Recycling', 'EMobility']         0    BMW  \\\n",
       "4484                                      ['Compliance']         0    DPW   \n",
       "4420   ['Diversity', 'StakeholderEngagement', 'Collus...         0    DB1   \n",
       "9242                                     ['Environment']         0    ENR   \n",
       "1106                  ['RussianFederation', 'Petroleum']         0    ALV   \n",
       "4738                                          ['Social']         0    DTE   \n",
       "5587   ['CarbonCaptureAndStorage', 'Recarbonation', '...         0    HEI   \n",
       "8563   ['ValueChain', 'EnergyManagement', 'WasteManag...         0    SAP   \n",
       "9762                                          ['Social']         0    SHL   \n",
       "10055              ['Environment', 'Social', 'Toxicity']         0    SHL   \n",
       "\n",
       "                                                   title   \n",
       "1794   BMW concept shows'sustainable car of the futur...  \\\n",
       "4484   Deutsche Post AG: Announcement pursuant to Art...   \n",
       "4420                   Dräger Sustainability Report 2021   \n",
       "9242   Siemens Energy's CEO increases pressure on Sie...   \n",
       "1106   Oil Steadies After Weekly Jump on Concern Fed ...   \n",
       "4738   UNI stands in solidarity with Crnogorski Telek...   \n",
       "5587   HeidelbergCement pioneers new carbon capture t...   \n",
       "8563   Global Smart Cities Markets Report 2021: Smart...   \n",
       "9762   Blue Earth Diagnostics Announces Results on Cl...   \n",
       "10055       Rice Refines Analysis of MRI Contrast Agents   \n",
       "\n",
       "                                         cleaned_content   \n",
       "1794   bmw's i vision circular concept previews    a ...  \\\n",
       "4484   dgap news  deutsche post ag   key word   s   s...   \n",
       "4420     foreword 3 about the report 6 sustainability...   \n",
       "9242      the market environment in the onshore busin...   \n",
       "1106   , please check back again soon. a worker drill...   \n",
       "4738     ct , a deutsche telekom subsidiary, in monte...   \n",
       "5587   . here are the instructions how to enable java...   \n",
       "8563   dublin      business wire     the    smart cit...   \n",
       "9762   18f rhpsma 7.3 pet image showing prostate canc...   \n",
       "10055  . in this model, green gadolinium is surrounde...   \n",
       "\n",
       "                                             word_tokens   \n",
       "1794   ['bmws', 'vision', 'circular', 'concept', 'pre...  \\\n",
       "4484   ['dgap', 'news', 'deutsche', 'post', 'ag', 'ke...   \n",
       "4420   ['foreword', 'report', 'sustainability', 'stak...   \n",
       "9242   ['market', 'environment', 'onshore', 'business...   \n",
       "1106   ['please', 'check', 'back', 'soon', 'worker', ...   \n",
       "4738   ['ct', 'deutsche', 'telekom', 'subsidiary', 'm...   \n",
       "5587   ['instruction', 'enable', 'javascript', 'web',...   \n",
       "8563   ['dublin', 'business', 'wire', 'smart', 'city'...   \n",
       "9762   ['18f', 'rhpsma', 'pet', 'image', 'showing', '...   \n",
       "10055  ['model', 'green', 'gadolinium', 'surrounded',...   \n",
       "\n",
       "                                         sentence_tokens   \n",
       "1794   ['s vision circular concept previews sustainab...  \\\n",
       "4484   ['dgap news deutsche post ag key word share bu...   \n",
       "4420   ['foreword report sustainability stakeholders ...   \n",
       "9242   ['market environment onshore business remain d...   \n",
       "1106   [' please check back soon ', 'worker drills oi...   \n",
       "4738   ['ct  deutsche telekom subsidiary  montenegro ...   \n",
       "5587   ['', 'instructions enable javascript web brows...   \n",
       "8563   ['dublin business wire smart cities market sha...   \n",
       "9762   ['rhpsma pet image showing prostate cancer spr...   \n",
       "10055  ['', 'model  green gadolinium surrounded blue ...   \n",
       "\n",
       "                                  pos_tagged_word_tokens   \n",
       "1794   [('bmws', 'NN'), ('vision', 'NN'), ('circular'...  \\\n",
       "4484   [('dgap', 'NN'), ('news', 'NN'), ('deutsche', ...   \n",
       "4420   [('foreword', 'JJ'), ('report', 'NN'), ('susta...   \n",
       "9242   [('market', 'NN'), ('environment', 'NN'), ('on...   \n",
       "1106   [('please', 'VB'), ('check', 'VB'), ('back', '...   \n",
       "4738   [('ct', 'NN'), ('deutsche', 'NN'), ('telekom',...   \n",
       "5587   [('instruction', 'NN'), ('enable', 'JJ'), ('ja...   \n",
       "8563   [('dublin', 'NN'), ('business', 'NN'), ('wire'...   \n",
       "9762   [('18f', 'CD'), ('rhpsma', 'NN'), ('pet', 'JJ'...   \n",
       "10055  [('model', 'NN'), ('green', 'JJ'), ('gadoliniu...   \n",
       "\n",
       "                              pos_tagged_sentence_tokens  market_cap_in_usd_b   \n",
       "1794   [[('vision', 'NN'), ('circular', 'JJ'), ('conc...                60.24  \\\n",
       "4484   [[('dgap', 'NN'), ('news', 'NN'), ('deutsche',...                45.40   \n",
       "4420   [[('foreword', 'NN'), ('report', 'NN'), ('sust...                31.43   \n",
       "9242   [[('market', 'NN'), ('environment', 'NN'), ('o...                13.70   \n",
       "1106   [[('please', 'VB'), ('check', 'VB'), ('back', ...                88.44   \n",
       "4738   [[('ct', 'NN'), ('deutsche', 'NN'), ('telekom'...               101.78   \n",
       "5587   [[], [('enable', 'JJ'), ('javascript', 'NN'), ...                11.91   \n",
       "8563   [[('dublin', 'NN'), ('business', 'NN'), ('wire...               121.03   \n",
       "9762   [[('rhpsma', 'NN'), ('pet', 'NN'), ('image', '...                55.11   \n",
       "10055  [[], [('model', 'NN'), ('green', 'JJ'), ('gado...                55.11   \n",
       "\n",
       "                       sector                               industry  \n",
       "1794   Consumer Discretionary                     Auto Manufacturers  \n",
       "4484              Industrials         Integrated Freight & Logistics  \n",
       "4420               Financials       Financial Data & Stock Exchanges  \n",
       "9242                Utilities  Utilities—Independent Power Producers  \n",
       "1106               Financials                  Insurance—Diversified  \n",
       "4738   Communication Services                       Telecom Services  \n",
       "5587          Basic Materials                     Building Materials  \n",
       "8563               Technology                   Software—Application  \n",
       "9762               Healthcare                 Diagnostics & Research  \n",
       "10055              Healthcare                 Diagnostics & Research  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check final dataframe\n",
    "enriched_cleaned_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the processing, some rows (2) have no content anymore. These are dropped.\n",
    "enriched_cleaned_data[enriched_cleaned_data['cleaned_content'].isna()]\n",
    "enriched_cleaned_data = enriched_cleaned_data.dropna(subset=['cleaned_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to enriched_cleaned_data.csv\n",
      "Loaded DataFrame from enriched_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file for further analysis\n",
    "enriched_cleaned_data = csv_checkpoint(enriched_cleaned_data, 'enriched_cleaned_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
