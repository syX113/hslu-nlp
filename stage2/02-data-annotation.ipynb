{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Data Annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll leverage a Large Language Model (LLM) to perform sentiment annotation on the ESG document dataset, assigning scores of 0 for negative, 0.5 for neutral, and 1 for positive sentiment.  \n",
    "The workflow involves manually creating a \"gold standard\" by annotating ~500 sentences, afterward setting up 2-3 LLMs for trial annotations, and experimenting with prompting strategies (zero-shot/few-shot) that we'll evaluate against the \"gold standard\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "cleaned_data = pd.read_csv('../data/checkpoints/enriched_cleaned_data.csv', delimiter = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert a string representation of a list to a list datatype\n",
    "def string_to_list(string):\n",
    "    try:\n",
    "        return ast.literal_eval(string)\n",
    "    except (ValueError, SyntaxError):\n",
    "        print('List conversion failed')\n",
    "        return []\n",
    "\n",
    "# Convert the string representations of the lists to the correct 'list' datatype\n",
    "cleaned_data['word_tokens'] = cleaned_data['word_tokens'].apply(string_to_list)\n",
    "cleaned_data['sentence_tokens'] = cleaned_data['sentence_tokens'].apply(string_to_list)\n",
    "cleaned_data['pos_tagged_word_tokens'] = cleaned_data['pos_tagged_word_tokens'].apply(string_to_list)\n",
    "cleaned_data['pos_tagged_sentence_tokens'] = cleaned_data['pos_tagged_sentence_tokens'].apply(string_to_list)\n",
    "cleaned_data['esg_topics'] = cleaned_data['esg_topics'].apply(string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some count features for the analysis\n",
    "cleaned_data['cnt_word'] = cleaned_data['word_tokens'].apply(len)\n",
    "cleaned_data['cnt_sentence'] = cleaned_data['sentence_tokens'].apply(len)\n",
    "cleaned_data['cnt_esg'] = cleaned_data['esg_topics'].apply(len)\n",
    "\n",
    "# Calculate ratio between words/sentences\n",
    "cleaned_data['ratio_word_sentence'] = cleaned_data['cnt_word'] / cleaned_data['cnt_sentence']\n",
    "\n",
    "# Convert date to correct datatype\n",
    "cleaned_data['date'] = pd.to_datetime(cleaned_data['date'])\n",
    "\n",
    "# Derive year and month to aggregate\n",
    "cleaned_data['year_month'] = cleaned_data['date'].apply(lambda x: x.strftime('%Y-%m'))\n",
    "cleaned_data['year'] = cleaned_data['date'].apply(lambda x: x.strftime('%Y'))\n",
    "cleaned_data['month'] = cleaned_data['date'].apply(lambda x: x.strftime('%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to save intermediary steps in a file\n",
    "def csv_checkpoint(df, filename='checkpoint'):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a CSV file and loads it back into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to save and load.\n",
    "        filename (str): The name of the CSV file to save the DataFrame to (default: 'checkpoint').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../data/checkpoints/'):  # Check if the directory exists and create it if it doesn't\n",
    "        os.makedirs('../data/checkpoints/')\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(f'../data/checkpoints/{filename}.csv', index=False, sep='|')  # Save DataFrame to CSV with specified filename\n",
    "    print(f'Saved DataFrame to {filename}.csv')\n",
    "\n",
    "    # Load CSV back into DataFrame\n",
    "    df = pd.read_csv(f'../data/checkpoints/{filename}.csv', delimiter='|')  # Load CSV back into DataFrame\n",
    "    print(f'Loaded DataFrame from {filename}.csv')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual sentence sentiment annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a \"gold standard\" for the sentiment, 500 randomly sampled sentences are manually annotate with:  \n",
    "**0 = negative, 0.5 = neutral, 1 = positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate a deep copy so no reload from CSV files is necessary\n",
    "documents = cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craete new column to store the sentence sentiment\n",
    "documents['sentence_sentiment_value_llm'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the dataset based on the sentence tokens, so each row contains one sentence\n",
    "documents = documents.explode('sentence_tokens')\n",
    "\n",
    "# Preserve original index, so a later aggregation is possible\n",
    "documents['original_index'] = documents.index\n",
    "\n",
    "# Reset the index\n",
    "documents = documents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the DataFrame into internal/external sentences with a defined ratio\n",
    "internal = documents[documents['internal'] == 1]\n",
    "external = documents[documents['internal'] == 0]\n",
    "\n",
    "# Determine the number of samples from each group, 1000 sentences in total\n",
    "n_internal = int(0.2 * 1000)  # 20% of samples\n",
    "n_external = 1000 - n_internal  # Remaining samples\n",
    "\n",
    "# Sample 1000 random sentences with a seed, so a re-run samples the same sentences\n",
    "sampled_internal = internal.sample(n=n_internal, random_state=42)\n",
    "sampled_external = external.sample(n=n_external, random_state=42)\n",
    "\n",
    "# Concatenate and shuffle the samples the DataFrames\n",
    "sampled_documents = pd.concat([sampled_internal, sampled_external])\n",
    "sampled_documents = sampled_documents.sample(frac=1, random_state=42)\n",
    "\n",
    "# Drop the sampled rows from the original DataFrame\n",
    "documents = documents.drop(sampled_documents.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>internal</th>\n",
       "      <th>sentence_sentiment_value_llm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529113</th>\n",
       "      <td>Transcript levels in plasma contribute substan...</td>\n",
       "      <td>therefore adjust differences sample quality in...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339673</th>\n",
       "      <td>Absolutely everything you need to go bikepacki...</td>\n",
       "      <td>way little quicker easier make coffee porridge...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390711</th>\n",
       "      <td>STARTUP STAGE: Tripshifu connects experienced ...</td>\n",
       "      <td>started career multinational tata steel joinin...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354554</th>\n",
       "      <td>Automotive Aftermarket Market by Global Busine...</td>\n",
       "      <td>notable trend currently influencing dynamics a...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420428</th>\n",
       "      <td>Smashing Podcast Episode 50 With Marko Dugonji...</td>\n",
       "      <td>know never used tables layout</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title   \n",
       "529113  Transcript levels in plasma contribute substan...  \\\n",
       "339673  Absolutely everything you need to go bikepacki...   \n",
       "390711  STARTUP STAGE: Tripshifu connects experienced ...   \n",
       "354554  Automotive Aftermarket Market by Global Busine...   \n",
       "420428  Smashing Podcast Episode 50 With Marko Dugonji...   \n",
       "\n",
       "                                          sentence_tokens  internal   \n",
       "529113  therefore adjust differences sample quality in...         0  \\\n",
       "339673  way little quicker easier make coffee porridge...         0   \n",
       "390711  started career multinational tata steel joinin...         0   \n",
       "354554  notable trend currently influencing dynamics a...         0   \n",
       "420428                      know never used tables layout         0   \n",
       "\n",
       "        sentence_sentiment_value_llm  \n",
       "529113                           NaN  \n",
       "339673                           NaN  \n",
       "390711                           NaN  \n",
       "354554                           NaN  \n",
       "420428                           NaN  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sampled data\n",
    "sampled_documents[['title','sentence_tokens','internal','sentence_sentiment_value_llm']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop the samples to annotate them\n",
    "for idx, row in sampled_documents.iterrows():\n",
    "    # Loop until valid input is received\n",
    "    while True:\n",
    "        # Print the title of the document and the sentence\n",
    "        print(f\"Title: {row['title']}\\nSentence: {row['sentence_tokens']}\\n\")\n",
    "\n",
    "        # Wait for user input\n",
    "        sentiment = input(\"Enter sentiment value (+ for 1.0, - for 0.0, Enter for 0.5): \")\n",
    "\n",
    "        # Check if the input is valid\n",
    "        if sentiment == '+':\n",
    "            sampled_documents.at[idx, 'sentence_sentiment_value_llm'] = 1.0\n",
    "            break\n",
    "        elif sentiment == '-':\n",
    "            sampled_documents.at[idx, 'sentence_sentiment_value_llm'] = 0.0\n",
    "            break\n",
    "        elif sentiment == '':\n",
    "            sampled_documents.at[idx, 'sentence_sentiment_value_llm'] = 0.5\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the annotated samples with the complete dataset\n",
    "documents = pd.concat([documents, sampled_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>cnt_word</th>\n",
       "      <th>cnt_sentence</th>\n",
       "      <th>cnt_esg</th>\n",
       "      <th>ratio_word_sentence</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentence_sentiment_value_llm</th>\n",
       "      <th>original_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529113</th>\n",
       "      <td>Qiagen</td>\n",
       "      <td>thinktank</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>thelancet</td>\n",
       "      <td>[GenderDiversity, Privacy]</td>\n",
       "      <td>0</td>\n",
       "      <td>QIA</td>\n",
       "      <td>Transcript levels in plasma contribute substan...</td>\n",
       "      <td>aa remain underrepresented alzheimers disease ...</td>\n",
       "      <td>[aa, remain, underrepresented, alzheimers, dis...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100772</td>\n",
       "      <td>5054</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>2022-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339673</th>\n",
       "      <td>Beiersdorf</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>cyclingweekly</td>\n",
       "      <td>[Compliance, Recycling, CustomerService, Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>BEI</td>\n",
       "      <td>Absolutely everything you need to go bikepacki...</td>\n",
       "      <td>get know area far intimately staying accommoda...</td>\n",
       "      <td>[get, know, area, far, intimately, staying, ac...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314033</td>\n",
       "      <td>5031</td>\n",
       "      <td>431</td>\n",
       "      <td>4</td>\n",
       "      <td>11.672854</td>\n",
       "      <td>2021-04</td>\n",
       "      <td>2021</td>\n",
       "      <td>04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390711</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-09-26</td>\n",
       "      <td>phocuswire</td>\n",
       "      <td>[HumanCapital, Social, Recruiting, Misinformat...</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>STARTUP STAGE: Tripshifu connects experienced ...</td>\n",
       "      <td>founded february currently five employee idea ...</td>\n",
       "      <td>[founded, february, currently, five, employee,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249278</td>\n",
       "      <td>520</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>2022-09</td>\n",
       "      <td>2022</td>\n",
       "      <td>09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354554</th>\n",
       "      <td>Continental</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>ecochunk</td>\n",
       "      <td>[RussianFederation]</td>\n",
       "      <td>0</td>\n",
       "      <td>CON</td>\n",
       "      <td>Automotive Aftermarket Market by Global Busine...</td>\n",
       "      <td>recording estimating analysing market data rep...</td>\n",
       "      <td>[recording, estimating, analysing, market, dat...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280167</td>\n",
       "      <td>676</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>18.777778</td>\n",
       "      <td>2021-10</td>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420428</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-08-09</td>\n",
       "      <td>smashingmagazine</td>\n",
       "      <td>[CorporateCulture, HumanCapital, Environment, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>Smashing Podcast Episode 50 With Marko Dugonji...</td>\n",
       "      <td>ask affect change ux design large organization...</td>\n",
       "      <td>[ask, affect, change, ux, design, large, organ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203908</td>\n",
       "      <td>2392</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>7.475000</td>\n",
       "      <td>2022-08</td>\n",
       "      <td>2022</td>\n",
       "      <td>08</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 company   datatype       date            domain   \n",
       "529113            Qiagen  thinktank 2022-03-17         thelancet  \\\n",
       "339673        Beiersdorf    general 2021-04-27     cyclingweekly   \n",
       "390711     Deutsche Bank    general 2022-09-26        phocuswire   \n",
       "354554       Continental    general 2021-10-05          ecochunk   \n",
       "420428  Deutsche Telekom   business 2022-08-09  smashingmagazine   \n",
       "\n",
       "                                               esg_topics  internal symbol   \n",
       "529113                         [GenderDiversity, Privacy]         0    QIA  \\\n",
       "339673  [Compliance, Recycling, CustomerService, Gende...         0    BEI   \n",
       "390711  [HumanCapital, Social, Recruiting, Misinformat...         0    DBK   \n",
       "354554                                [RussianFederation]         0    CON   \n",
       "420428  [CorporateCulture, HumanCapital, Environment, ...         0    DTE   \n",
       "\n",
       "                                                    title   \n",
       "529113  Transcript levels in plasma contribute substan...  \\\n",
       "339673  Absolutely everything you need to go bikepacki...   \n",
       "390711  STARTUP STAGE: Tripshifu connects experienced ...   \n",
       "354554  Automotive Aftermarket Market by Global Busine...   \n",
       "420428  Smashing Podcast Episode 50 With Marko Dugonji...   \n",
       "\n",
       "                                          cleaned_content   \n",
       "529113  aa remain underrepresented alzheimers disease ...  \\\n",
       "339673  get know area far intimately staying accommoda...   \n",
       "390711  founded february currently five employee idea ...   \n",
       "354554  recording estimating analysing market data rep...   \n",
       "420428  ask affect change ux design large organization...   \n",
       "\n",
       "                                              word_tokens  ...   \n",
       "529113  [aa, remain, underrepresented, alzheimers, dis...  ...  \\\n",
       "339673  [get, know, area, far, intimately, staying, ac...  ...   \n",
       "390711  [founded, february, currently, five, employee,...  ...   \n",
       "354554  [recording, estimating, analysing, market, dat...  ...   \n",
       "420428  [ask, affect, change, ux, design, large, organ...  ...   \n",
       "\n",
       "       sentiment_value cnt_word cnt_sentence  cnt_esg ratio_word_sentence   \n",
       "529113        0.100772     5054          280        2           18.050000  \\\n",
       "339673        0.314033     5031          431        4           11.672854   \n",
       "390711        0.249278      520           50        4           10.400000   \n",
       "354554        0.280167      676           36        1           18.777778   \n",
       "420428        0.203908     2392          320        4            7.475000   \n",
       "\n",
       "       year_month  year  month  sentence_sentiment_value_llm  original_index  \n",
       "529113    2022-03  2022     03                           1.0            7601  \n",
       "339673    2021-04  2021     04                           0.0            2774  \n",
       "390711    2022-09  2022     09                           1.0            4228  \n",
       "354554    2021-10  2021     10                           0.0            3046  \n",
       "420428    2022-08  2022     08                           0.5            4946  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the manually annotated data\n",
    "documents[documents['sentence_sentiment_value_llm'].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'original_index' and aggregate the columns\n",
    "documents = documents.groupby('original_index').agg({\n",
    "    # Use 'first' function for all columns except for 'sentence_tokens' and 'sentence_sentiment_value_llm'\n",
    "    'title': 'first',  \n",
    "    'company': 'first',  \n",
    "    'datatype': 'first',  \n",
    "    'date': 'first',  \n",
    "    'domain': 'first',  \n",
    "    'esg_topics': 'first',  \n",
    "    'internal': 'first',  \n",
    "    'symbol': 'first',  \n",
    "    'title': 'first',  \n",
    "    'cleaned_content': 'first',  \n",
    "    'word_tokens': 'first',  \n",
    "    'sentence_tokens': list,  # Combine the 'sentence_tokens' into a list\n",
    "    'sentence_tokens': 'first',  \n",
    "    'pos_tagged_word_tokens': 'first',  \n",
    "    'pos_tagged_sentence_tokens': 'first',  \n",
    "    'market_cap_in_usd_b': 'first',  \n",
    "    'sector': 'first',  \n",
    "    'industry': 'first',  \n",
    "    'sentiment_value': 'first',  \n",
    "    'cnt_word': 'first',  \n",
    "    'cnt_sentence': 'first',  \n",
    "    'cnt_esg': 'first',  \n",
    "    'ratio_word_sentence': 'first',  \n",
    "    'year_month': 'first',  \n",
    "    'year': 'first',  \n",
    "    'month': 'first',  \n",
    "    'sentence_sentiment_value_llm': lambda x: [i if pd.notnull(i) else np.nan for i in x]  # Combine the 'sentence_sentiment_value_llm' into a list, substituting NaN where no manual sentiment was added\n",
    "})\n",
    "\n",
    "# Reset the index\n",
    "documents = documents.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Setups\n",
    "As a first step, different LLM models for comparison needs to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Zero-Shot Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Few-Shot Strategies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Annotation\n",
    "Based on the comparison, XY demonstrated best results to annotate the ESG documents. This LLM with the respective prompting strategy is used to annotate all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
