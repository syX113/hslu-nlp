{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from unidecode import unidecode\n",
    "\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_csv('../data/esg_documents_for_dax_companies.csv', delimiter = '|', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check loaded data and reset index\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column descriptions**\n",
    "- symbol: stock symbol of the company\n",
    "- company: company name\n",
    "- date: publication date of document\n",
    "- title: document title\n",
    "- content: document content\n",
    "- datatype: document type\n",
    "- internal: is this a report by company (1) or a third-party document (0)\n",
    "- domain (optional): Web domain where the document was published\n",
    "- url (optional): URL where the document can be accessed\n",
    "- esg_topics (optional): ESG topics extracted from the data using our internal NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape (row and column amount)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes\n",
    "raw_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, several steps are conducted to normalize the text. These include lowercase conversion, expanding abbreviations, removing stopwords, applying lemmatization (dimensionality reduction), removing URLs and email addresses and extra whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = raw_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_urls(text):\n",
    "    urls = re.findall(r'http\\S+|www\\S+|https\\S+', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE), len(urls)\n",
    "\n",
    "def remove_emails(text):\n",
    "    mail_addresses = re.findall(r'\\S+@\\S+\\s?', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'\\S+@\\S+\\s?', '', text, flags=re.MULTILINE), len(mail_addresses)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    extra_spaces = re.findall(r'\\s{2,}', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip(), len(extra_spaces)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['content'].astype(str)\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: x.lower())\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: unidecode(x, errors=\"preserve\")) # Remove diacritics\n",
    "cleaned_data['cleaned_content'], url_count = zip(*cleaned_data['cleaned_content'].apply(remove_urls))\n",
    "cleaned_data['cleaned_content'], email_count = zip(*cleaned_data['cleaned_content'].apply(remove_emails))\n",
    "\n",
    "cleaned_data['cleaned_content'], extra_space_count = zip(*cleaned_data['cleaned_content'].apply(remove_extra_whitespace))\n",
    "\n",
    "print(\"URLs removed:\", sum(url_count))\n",
    "print(\"Mail addresses removed:\", sum(email_count))\n",
    "print(\"Extra whitespaces removed:\", sum(extra_space_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = []   \n",
    "    for word in text.split():\n",
    "        expanded_text.append(contractions.fix(word))  \n",
    "    \n",
    "    expanded_text = ' '.join(expanded_text)\n",
    "    return contractions.fix(expanded_text)\n",
    "\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['cleaned_content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand abbreviations\n",
    "# Basic idea from: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# Adjusted the patterns, so the regex patterns are complied once\n",
    "# Compile the regular expressions for efficiency\n",
    "specific_patterns = [\n",
    "    (re.compile(r\"won['’]t\"), \"will not\"),\n",
    "    (re.compile(r\"can['’]t\"), \"can not\"),\n",
    "]\n",
    "\n",
    "general_patterns = [\n",
    "    (re.compile(r\"n['’]t\"), \" not\"),\n",
    "    (re.compile(r\"['’]re\"), \" are\"),\n",
    "    (re.compile(r\"['’]s\"), \" is\"),\n",
    "    (re.compile(r\"['’]d\"), \" would\"),\n",
    "    (re.compile(r\"['’]ll\"), \" will\"),\n",
    "    (re.compile(r\"['’]t\"), \" not\"),\n",
    "    (re.compile(r\"['’]ve\"), \" have\"),\n",
    "    (re.compile(r\"['’]m\"), \" am\"),\n",
    "]\n",
    "\n",
    "extended_patterns = [\n",
    "    (re.compile(r\"ain['’]t\"), \"am not\"), # or \"is not\", \"are not\", \"has not\", \"have not\", depending on context\n",
    "    (re.compile(r\"shan['’]t\"), \"shall not\"),\n",
    "    (re.compile(r\"y['’]all\"), \"you all\"),\n",
    "    (re.compile(r\"o['’]clock\"), \"of the clock\"),\n",
    "    (re.compile(r\"ma['’]am\"), \"madam\"),\n",
    "    (re.compile(r\"let['’]s\"), \"let us\"),\n",
    "    (re.compile(r\"how['’]d\"), \"how did\"),\n",
    "    (re.compile(r\"how['’]ll\"), \"how will\"),\n",
    "    (re.compile(r\"what['’]re\"), \"what are\"),\n",
    "    (re.compile(r\"what['’]ve\"), \"what have\"),\n",
    "    (re.compile(r\"when['’]s\"), \"when is\"),\n",
    "    (re.compile(r\"where['’]d\"), \"where did\"),\n",
    "    (re.compile(r\"where['’]s\"), \"where is\"),\n",
    "    (re.compile(r\"why['’]s\"), \"why is\"),\n",
    "    (re.compile(r\"why['’]d\"), \"why did\"),\n",
    "    (re.compile(r\"who['’]s\"), \"who is\"),\n",
    "    (re.compile(r\"who['’]ll\"), \"who will\"),\n",
    "    (re.compile(r\"who['’]ve\"), \"who have\"),\n",
    "    (re.compile(r\"that['’]s\"), \"that is\"),\n",
    "    (re.compile(r\"that['’]ll\"), \"that will\"),\n",
    "    (re.compile(r\"there['’]s\"), \"there is\"),\n",
    "    (re.compile(r\"there['’]re\"), \"there are\"),\n",
    "    (re.compile(r\"there['’]d\"), \"there would\"),\n",
    "    (re.compile(r\"there['’]ll\"), \"there will\"),\n",
    "]\n",
    "\n",
    "def decontracted(phrase):\n",
    "    count = 0\n",
    "\n",
    "    # specific\n",
    "    for pattern, replacement in specific_patterns:\n",
    "        matches = len(pattern.findall(phrase))\n",
    "        count += matches\n",
    "        phrase = pattern.sub(replacement, phrase)\n",
    "\n",
    "    # general\n",
    "    for pattern, replacement in general_patterns + extended_patterns:\n",
    "        matches = len(pattern.findall(phrase))\n",
    "        count += matches\n",
    "        phrase = pattern.sub(replacement, phrase)\n",
    "\n",
    "    return phrase, count\n",
    "\n",
    "# Apply the function to expand abbreviations\n",
    "cleaned_data['cleaned_content'], abbreviation_counts = zip(*cleaned_data['cleaned_content'].apply(decontracted))\n",
    "print(\"Expanded abbreviations:\", sum(abbreviation_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters excl. punctuation\n",
    "def remove_non_alphanumeric(text):\n",
    "    special_chars = re.findall(r'[^a-zA-Z0-9\\s.,!?\\'\"]', text)\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"]', ' ', text), len(special_chars)\n",
    "\n",
    "cleaned_data['cleaned_content'], special_char_count = zip(*cleaned_data['cleaned_content'].apply(remove_non_alphanumeric))\n",
    "print(\"Special characters removed:\", sum(special_char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    return {\"word_tokens\": word_tokens, \"sentence_tokens\": sentence_tokens}\n",
    "\n",
    "cleaned_data['tokenized_content'] = cleaned_data['cleaned_content'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_lemmatize(tokenized_content):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    word_tokens = tokenized_content[\"word_tokens\"]\n",
    "    filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "    return {\"word_tokens\": lemmatized_words, \"sentence_tokens\": tokenized_content[\"sentence_tokens\"]}, len(word_tokens) - len(filtered_words)\n",
    "\n",
    "cleaned_data['cleaned_tokenized_content'], stopwords_count = zip(*cleaned_data['tokenized_content'].apply(remove_stopwords_and_lemmatize))\n",
    "\n",
    "print(\"Stopwords removed:\", sum(stopwords_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_data['cleaned_tokenized_content'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminaries\n",
    "- Sentence segmentation\n",
    "- Word tokenization\n",
    "- Normalization\n",
    "\n",
    "Frequent preprocessing\n",
    "- Stopword removal\n",
    "- Stemming and/or lemmatization\n",
    "- Digits/Punctuations removal\n",
    "- Case normalization\n",
    "\n",
    "Task-specific preprocessing\n",
    "- Unicode normalization\n",
    "- Language detection\n",
    "- Code mixing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
