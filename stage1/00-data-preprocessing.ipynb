{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # library for data manipulation and analysis\n",
    "import re  # library for regular expressions\n",
    "import os  # library for operating system dependent functionality\n",
    "import string  # library for string operations\n",
    "import nltk  # library for natural language processing\n",
    "import spacy  # library for advanced natural language processing\n",
    "import requests  # library for making HTTP requests\n",
    "import contractions  # library for expanding contractions\n",
    "from langdetect import detect  # library for language detection\n",
    "from nltk.corpus import stopwords  # library for stop words\n",
    "from nltk.stem import WordNetLemmatizer  # library for lemmatizing words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # libraries for tokenizing text\n",
    "from unidecode import unidecode  # library for converting accented characters\n",
    "from bs4 import BeautifulSoup  # library for parsing HTML and XML documents\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # library for sentiment analysis\n",
    "nltk.download('vader_lexicon')  # download the VADER sentiment analysis lexicon\n",
    "nltk.download('wordnet')  # download WordNet for lemmatization\n",
    "nltk.download('stopwords')  # download stopwords for text preprocessing\n",
    "nltk.download('punkt')  # download the Punkt tokenizer for sentence segmentation\n",
    "nltk.download('maxent_ne_chunker')  # download the maximum entropy chunker for named entity recognition\n",
    "nltk.download('averaged_perceptron_tagger')  # download the averaged perceptron tagger for part-of-speech tagging\n",
    "nltk.download('words')  # download the NLTK corpus of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# Download a spacy model, can also be adjusted (medium = en_core_web_sm, large = en_core_web_lg)\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = '../data/esg_documents_for_dax_companies.csv'\n",
    "\n",
    "# Check if the directory and file exist\n",
    "if os.path.exists(file_path):\n",
    "    # Read the dat\n",
    "    raw_data = pd.read_csv(file_path, delimiter='|', index_col=0)\n",
    "else:\n",
    "    print(f'The file {file_path} does not exist, please download the file and store it in \"/data\".')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beiersdorf AG</td>\n",
       "      <td>Sustainability Highlight Report CARE BEYOND SK...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'GHGEmission', 'ProductLiabilit...</td>\n",
       "      <td>1</td>\n",
       "      <td>BEI</td>\n",
       "      <td>BeiersdorfAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>Corporate Responsibility Report 2021 2 Content...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DTE</td>\n",
       "      <td>DeutscheTelekomAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vonovia SE</td>\n",
       "      <td>VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'DataSecurity', 'Vaccine', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>VNA</td>\n",
       "      <td>VonoviaSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merck KGaA</td>\n",
       "      <td>Sustainability Report 2021 TABLE OF CONTENTS S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'DataMisuse', 'DrugResistance...</td>\n",
       "      <td>1</td>\n",
       "      <td>MRK</td>\n",
       "      <td>MerckKGaA Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTU</td>\n",
       "      <td>Our ideas and concepts FOR A SUSTAINABLE FUTUR...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'AirQuality'...</td>\n",
       "      <td>1</td>\n",
       "      <td>MTX</td>\n",
       "      <td>MTUAeroEngines Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>#StandWithUkraine Sustainability Report 2021 C...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>E.ONSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RWE AG</td>\n",
       "      <td>Focus on tomorrow. Sustainability Report 2021 ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'Iso50001', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>RWE</td>\n",
       "      <td>RWEAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Annual Report 2021 HeidelbergCement at a glanc...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Vaccine', 'DataSecurity',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Annual Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Company Strategy &amp; Business &amp; Product &amp; Produc...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'Corruption', 'Whistleblowing',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>Sustainability 1 Siemens 2 Our 3 Governance – ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'EmployeeTurnover...</td>\n",
       "      <td>1</td>\n",
       "      <td>SIE</td>\n",
       "      <td>SiemensAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company                                            content   \n",
       "0         Beiersdorf AG  Sustainability Highlight Report CARE BEYOND SK...  \\\n",
       "1   Deutsche Telekom AG  Corporate Responsibility Report 2021 2 Content...   \n",
       "2            Vonovia SE  VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...   \n",
       "3            Merck KGaA  Sustainability Report 2021 TABLE OF CONTENTS S...   \n",
       "4                   MTU  Our ideas and concepts FOR A SUSTAINABLE FUTUR...   \n",
       "5                E ONSE  #StandWithUkraine Sustainability Report 2021 C...   \n",
       "6                RWE AG  Focus on tomorrow. Sustainability Report 2021 ...   \n",
       "7  Heidelberg Cement AG  Annual Report 2021 HeidelbergCement at a glanc...   \n",
       "8  Heidelberg Cement AG  Company Strategy & Business & Product & Produc...   \n",
       "9            Siemens AG  Sustainability 1 Siemens 2 Our 3 Governance – ...   \n",
       "\n",
       "                datatype        date domain   \n",
       "0  sustainability_report  2021-03-31    NaN  \\\n",
       "1  sustainability_report  2021-03-31    NaN   \n",
       "2  sustainability_report  2021-03-31    NaN   \n",
       "3  sustainability_report  2021-03-31    NaN   \n",
       "4  sustainability_report  2020-03-31    NaN   \n",
       "5  sustainability_report  2021-03-31    NaN   \n",
       "6  sustainability_report  2021-03-31    NaN   \n",
       "7          annual_report  2021-03-31    NaN   \n",
       "8  sustainability_report  2020-03-31    NaN   \n",
       "9  sustainability_report  2020-03-31    NaN   \n",
       "\n",
       "                                          esg_topics  internal symbol   \n",
       "0  ['CleanWater', 'GHGEmission', 'ProductLiabilit...         1    BEI  \\\n",
       "1  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1    DTE   \n",
       "2  ['Whistleblowing', 'DataSecurity', 'Vaccine', ...         1    VNA   \n",
       "3  ['DataSecurity', 'DataMisuse', 'DrugResistance...         1    MRK   \n",
       "4  ['WorkLifeBalance', 'Corruption', 'AirQuality'...         1    MTX   \n",
       "5  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1   EOAN   \n",
       "6  ['WorkLifeBalance', 'Corruption', 'Iso50001', ...         1    RWE   \n",
       "7  ['WorkLifeBalance', 'Vaccine', 'DataSecurity',...         1    HEI   \n",
       "8  ['CleanWater', 'Corruption', 'Whistleblowing',...         1    HEI   \n",
       "9  ['DataSecurity', 'Iso50001', 'EmployeeTurnover...         1    SIE   \n",
       "\n",
       "                                           title  url  \n",
       "0        BeiersdorfAG Sustainability Report 2021  NaN  \n",
       "1   DeutscheTelekomAG Sustainability Report 2021  NaN  \n",
       "2           VonoviaSE Sustainability Report 2021  NaN  \n",
       "3           MerckKGaA Sustainability Report 2021  NaN  \n",
       "4      MTUAeroEngines Sustainability Report 2020  NaN  \n",
       "5              E.ONSE Sustainability Report 2021  NaN  \n",
       "6               RWEAG Sustainability Report 2021  NaN  \n",
       "7          HeidelbergCementAG Annual Report 2021  NaN  \n",
       "8  HeidelbergCementAG Sustainability Report 2020  NaN  \n",
       "9           SiemensAG Sustainability Report 2020  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded data and reset index\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column descriptions**\n",
    "- symbol: stock symbol of the company\n",
    "- company: company name\n",
    "- date: publication date of document\n",
    "- title: document title\n",
    "- content: document content\n",
    "- datatype: document type\n",
    "- internal: is this a report by company (1) or a third-party document (0)\n",
    "- domain (optional): Web domain where the document was published\n",
    "- url (optional): URL where the document can be accessed\n",
    "- esg_topics (optional): ESG topics extracted from the data using our internal NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11188, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape (row and column amount)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company       object\n",
       "content       object\n",
       "datatype      object\n",
       "date          object\n",
       "domain        object\n",
       "esg_topics    object\n",
       "internal       int64\n",
       "symbol        object\n",
       "title         object\n",
       "url           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "raw_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_checkpoint(df, filename='checkpoint'):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a CSV file and loads it back into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to save and load.\n",
    "        filename (str): The name of the CSV file to save the DataFrame to (default: 'checkpoint').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../data/checkpoints/'):  # Check if the directory exists and create it if it doesn't\n",
    "        os.makedirs('../data/checkpoints/')\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(f'../data/checkpoints/{filename}.csv', index=False, sep='|')  # Save DataFrame to CSV with specified filename\n",
    "    print(f'Saved DataFrame to {filename}.csv')\n",
    "\n",
    "    # Load CSV back into DataFrame\n",
    "    df = pd.read_csv(f'../data/checkpoints/{filename}.csv', delimiter='|')  # Load CSV back into DataFrame\n",
    "    print(f'Loaded DataFrame from {filename}.csv')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As initial data cleaning steps, the following is conducted:\n",
    "- Rows with missing \"content\" were dropped to prevent any missing data-related issues. Missing data can create gaps in the data and lead to errors or distortions in the analysis.\n",
    "- The \"URL\" column was removed as the relevant information was available in the \"domain\" column. Removing redundant columns simplifies the data set and makes it easier to work with\n",
    "- Duplicate entries were identified and removed, resulting in a cleaner and more concise dataset. Duplicates can distort the data and lead to biased analysis. \n",
    "- Language checking was conducted and all rows with non-English content were dropped to ensure consistent language. Language inconsistencies can create bias in the data and lead to inaccurate conclusions. Therefore, it is important to ensure that the data is consistent in language to prevent linguistic biases.\n",
    "- \"Date\" is formatted as a date and wrong dates, e.g. \"bayer-03-31\" are replaced with a default date (2023-03-31).\n",
    "- Remove company name parts like \"AG\" for clarity\n",
    "- The \"sample\" method was used to check the data for representativeness and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_cleaned_data = raw_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"url\" column, since the most relevant information from an analysis perspective is already in the \"domain\" column (e.g. the source of the report)\n",
    "general_cleaned_data = general_cleaned_data.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 6\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and delete them\n",
    "duplicates = general_cleaned_data[general_cleaned_data.duplicated()]\n",
    "print(f'Duplicated rows: {len(duplicates)}')\n",
    "general_cleaned_data = general_cleaned_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted amount of rows with language other ehan English: 106\n"
     ]
    }
   ],
   "source": [
    "# Check for other languange than English\n",
    "general_cleaned_data['language'] = general_cleaned_data['content'].apply(lambda x: detect(x))\n",
    "not_english = len(general_cleaned_data) - len(general_cleaned_data.loc[general_cleaned_data['language'] == 'en'])\n",
    "\n",
    "# Drop rows with other languange, since other languanges influences to quality of the later analysis\n",
    "general_cleaned_data = general_cleaned_data.loc[general_cleaned_data['language'] == 'en']\n",
    "\n",
    "print(f'Deleted amount of rows with language other ehan English: {not_english}')\n",
    "general_cleaned_data.drop(['language'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly formatted dates:\n",
      "Row 13: p.DE-03-31\n",
      "Row 18: p.DE-03-31\n",
      "Row 20: bayer-03-31\n",
      "Row 22: p.DE-03-31\n",
      "Row 25: p.DE-03-31\n",
      "Row 26: p.DE-03-31\n",
      "Row 31: p.DE-03-31\n",
      "Row 32: p.DE-03-31\n",
      "Row 33: p.DE-03-31\n",
      "Row 37: p.DE-03-31\n",
      "Row 41: p.DE-03-31\n",
      "Row 50: p.DE-03-31\n",
      "Row 78: p.DE-03-31\n",
      "Row 80: p.DE-03-31\n",
      "Row 86: p.DE-03-31\n",
      "Row 87: p.DE-03-31\n",
      "Row 88: p.DE-03-31\n"
     ]
    }
   ],
   "source": [
    "# Correct the dates to ISO standard\n",
    "def find_incorrect_dates(data):\n",
    "    incorrect_dates = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            pd.to_datetime(row['date'], format='%Y-%m-%d', errors='raise')\n",
    "        except ValueError:\n",
    "            incorrect_dates.append((index, row['date']))\n",
    "\n",
    "    return incorrect_dates\n",
    "\n",
    "incorrect_date_rows = find_incorrect_dates(general_cleaned_data)\n",
    "print(\"Incorrectly formatted dates:\")\n",
    "for index, date in incorrect_date_rows:\n",
    "    print(f\"Row {index}: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the wrong formatted dates and set default date\n",
    "def correct_date_format(data):\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce').fillna('2022-03-31')\n",
    "    return data\n",
    "\n",
    "general_cleaned_data = correct_date_format(general_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace company name parts like \"AG\" to have a cleaner name\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' AG', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' SE', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' KGaA', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Laser specialist TRUMPF has for the first time...</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-07-04</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['CarbonDioxide']</td>\n",
       "      <td>0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>Vitesco Technologies: Increased sustainability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>Airbus</td>\n",
       "      <td>European aircraft manufacturer Airbus has spar...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2022-02-08</td>\n",
       "      <td>airport-technology</td>\n",
       "      <td>['Privacy', 'extremism']</td>\n",
       "      <td>0</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Airbus warns it could leave the UK in case of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>Cost and carbon savings from electric vehicles...</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>edie</td>\n",
       "      <td>['Environment', 'FossilFuels', 'EMobility', 'G...</td>\n",
       "      <td>0</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>Convenience is key, says E.ON in new drive to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Volkswagen's IT board member Hauke Stars belie...</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['Environment']</td>\n",
       "      <td>0</td>\n",
       "      <td>VOW3</td>\n",
       "      <td>VW IT boss after Conti hack: business-critical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6651</th>\n",
       "      <td>Merck</td>\n",
       "      <td>You are currently accessing Investment Week vi...</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>investmentweek</td>\n",
       "      <td>['CustomerService', 'Governance']</td>\n",
       "      <td>0</td>\n",
       "      <td>MRK</td>\n",
       "      <td>Deep Dive: Will newer or older artists win the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company                                            content   \n",
       "1983          BMW  Laser specialist TRUMPF has for the first time...  \\\n",
       "1022       Airbus  European aircraft manufacturer Airbus has spar...   \n",
       "5265       E ONSE  Cost and carbon savings from electric vehicles...   \n",
       "10430  Volkswagen  Volkswagen's IT board member Hauke Stars belie...   \n",
       "6651        Merck  You are currently accessing Investment Week vi...   \n",
       "\n",
       "       datatype        date              domain   \n",
       "1983   business  2022-07-04      marketscreener  \\\n",
       "1022       tech  2022-02-08  airport-technology   \n",
       "5265    general  2022-11-10                edie   \n",
       "10430  business  2022-12-22      marketscreener   \n",
       "6651   business  2022-10-14      investmentweek   \n",
       "\n",
       "                                              esg_topics  internal symbol   \n",
       "1983                                   ['CarbonDioxide']         0    BMW  \\\n",
       "1022                            ['Privacy', 'extremism']         0    AIR   \n",
       "5265   ['Environment', 'FossilFuels', 'EMobility', 'G...         0   EOAN   \n",
       "10430                                    ['Environment']         0   VOW3   \n",
       "6651                   ['CustomerService', 'Governance']         0    MRK   \n",
       "\n",
       "                                                   title  \n",
       "1983   Vitesco Technologies: Increased sustainability...  \n",
       "1022   Airbus warns it could leave the UK in case of ...  \n",
       "5265   Convenience is key, says E.ON in new drive to ...  \n",
       "10430  VW IT boss after Conti hack: business-critical...  \n",
       "6651   Deep Dive: Will newer or older artists win the...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the date with some samples\n",
    "general_cleaned_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of \"Muenchener Rueckversicherungs Gesellschaft AGin Muenchen\" to something more readable\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].replace('Muenchener Rueckversicherungs Gesellschaftin Muenchen', 'Munich R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to general_cleaned_data.csv\n",
      "Loaded DataFrame from general_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "general_cleaned_data = csv_checkpoint(general_cleaned_data, 'general_cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning & Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"content\" column, containing the text of the reports, undergoes a series of cleaning, normalization, and preprocessing steps to ensure accurate and efficient analysis. These steps include:\n",
    "\n",
    "- **String conversion**: Converting the input to a string format ensures consistency and compatibility during subsequent processing tasks.\n",
    "- **Lowercase conversion**: Transforming all text to lowercase serves as a simple normalization step, reducing the complexity and variability of the input data.\n",
    "- **Unicode decoding**: Removing diacritics (e.g., accented characters) and normalizing the text encoding mitigates potential discrepancies arising from different encoding formats.\n",
    "- **URL and email address removal**: Eliminating URLs and email addresses reduces noise in the dataset, as these elements do not contribute valuable information for the analysis.\n",
    "- **Extra whitespace removal**: Eradicating extra whitespaces improves text analysis and tokenization by ensuring that only meaningful spaces are retained.\n",
    "- **Contact detail removal**: Excluding phone numbers, contact person strings, and social media references further minimizes noise in the dataset, honing the focus on relevant text.\n",
    "- **Table of contents removal**: Discarding the table of contents enhances the data quality by eliminating repetitive and non-essential information.\n",
    "- **Named entity removal**: Employing the spaCy model to remove human names and other named entities optimizes the text for analysis and modeling by concentrating on pertinent content.\n",
    "- **Abbreviation expansion**: Utilizing the contractions library and custom functions with regular expressions, common and uncommon abbreviations are expanded to improve text interpretation.\n",
    "- **Special character elimination**: Excluding all special characters, except punctuation, refines the input data. Retaining punctuation is necessary for accurate sentence tokenization and removed after sentence tokenization..\n",
    "- **Tokenization and lemmatization**: Tokenizing words and sentences, and subsequently lemmatizing words using the WordNetLemmatizer from nltk, streamlines the text and reduces morphological variations.\n",
    "- **Stopword removal**: Customizing the nltk stopwords list by adding or removing specific stopwords enables more precise and tailored text analysis.\n",
    "- **Part-of-speech (POS) tagging**: Assigning POS tags to words and sentences enhances the text representation by providing additional linguistic information, which may be beneficial for subsequent analysis and modeling tasks.\n",
    "- **Sentiment Analysis**: Basic sentiment value calculation on the tokenzued sentences to get first insights im terms of the sentiments in the reports within the EDA.\n",
    "\n",
    "Spellchecking was tested with TextBlob and PySpellChecker but deliverd not useful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = general_cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name entities removed: 94745\n"
     ]
    }
   ],
   "source": [
    "# Since the spacy model shows better results on the \"raw\" text, the named entity removal is conducted before all normalization and cleaning steps\n",
    "spacy_model = spacy.load('en_core_web_md')\n",
    "spacy_model.max_length = 1800000 # Increase max text length\n",
    "\n",
    "def remove_named_entities(text):\n",
    "    \"\"\"\n",
    "    Removes named entities from text and returns the modified text and the count of named entities removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to remove named entities from.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified text (str) and the count of named entities removed (int).\n",
    "    \"\"\"\n",
    "    doc = spacy_model(text)\n",
    "    \n",
    "    named_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\"]:\n",
    "            named_entities.add(ent.text)\n",
    "    \n",
    "    named_entities_count = len(named_entities)\n",
    "    \n",
    "    for named_entity in named_entities:\n",
    "        text = text.replace(named_entity, '')\n",
    "    \n",
    "    return text, named_entities_count\n",
    "\n",
    "# Assuming cleaned_data is a pandas DataFrame with a 'content' column\n",
    "cleaned_data['cleaned_content'], name_entity_count = zip(*cleaned_data['content'].apply(remove_named_entities))\n",
    "print(\"Name entities removed:\", sum(name_entity_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs removed: 7496\n",
      "Mail addresses removed: 434\n",
      "Extra whitespaces removed: 149381\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    urls = re.findall(r'http\\S+|www\\S+|https\\S+', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE), len(urls)\n",
    "\n",
    "def remove_emails(text):\n",
    "    mail_addresses = re.findall(r'\\S+@\\S+\\s?', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'\\S+@\\S+\\s?', '', text, flags=re.MULTILINE), len(mail_addresses)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    extra_spaces = re.findall(r'\\s{2,}', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip(), len(extra_spaces)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].astype(str) # Convert all texts to string\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: x.lower()) # Convert all texts to lower-case\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: unidecode(x, errors=\"preserve\")) # Remove diacritics / accented characters and unicode normalization\n",
    "cleaned_data['cleaned_content'], url_count = zip(*cleaned_data['cleaned_content'].apply(remove_urls)) # Remove URLs from texts\n",
    "cleaned_data['cleaned_content'], email_count = zip(*cleaned_data['cleaned_content'].apply(remove_emails)) # Remove e-mail addresses from texts\n",
    "cleaned_data['cleaned_content'], extra_space_count = zip(*cleaned_data['cleaned_content'].apply(remove_extra_whitespace)) # Remove extra whitespaces from texts\n",
    "\n",
    "print(\"URLs removed:\", sum(url_count))\n",
    "print(\"Mail addresses removed:\", sum(email_count))\n",
    "print(\"Extra whitespaces removed:\", sum(extra_space_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact information removed: 44260\n",
      "TOCs removed: 9607\n"
     ]
    }
   ],
   "source": [
    "def remove_contact_details(text):\n",
    "    # Remove phone numbers\n",
    "    phone_regex = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
    "    phone_count = len(re.findall(phone_regex, text))\n",
    "    text = re.sub(phone_regex, '', text)\n",
    "\n",
    "    # Remove common contact-related phrases\n",
    "    contact_phrases_regex = r'\\b(?:Contact Person|Phone|Tel|Fax|Mobile|E?mail|Skype|Twitter|Facebook|LinkedIn|Website):\\b'\n",
    "    contact_phrases_count = len(re.findall(contact_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(contact_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    total_count = phone_count + contact_phrases_count\n",
    "    return text, total_count\n",
    "\n",
    "def remove_table_of_contents(text):\n",
    "    # Remove common table of contents phrases\n",
    "    toc_phrases_regex = r'\\b(?:Table of Contents|Contents)\\b'\n",
    "    toc_phrases_count = len(re.findall(toc_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(toc_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove content with numbering like \"1. Introduction\", \"1.1. Background\", \"A. Overview\", etc.\n",
    "    toc_entries_regex = r'(^|\\n)\\s*\\w+(\\.\\w+)*\\s+\\w+([\\w\\s]+)?'\n",
    "    toc_entries_count = len(re.findall(toc_entries_regex, text))\n",
    "    text = re.sub(toc_entries_regex, '', text)\n",
    "\n",
    "    total_count = toc_phrases_count + toc_entries_count\n",
    "    return text, total_count\n",
    "\n",
    "cleaned_data['cleaned_content'], contact_count = zip(*cleaned_data['cleaned_content'].apply(remove_contact_details))\n",
    "cleaned_data['cleaned_content'], toc_count = zip(*cleaned_data['cleaned_content'].apply(remove_table_of_contents))\n",
    "print(\"Contact information removed:\", sum(contact_count))\n",
    "print(\"TOCs removed:\", sum(toc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = []\n",
    "    for word in text.split():\n",
    "        expanded_text.append(contractions.fix(word))\n",
    "    expanded_text = ' '.join(expanded_text)\n",
    "    return contractions.fix(expanded_text)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded custom abbreviations: 0\n"
     ]
    }
   ],
   "source": [
    "# Expand custom abbreviations which are not captured by \"contractions\"\n",
    "# Basic idea from: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# Compile the regular expressions only once for efficiency\n",
    "specific_patterns = [\n",
    "    (re.compile(r\"won['’]t\"), \"will not\"),\n",
    "    (re.compile(r\"can['’]t\"), \"can not\"),\n",
    "]\n",
    "\n",
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Expands contractions in a given phrase and returns the modified phrase and the count of contractions expanded.\n",
    "\n",
    "    Args:\n",
    "        phrase (str): The phrase to expand contractions in.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified phrase (str) and the count of contractions expanded (int).\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    # Replace specific patterns\n",
    "    for pattern, replacement in specific_patterns:\n",
    "        matches = len(pattern.findall(phrase))\n",
    "        count += matches\n",
    "        phrase = pattern.sub(replacement, phrase)\n",
    "\n",
    "    return phrase, count\n",
    "\n",
    "# Apply the function to expand abbreviations\n",
    "cleaned_data['cleaned_content'], abbreviation_counts = zip(*cleaned_data['cleaned_content'].apply(decontracted))\n",
    "print(\"Expanded custom abbreviations:\", sum(abbreviation_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters excl. punctuation removed: 1388738\n"
     ]
    }
   ],
   "source": [
    "# Remove special characters excl. punctuation since this is needed by the sentence tokenization\n",
    "def remove_non_alphanumeric(text, remove_punctuation=False):\n",
    "    if remove_punctuation:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s.,!?\\'\"]'\n",
    "    \n",
    "    special_chars = re.findall(pattern, text)\n",
    "    return re.sub(pattern, '', text), len(special_chars)\n",
    "\n",
    "cleaned_data['cleaned_content'], special_char_count = zip(*cleaned_data['cleaned_content'].apply(remove_non_alphanumeric, remove_punctuation=False))\n",
    "print(\"Special characters excl. punctuation removed:\", sum(special_char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word token amount: 16963356\n"
     ]
    }
   ],
   "source": [
    "def tokenize_words(text):\n",
    "    # Remove numbers, digits, and punctuation\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['word_tokens'], word_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_words))\n",
    "print(\"Generated word token amount:\", sum(word_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence token amount: 687669\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(text):\n",
    "    # Tokenize sentences\n",
    "    tokens = sent_tokenize(text)\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['sentence_tokens'], sentence_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_sentences))\n",
    "print(\"Generated sentence token amount:\", sum(sentence_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stopwords in word tokens 6852067\n",
      "Removed stopwords in sentence tokens 9661393\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_word_tokens(tokens, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Removes stopwords and one-character tokens from a list of word tokens and returns the modified list and the count of removed items.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): The list of word tokens to remove stopwords from.\n",
    "        custom_stopwords (list): A list of custom stopwords to remove from the word tokens.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified list of word tokens (list) and the count of removed items (int).\n",
    "    \"\"\"\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens\n",
    "        if token.lower() not in custom_stopwords and len(token) > 1\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens, len(tokens) - len(filtered_tokens)\n",
    "\n",
    "def remove_stopwords_from_sentence_tokens(sentences_list, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Removes stopwords, one-character tokens, digits, numbers, and special characters (excluding whitespace) from a list of sentence tokens and returns the modified list and the count of removed items.\n",
    "\n",
    "    Args:\n",
    "        sentences_list (list): The list of sentence tokens to remove stopwords from.\n",
    "        custom_stopwords (list): A list of custom stopwords to remove from the sentence tokens.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified list of sentence tokens (list) and the count of removed items (int).\n",
    "    \"\"\"\n",
    "    filtered_sentences_list = []\n",
    "    total_removed_items_count = 0\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        # Tokenize the sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords, one-character tokens, digits, numbers, and special characters (excluding whitespace) from word tokens\n",
    "        filtered_word_tokens = [\n",
    "            re.sub(rf\"[{re.escape(string.punctuation)}]\", '', token) for token in word_tokens\n",
    "            if token.lower() not in custom_stopwords\n",
    "            and len(token) > 1\n",
    "            and not re.search(r'\\d', token)\n",
    "            and not re.search(r'\\W', token)\n",
    "        ]\n",
    "\n",
    "        # Reconstruct the sentence without the removed words and special characters\n",
    "        filtered_sentence = ' '.join(filtered_word_tokens)\n",
    "        removed_items_count = len(word_tokens) - len(filtered_word_tokens)\n",
    "        filtered_sentences_list.append(filtered_sentence)\n",
    "        total_removed_items_count += removed_items_count\n",
    "\n",
    "    return filtered_sentences_list, total_removed_items_count\n",
    "\n",
    "# Define custom stopwords to add or remove (the extra stopwords were identified by the TFIDF based wordcloud)\n",
    "custom_stopwords = {\n",
    "    'add': ['said','company','companies','year','billion','million','siemens','linde','rwe','volkswagen','symrise','porsche','sap','adidas','puma','airbus','bmw','hannover','mtu','heiderbergcement','qiagen','benz','continental','bayer','fresenius','wa', 'ha', 'eur', 'allianz', 'board'],\n",
    "    'remove': [''] # Currently not needed\n",
    "}\n",
    "\n",
    "# Combine stopwords to filter the content of the reports\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "all_stopwords |= set(custom_stopwords['add'])\n",
    "all_stopwords -= set(custom_stopwords['remove'])\n",
    "\n",
    "cleaned_data['word_tokens'], stopword_count_words = zip(*cleaned_data['word_tokens'].apply(remove_stopwords_from_word_tokens, custom_stopwords=all_stopwords))\n",
    "cleaned_data['sentence_tokens'], stopword_count_sentences = zip(*cleaned_data['sentence_tokens'].apply(remove_stopwords_from_sentence_tokens, custom_stopwords=all_stopwords))\n",
    "\n",
    "print(\"Removed stopwords in word tokens\", sum(stopword_count_words))\n",
    "print(\"Removed stopwords in sentence tokens\", sum(stopword_count_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the cleaned content based on the cleaned word tokens\n",
    "cleaned_data['cleaned_content'] = cleaned_data['word_tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_tokens(word_tokens, sentence_list):\n",
    "    \"\"\"\n",
    "    Performs POS tagging on a given list of word tokens and a list of sentence tokens and returns the POS tagged word tokens and POS tagged sentence tokens.\n",
    "\n",
    "    Args:\n",
    "        word_tokens (list): The list of word tokens to perform POS tagging on.\n",
    "        sentence_list (list): The list of sentence tokens to perform POS tagging on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the POS tagged word tokens (list) and the POS tagged sentence tokens (list of lists).\n",
    "    \"\"\"\n",
    "    \n",
    "    # POS tagging for word tokens\n",
    "    pos_tagged_word_tokens = nltk.pos_tag(word_tokens)\n",
    "\n",
    "    # Create a dictionary to map word tokens to their POS tags, this reduces the effort to call nltk.pos_tag twice\n",
    "    pos_tags_dict = dict(pos_tagged_word_tokens)\n",
    "\n",
    "    # POS tagging for sentence tokens\n",
    "    pos_tagged_sentence_list = []\n",
    "    for sentence in sentence_list:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = [(token, pos_tags_dict[token]) for token in tokenized_sentence if token in pos_tags_dict]\n",
    "        pos_tagged_sentence_list.append(pos_tagged_sentence)\n",
    "\n",
    "    return pos_tagged_word_tokens, pos_tagged_sentence_list\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = cleaned_data.apply(lambda row: pos_tagging_tokens(row['word_tokens'], row['sentence_tokens']), axis=1)\n",
    "cleaned_data['pos_tagged_word_tokens'], cleaned_data['pos_tagged_sentence_tokens'] = zip(*pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to cleaned_data.csv\n",
      "Loaded DataFrame from cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "cleaned_data = csv_checkpoint(cleaned_data, 'cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several additional information could be helpful in the further analysis, which are not included in the dataset. Therefore a small scraper is used to enrich the the dataset with the sector, industry and market capitalization of the DAX companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>symbol</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linde plc</td>\n",
       "      <td>LIN</td>\n",
       "      <td>156.93</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Specialty Chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAP SE</td>\n",
       "      <td>SAP</td>\n",
       "      <td>121.03</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software—Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>SIE</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>DTE</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Airbus SE</td>\n",
       "      <td>AIR</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company_name symbol  market_cap_in_usd_b         country   \n",
       "0            Linde plc    LIN               156.93  United Kingdom  \\\n",
       "1               SAP SE    SAP               121.03         Germany   \n",
       "2           Siemens AG    SIE               110.13         Germany   \n",
       "3  Deutsche Telekom AG    DTE               101.78         Germany   \n",
       "4            Airbus SE    AIR                96.87     Netherlands   \n",
       "\n",
       "                   sector                        industry  \n",
       "0         Basic Materials             Specialty Chemicals  \n",
       "1              Technology            Software—Application  \n",
       "2             Industrials  Specialty Industrial Machinery  \n",
       "3  Communication Services                Telecom Services  \n",
       "4             Industrials             Aerospace & Defense  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://disfold.com/stock-index/dax/companies/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "scraped_data = []\n",
    "for row in table.find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    scraped_data.append(cols)\n",
    "\n",
    "def clean_scraped_data(data):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in data:\n",
    "        # Remove empty rows\n",
    "        if len(row) > 0:\n",
    "            # Remove the '$' and ',' signs from the market cap and convert it to float\n",
    "            market_cap = float(row[3].replace('$', '').replace(',', '').replace('B', ''))\n",
    "            cleaned_data.append([row[1], row[2], market_cap, row[4], row[5], row[6]])\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_data, columns=['company_name', 'symbol', 'market_cap_in_usd_b', 'country', 'sector', 'industry'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "company_enrichments = clean_scraped_data(scraped_data)\n",
    "company_enrichments.to_csv('../data/dax_company_sectors.csv', index=False)\n",
    "company_enrichments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the ticker symbols to prevent NaN and ensure correct join conditions\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('SRT3', 'SRT')\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('HEN3', 'HNK')\n",
    "company_enrichments.loc[company_enrichments['company_name'] == 'Mercedes-Benz Group AG', 'symbol'] = 'DAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['symbol'] = cleaned_data['symbol'].astype(pd.StringDtype())\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].astype(pd.StringDtype())\n",
    "\n",
    "# Merge the cleaned data with the enrichment\n",
    "enriched_cleaned_data = pd.merge(cleaned_data, company_enrichments, how='left', on='symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>company_name</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Sustainability Report 2020 We face up to futur...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'Vaccine', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Sustainability Repo...</td>\n",
       "      <td>approach purpose value reflect robust corporat...</td>\n",
       "      <td>['approach', 'purpose', 'value', 'reflect', 'r...</td>\n",
       "      <td>['', 'approach', 'purpose values reflect robus...</td>\n",
       "      <td>[('approach', 'NN'), ('purpose', 'JJ'), ('valu...</td>\n",
       "      <td>[[], [('approach', 'NN')], [('purpose', 'JJ'),...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Annual Report An overview Gross premium E 01 i...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Vaccine', 'Monopolization', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Annual Report 2021</td>\n",
       "      <td>group net income policyholder surplus book vue...</td>\n",
       "      <td>['group', 'net', 'income', 'policyholder', 'su...</td>\n",
       "      <td>['group net income policyholders', 'surplus bo...</td>\n",
       "      <td>[('group', 'NN'), ('net', 'JJ'), ('income', 'N...</td>\n",
       "      <td>[[('group', 'NN'), ('net', 'JJ'), ('income', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company                                            content   \n",
       "48  Hannover R  Sustainability Report 2020 We face up to futur...  \\\n",
       "76  Hannover R  Annual Report An overview Gross premium E 01 i...   \n",
       "\n",
       "                 datatype        date domain   \n",
       "48  sustainability_report  2020-03-31    NaN  \\\n",
       "76          annual_report  2021-03-31    NaN   \n",
       "\n",
       "                                           esg_topics  internal symbol   \n",
       "48  ['Whistleblowing', 'Vaccine', 'Corruption', 'G...         1   HNR1  \\\n",
       "76  ['Vaccine', 'Monopolization', 'Corruption', 'G...         1   HNR1   \n",
       "\n",
       "                                                title   \n",
       "48  HannoverRückversicherungAG Sustainability Repo...  \\\n",
       "76      HannoverRückversicherungAG Annual Report 2021   \n",
       "\n",
       "                                      cleaned_content   \n",
       "48  approach purpose value reflect robust corporat...  \\\n",
       "76  group net income policyholder surplus book vue...   \n",
       "\n",
       "                                          word_tokens   \n",
       "48  ['approach', 'purpose', 'value', 'reflect', 'r...  \\\n",
       "76  ['group', 'net', 'income', 'policyholder', 'su...   \n",
       "\n",
       "                                      sentence_tokens   \n",
       "48  ['', 'approach', 'purpose values reflect robus...  \\\n",
       "76  ['group net income policyholders', 'surplus bo...   \n",
       "\n",
       "                               pos_tagged_word_tokens   \n",
       "48  [('approach', 'NN'), ('purpose', 'JJ'), ('valu...  \\\n",
       "76  [('group', 'NN'), ('net', 'JJ'), ('income', 'N...   \n",
       "\n",
       "                           pos_tagged_sentence_tokens company_name   \n",
       "48  [[], [('approach', 'NN')], [('purpose', 'JJ'),...          NaN  \\\n",
       "76  [[('group', 'NN'), ('net', 'JJ'), ('income', '...          NaN   \n",
       "\n",
       "    market_cap_in_usd_b country sector industry  \n",
       "48                  NaN     NaN    NaN      NaN  \n",
       "76                  NaN     NaN    NaN      NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_cleaned_data[enriched_cleaned_data['industry'].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hannover R AG cannot be matched, since it is not present in the scraped data. Since there are only 2 records this is negligible and will be fixed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'sector'] = 'Financials'\n",
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'industry'] = 'Insurance—Reinsurance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns/data\n",
    "enriched_cleaned_data = enriched_cleaned_data.drop(columns=['content', 'company_name', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9113</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-10-03</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Cybersecurity', 'Social', 'GenderDiversity']</td>\n",
       "      <td>0</td>\n",
       "      <td>SIE</td>\n",
       "      <td>Siemens Healthineers Demonstrates Artificial I...</td>\n",
       "      <td>healthcare information management system socie...</td>\n",
       "      <td>['healthcare', 'information', 'management', 's...</td>\n",
       "      <td>['healthcare information management systems so...</td>\n",
       "      <td>[('healthcare', 'NN'), ('information', 'NN'), ...</td>\n",
       "      <td>[[('healthcare', 'NN'), ('information', 'NN'),...</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>Infineon Technologies</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>smart2zero</td>\n",
       "      <td>['EMobility']</td>\n",
       "      <td>0</td>\n",
       "      <td>IFX</td>\n",
       "      <td>Power MOSFET packages for high current designs</td>\n",
       "      <td>ruggedness extended lifetime ask would right p...</td>\n",
       "      <td>['ruggedness', 'extended', 'lifetime', 'ask', ...</td>\n",
       "      <td>['ruggedness extended lifetime', 'ask would ri...</td>\n",
       "      <td>[('ruggedness', 'NN'), ('extended', 'VBD'), ('...</td>\n",
       "      <td>[[('ruggedness', 'NN'), ('extended', 'VBD'), (...</td>\n",
       "      <td>41.33</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9459</th>\n",
       "      <td>Siemens Energy</td>\n",
       "      <td>general</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['Electrolysis', 'Compliance', 'RenewableEnerg...</td>\n",
       "      <td>0</td>\n",
       "      <td>ENR</td>\n",
       "      <td>Industry Mixed Over EU's New Green Hydrogen Rules</td>\n",
       "      <td>recently released proposed rule set renewable ...</td>\n",
       "      <td>['recently', 'released', 'proposed', 'rule', '...</td>\n",
       "      <td>['recently released proposed rule set renewabl...</td>\n",
       "      <td>[('recently', 'RB'), ('released', 'VBN'), ('pr...</td>\n",
       "      <td>[[('recently', 'RB'), ('released', 'VBN'), ('p...</td>\n",
       "      <td>13.70</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Utilities—Independent Power Producers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>Airbus</td>\n",
       "      <td>tech</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>simpleflying</td>\n",
       "      <td>['Transparency', 'Environment']</td>\n",
       "      <td>0</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Airbus Updates 2022 Delivery Estimate To 700 A...</td>\n",
       "      <td>say aim deliver around aircraft slightly fewer...</td>\n",
       "      <td>['say', 'aim', 'deliver', 'around', 'aircraft'...</td>\n",
       "      <td>['', 'says aims deliver around aircraft slight...</td>\n",
       "      <td>[('say', 'VB'), ('aim', 'NN'), ('deliver', 'NN...</td>\n",
       "      <td>[[], [('deliver', 'VB'), ('around', 'IN'), ('a...</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11007</th>\n",
       "      <td>Zalando</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-05-28</td>\n",
       "      <td>textileworld</td>\n",
       "      <td>['SustainableFibre', 'ValueChain', 'Sustainabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>ZAL</td>\n",
       "      <td>Sustainable Apparel Coalition, Higg Launch New...</td>\n",
       "      <td>amsterdam may today sustainable apparel coalit...</td>\n",
       "      <td>['amsterdam', 'may', 'today', 'sustainable', '...</td>\n",
       "      <td>['amsterdam may today sustainable apparel coal...</td>\n",
       "      <td>[('amsterdam', 'NN'), ('may', 'MD'), ('today',...</td>\n",
       "      <td>[[('amsterdam', 'NN'), ('may', 'MD'), ('today'...</td>\n",
       "      <td>10.41</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Internet Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>Qiagen</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>livescience</td>\n",
       "      <td>['Diversity', 'Aquaculture', 'Compliance', 'Be...</td>\n",
       "      <td>0</td>\n",
       "      <td>QIA</td>\n",
       "      <td>Pupal cannibalism by worker honey bees contrib...</td>\n",
       "      <td>com using browser version limited support cs o...</td>\n",
       "      <td>['com', 'using', 'browser', 'version', 'limite...</td>\n",
       "      <td>['', 'using browser version limited support cs...</td>\n",
       "      <td>[('com', 'NN'), ('using', 'VBG'), ('browser', ...</td>\n",
       "      <td>[[], [('using', 'VBG'), ('browser', 'NN'), ('v...</td>\n",
       "      <td>11.38</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8786</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-10-03</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Cybersecurity', 'Oversight', 'Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>SIE</td>\n",
       "      <td>Why We Have to Pay Attention to AI Right Now</td>\n",
       "      <td>run consulting service freiherr group snapshot...</td>\n",
       "      <td>['run', 'consulting', 'service', 'freiherr', '...</td>\n",
       "      <td>['', 'runs consulting service freiherr group',...</td>\n",
       "      <td>[('run', 'VB'), ('consulting', 'VBG'), ('servi...</td>\n",
       "      <td>[[], [('consulting', 'VBG'), ('service', 'NN')...</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>Merck</td>\n",
       "      <td>tech</td>\n",
       "      <td>2022-12-06</td>\n",
       "      <td>biopharmadive</td>\n",
       "      <td>['HumanCapital', 'Recruiting']</td>\n",
       "      <td>0</td>\n",
       "      <td>MRK</td>\n",
       "      <td>With $ 81M, Entact Bio takes a new approach to...</td>\n",
       "      <td>otherwise caught pathology disease contrast tr...</td>\n",
       "      <td>['otherwise', 'caught', 'pathology', 'disease'...</td>\n",
       "      <td>['otherwise caught pathology disease', 'contra...</td>\n",
       "      <td>[('otherwise', 'RB'), ('caught', 'VBN'), ('pat...</td>\n",
       "      <td>[[('otherwise', 'RB'), ('caught', 'VBN'), ('pa...</td>\n",
       "      <td>87.64</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Drug Manufacturers—Specialty &amp; Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>Heidelberg Cement</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-07-21</td>\n",
       "      <td>dezeen</td>\n",
       "      <td>['ClimateTech', 'CarbonCaptureAndStorage', 'Ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>HEI</td>\n",
       "      <td>Norway begins `` absolutely necessary '' proje...</td>\n",
       "      <td>EUR1 project could bury vast amount captured i...</td>\n",
       "      <td>['EUR1', 'project', 'could', 'bury', 'vast', '...</td>\n",
       "      <td>['project could bury vast amounts captured ini...</td>\n",
       "      <td>[('EUR1', 'NNP'), ('project', 'NN'), ('could',...</td>\n",
       "      <td>[[('project', 'JJ'), ('could', 'MD'), ('bury',...</td>\n",
       "      <td>11.91</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Building Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8872</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>thefishsite</td>\n",
       "      <td>['ValueChain', 'Aquaculture']</td>\n",
       "      <td>0</td>\n",
       "      <td>SIE</td>\n",
       "      <td>A new name in automated aquafeed systems</td>\n",
       "      <td>inspired food manufacturing engineering indust...</td>\n",
       "      <td>['inspired', 'food', 'manufacturing', 'enginee...</td>\n",
       "      <td>['inspired food manufacturing engineering indu...</td>\n",
       "      <td>[('inspired', 'VBN'), ('food', 'NN'), ('manufa...</td>\n",
       "      <td>[[('inspired', 'VBN'), ('food', 'NN'), ('manuf...</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     company  datatype        date         domain   \n",
       "9113                 Siemens   general  2021-10-03      itnonline  \\\n",
       "5826   Infineon Technologies      tech  2021-07-12     smart2zero   \n",
       "9459          Siemens Energy   general  2023-02-28    energyintel   \n",
       "915                   Airbus      tech  2022-07-27   simpleflying   \n",
       "11007                Zalando   general  2021-05-28   textileworld   \n",
       "7565                  Qiagen   general  2021-04-26    livescience   \n",
       "8786                 Siemens   general  2021-10-03      itnonline   \n",
       "6915                   Merck      tech  2022-12-06  biopharmadive   \n",
       "5620       Heidelberg Cement  business  2021-07-21         dezeen   \n",
       "8872                 Siemens   general  2022-09-27    thefishsite   \n",
       "\n",
       "                                              esg_topics  internal symbol   \n",
       "9113      ['Cybersecurity', 'Social', 'GenderDiversity']         0    SIE  \\\n",
       "5826                                       ['EMobility']         0    IFX   \n",
       "9459   ['Electrolysis', 'Compliance', 'RenewableEnerg...         0    ENR   \n",
       "915                      ['Transparency', 'Environment']         0    AIR   \n",
       "11007  ['SustainableFibre', 'ValueChain', 'Sustainabl...         0    ZAL   \n",
       "7565   ['Diversity', 'Aquaculture', 'Compliance', 'Be...         0    QIA   \n",
       "8786            ['Cybersecurity', 'Oversight', 'Social']         0    SIE   \n",
       "6915                      ['HumanCapital', 'Recruiting']         0    MRK   \n",
       "5620   ['ClimateTech', 'CarbonCaptureAndStorage', 'Ca...         0    HEI   \n",
       "8872                       ['ValueChain', 'Aquaculture']         0    SIE   \n",
       "\n",
       "                                                   title   \n",
       "9113   Siemens Healthineers Demonstrates Artificial I...  \\\n",
       "5826      Power MOSFET packages for high current designs   \n",
       "9459   Industry Mixed Over EU's New Green Hydrogen Rules   \n",
       "915    Airbus Updates 2022 Delivery Estimate To 700 A...   \n",
       "11007  Sustainable Apparel Coalition, Higg Launch New...   \n",
       "7565   Pupal cannibalism by worker honey bees contrib...   \n",
       "8786        Why We Have to Pay Attention to AI Right Now   \n",
       "6915   With $ 81M, Entact Bio takes a new approach to...   \n",
       "5620   Norway begins `` absolutely necessary '' proje...   \n",
       "8872            A new name in automated aquafeed systems   \n",
       "\n",
       "                                         cleaned_content   \n",
       "9113   healthcare information management system socie...  \\\n",
       "5826   ruggedness extended lifetime ask would right p...   \n",
       "9459   recently released proposed rule set renewable ...   \n",
       "915    say aim deliver around aircraft slightly fewer...   \n",
       "11007  amsterdam may today sustainable apparel coalit...   \n",
       "7565   com using browser version limited support cs o...   \n",
       "8786   run consulting service freiherr group snapshot...   \n",
       "6915   otherwise caught pathology disease contrast tr...   \n",
       "5620   EUR1 project could bury vast amount captured i...   \n",
       "8872   inspired food manufacturing engineering indust...   \n",
       "\n",
       "                                             word_tokens   \n",
       "9113   ['healthcare', 'information', 'management', 's...  \\\n",
       "5826   ['ruggedness', 'extended', 'lifetime', 'ask', ...   \n",
       "9459   ['recently', 'released', 'proposed', 'rule', '...   \n",
       "915    ['say', 'aim', 'deliver', 'around', 'aircraft'...   \n",
       "11007  ['amsterdam', 'may', 'today', 'sustainable', '...   \n",
       "7565   ['com', 'using', 'browser', 'version', 'limite...   \n",
       "8786   ['run', 'consulting', 'service', 'freiherr', '...   \n",
       "6915   ['otherwise', 'caught', 'pathology', 'disease'...   \n",
       "5620   ['EUR1', 'project', 'could', 'bury', 'vast', '...   \n",
       "8872   ['inspired', 'food', 'manufacturing', 'enginee...   \n",
       "\n",
       "                                         sentence_tokens   \n",
       "9113   ['healthcare information management systems so...  \\\n",
       "5826   ['ruggedness extended lifetime', 'ask would ri...   \n",
       "9459   ['recently released proposed rule set renewabl...   \n",
       "915    ['', 'says aims deliver around aircraft slight...   \n",
       "11007  ['amsterdam may today sustainable apparel coal...   \n",
       "7565   ['', 'using browser version limited support cs...   \n",
       "8786   ['', 'runs consulting service freiherr group',...   \n",
       "6915   ['otherwise caught pathology disease', 'contra...   \n",
       "5620   ['project could bury vast amounts captured ini...   \n",
       "8872   ['inspired food manufacturing engineering indu...   \n",
       "\n",
       "                                  pos_tagged_word_tokens   \n",
       "9113   [('healthcare', 'NN'), ('information', 'NN'), ...  \\\n",
       "5826   [('ruggedness', 'NN'), ('extended', 'VBD'), ('...   \n",
       "9459   [('recently', 'RB'), ('released', 'VBN'), ('pr...   \n",
       "915    [('say', 'VB'), ('aim', 'NN'), ('deliver', 'NN...   \n",
       "11007  [('amsterdam', 'NN'), ('may', 'MD'), ('today',...   \n",
       "7565   [('com', 'NN'), ('using', 'VBG'), ('browser', ...   \n",
       "8786   [('run', 'VB'), ('consulting', 'VBG'), ('servi...   \n",
       "6915   [('otherwise', 'RB'), ('caught', 'VBN'), ('pat...   \n",
       "5620   [('EUR1', 'NNP'), ('project', 'NN'), ('could',...   \n",
       "8872   [('inspired', 'VBN'), ('food', 'NN'), ('manufa...   \n",
       "\n",
       "                              pos_tagged_sentence_tokens  market_cap_in_usd_b   \n",
       "9113   [[('healthcare', 'NN'), ('information', 'NN'),...               110.13  \\\n",
       "5826   [[('ruggedness', 'NN'), ('extended', 'VBD'), (...                41.33   \n",
       "9459   [[('recently', 'RB'), ('released', 'VBN'), ('p...                13.70   \n",
       "915    [[], [('deliver', 'VB'), ('around', 'IN'), ('a...                96.87   \n",
       "11007  [[('amsterdam', 'NN'), ('may', 'MD'), ('today'...                10.41   \n",
       "7565   [[], [('using', 'VBG'), ('browser', 'NN'), ('v...                11.38   \n",
       "8786   [[], [('consulting', 'VBG'), ('service', 'NN')...               110.13   \n",
       "6915   [[('otherwise', 'RB'), ('caught', 'VBN'), ('pa...                87.64   \n",
       "5620   [[('project', 'JJ'), ('could', 'MD'), ('bury',...                11.91   \n",
       "8872   [[('inspired', 'VBN'), ('food', 'NN'), ('manuf...               110.13   \n",
       "\n",
       "                       sector                                industry  \n",
       "9113              Industrials          Specialty Industrial Machinery  \n",
       "5826               Technology                          Semiconductors  \n",
       "9459                Utilities   Utilities—Independent Power Producers  \n",
       "915               Industrials                     Aerospace & Defense  \n",
       "11007  Consumer Discretionary                         Internet Retail  \n",
       "7565               Healthcare                  Diagnostics & Research  \n",
       "8786              Industrials          Specialty Industrial Machinery  \n",
       "6915               Healthcare  Drug Manufacturers—Specialty & Generic  \n",
       "5620          Basic Materials                      Building Materials  \n",
       "8872              Industrials          Specialty Industrial Machinery  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check final dataframe\n",
    "enriched_cleaned_data.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Sentiment Value with Polarity Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last preprocessing step, the sentiment is calculated with the (quite basic) SentimentIntensityAnalyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    \"\"\"\n",
    "    Computes the sentiment score for a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to compute the sentiment score for.\n",
    "\n",
    "    Returns:\n",
    "        float: The sentiment score of the text as a float between -1 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "\n",
    "    # Remove brackets and quotes, then split the text by commas\n",
    "    text = text.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "    sentences = text.split(\", \")\n",
    "\n",
    "    # Compute sentiment scores for each sentence and store them in a list\n",
    "    sentiment_scores = [sia.polarity_scores(sentence)['compound'] for sentence in sentences]\n",
    "\n",
    "    # Compute the average sentiment score\n",
    "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
    "\n",
    "    return avg_sentiment_score\n",
    "\n",
    "# Sentiment score calculation provided most \"balanced\" results with averaged sentence tokens. Therefore the sentiment is calculated on these texts.\n",
    "enriched_cleaned_data['sentiment_value'] = enriched_cleaned_data['sentence_tokens'].apply(get_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>sentiment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>AkzoNobelNV</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>chemistryworld</td>\n",
       "      <td>['AnimalTesting']</td>\n",
       "      <td>0</td>\n",
       "      <td>BAS</td>\n",
       "      <td>Non-animal test for skin sensitisation gets OE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>48.10</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-11-09</td>\n",
       "      <td>law360</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>Former Deutsche Bank Exec Says Discrimination ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-08-20</td>\n",
       "      <td>globalcapital</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>Deutsche Bank debuts green label in Formosa fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>RWE</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['RenewableEnergy']</td>\n",
       "      <td>0</td>\n",
       "      <td>RWE</td>\n",
       "      <td>Germany's RWE Expands US Green Power Portfolio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>29.97</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Utilities—Diversified</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            company  datatype        date          domain   \n",
       "1574    AkzoNobelNV   general  2021-09-09  chemistryworld  \\\n",
       "3881  Deutsche Bank   general  2022-11-09          law360   \n",
       "3995  Deutsche Bank  business  2021-08-20   globalcapital   \n",
       "7858            RWE   general  2022-10-03     energyintel   \n",
       "\n",
       "               esg_topics  internal symbol   \n",
       "1574    ['AnimalTesting']         0    BAS  \\\n",
       "3881                   []         0    DBK   \n",
       "3995                   []         0    DBK   \n",
       "7858  ['RenewableEnergy']         0    RWE   \n",
       "\n",
       "                                                  title cleaned_content   \n",
       "1574  Non-animal test for skin sensitisation gets OE...             NaN  \\\n",
       "3881  Former Deutsche Bank Exec Says Discrimination ...             NaN   \n",
       "3995  Deutsche Bank debuts green label in Formosa fo...             NaN   \n",
       "7858     Germany's RWE Expands US Green Power Portfolio             NaN   \n",
       "\n",
       "     word_tokens sentence_tokens pos_tagged_word_tokens   \n",
       "1574          []              []                     []  \\\n",
       "3881          []            ['']                     []   \n",
       "3995          []            ['']                     []   \n",
       "7858          []            ['']                     []   \n",
       "\n",
       "     pos_tagged_sentence_tokens  market_cap_in_usd_b           sector   \n",
       "1574                         []                48.10  Basic Materials  \\\n",
       "3881                       [[]]                24.97       Financials   \n",
       "3995                       [[]]                24.97       Financials   \n",
       "7858                       [[]]                29.97        Utilities   \n",
       "\n",
       "                   industry  sentiment_value  \n",
       "1574              Chemicals              0.0  \n",
       "3881                  Banks              0.0  \n",
       "3995                  Banks              0.0  \n",
       "7858  Utilities—Diversified              0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NaN rows\n",
    "enriched_cleaned_data[enriched_cleaned_data['cleaned_content'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN rows\n",
    "enriched_cleaned_data = enriched_cleaned_data.dropna(subset=['cleaned_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to enriched_cleaned_data.csv\n",
      "Loaded DataFrame from enriched_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file for further analysis\n",
    "enriched_cleaned_data = csv_checkpoint(enriched_cleaned_data, 'enriched_cleaned_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>sentiment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>Airbus</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>aithority</td>\n",
       "      <td>['Compliance']</td>\n",
       "      <td>0</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Mirakl Listed as a Representative Vendor in Ga...</td>\n",
       "      <td>mirakl industry first advanced enterprise mark...</td>\n",
       "      <td>['mirakl', 'industry', 'first', 'advanced', 'e...</td>\n",
       "      <td>['mirakl industry first advanced enterprise ma...</td>\n",
       "      <td>[('mirakl', 'NN'), ('industry', 'NN'), ('first...</td>\n",
       "      <td>[[('mirakl', 'NN'), ('industry', 'NN'), ('firs...</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>0.390390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Airbus</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>businesswire</td>\n",
       "      <td>['CarbonDioxide']</td>\n",
       "      <td>0</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Aviation Capital Group Announces Delivery of O...</td>\n",
       "      <td>photo business wire aviation capital group ann...</td>\n",
       "      <td>['photo', 'business', 'wire', 'aviation', 'cap...</td>\n",
       "      <td>['photo business wire aviation capital group a...</td>\n",
       "      <td>[('photo', 'NN'), ('business', 'NN'), ('wire',...</td>\n",
       "      <td>[[('photo', 'NN'), ('business', 'NN'), ('wire'...</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>0.359367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>RWE</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>energyvoice</td>\n",
       "      <td>['RenewableEnergy', 'GreenHydrogen', 'Recruiti...</td>\n",
       "      <td>0</td>\n",
       "      <td>RWE</td>\n",
       "      <td>BP to boost hydrogen team as energy giant seek...</td>\n",
       "      <td>bp lon bp beef hydrogen team according news re...</td>\n",
       "      <td>['bp', 'lon', 'bp', 'beef', 'hydrogen', 'team'...</td>\n",
       "      <td>['bp lon bp beef hydrogen team according news ...</td>\n",
       "      <td>[('bp', 'NN'), ('lon', 'NN'), ('bp', 'NN'), ('...</td>\n",
       "      <td>[[('bp', 'NN'), ('lon', 'NN'), ('bp', 'NN'), (...</td>\n",
       "      <td>29.97</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Utilities—Diversified</td>\n",
       "      <td>0.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612</th>\n",
       "      <td>Heidelberg Cement</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>worldcement</td>\n",
       "      <td>['ClimateChange', 'ValueChain', 'Governance', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>HEI</td>\n",
       "      <td>CDP celebrates HeidelbergCement as ‘ Supplier ...</td>\n",
       "      <td>instruction enable javascript web browser foll...</td>\n",
       "      <td>['instruction', 'enable', 'javascript', 'web',...</td>\n",
       "      <td>['', 'instructions enable javascript web brows...</td>\n",
       "      <td>[('instruction', 'NN'), ('enable', 'JJ'), ('ja...</td>\n",
       "      <td>[[], [('enable', 'JJ'), ('javascript', 'NN'), ...</td>\n",
       "      <td>11.91</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Building Materials</td>\n",
       "      <td>0.399864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>wsj</td>\n",
       "      <td>['EMobility', 'Governance', 'Vaccine']</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>How Bad Can Inflation Be? Turkey Offers a Warn...</td>\n",
       "      <td>turkey rate currently offer warning soaring in...</td>\n",
       "      <td>['turkey', 'rate', 'currently', 'offer', 'warn...</td>\n",
       "      <td>['turkey rate currently offers warning', 'soar...</td>\n",
       "      <td>[('turkey', 'JJ'), ('rate', 'NN'), ('currently...</td>\n",
       "      <td>[[('turkey', 'JJ'), ('rate', 'NN'), ('currentl...</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.129217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7053</th>\n",
       "      <td>Porsche</td>\n",
       "      <td>esg</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>esgtoday</td>\n",
       "      <td>['EMobility', 'LowCarbon']</td>\n",
       "      <td>0</td>\n",
       "      <td>PAH3</td>\n",
       "      <td>Porsche Leads $ 400 Million Capital Raise for ...</td>\n",
       "      <td>series funding round led luxury sport car manu...</td>\n",
       "      <td>['series', 'funding', 'round', 'led', 'luxury'...</td>\n",
       "      <td>['series funding round led luxury sports car m...</td>\n",
       "      <td>[('series', 'NN'), ('funding', 'NN'), ('round'...</td>\n",
       "      <td>[[('series', 'NN'), ('funding', 'NN'), ('round...</td>\n",
       "      <td>16.65</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>0.238658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-11-09</td>\n",
       "      <td>cnbc</td>\n",
       "      <td>['GreatResignation', 'Recruiting', 'Environment']</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>Switching jobs can lead to higher pay. Here's ...</td>\n",
       "      <td>forget negotiating current employer sometimes ...</td>\n",
       "      <td>['forget', 'negotiating', 'current', 'employer...</td>\n",
       "      <td>['', 'forget negotiating current employer', 's...</td>\n",
       "      <td>[('forget', 'VB'), ('negotiating', 'VBG'), ('c...</td>\n",
       "      <td>[[], [('forget', 'VB'), ('negotiating', 'VBG')...</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.328927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6380</th>\n",
       "      <td>MTU</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>irishexaminer</td>\n",
       "      <td>['HumanCapital', 'GenderDiversity']</td>\n",
       "      <td>0</td>\n",
       "      <td>MTX</td>\n",
       "      <td>A new dawn as CIT and ITT become Munster Techn...</td>\n",
       "      <td>academic council member attendee cit itt pictu...</td>\n",
       "      <td>['academic', 'council', 'member', 'attendee', ...</td>\n",
       "      <td>['academic council members attendees cit itt p...</td>\n",
       "      <td>[('academic', 'JJ'), ('council', 'NN'), ('memb...</td>\n",
       "      <td>[[('academic', 'JJ'), ('council', 'NN'), ('cit...</td>\n",
       "      <td>12.24</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>0.376223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7070</th>\n",
       "      <td>Porsche</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>businesswire</td>\n",
       "      <td>['CustomerService']</td>\n",
       "      <td>0</td>\n",
       "      <td>PAH3</td>\n",
       "      <td>XPEL Introduces ULTIMATE PLUS™ BLACK</td>\n",
       "      <td>gloss black film selfhealing property xpel ult...</td>\n",
       "      <td>['gloss', 'black', 'film', 'selfhealing', 'pro...</td>\n",
       "      <td>['gloss black film selfhealing properties xpel...</td>\n",
       "      <td>[('gloss', 'NN'), ('black', 'JJ'), ('film', 'N...</td>\n",
       "      <td>[[('gloss', 'NN'), ('black', 'JJ'), ('film', '...</td>\n",
       "      <td>16.65</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>0.227162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6158</th>\n",
       "      <td>Infineon Technologies</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-09-19</td>\n",
       "      <td>globenewswire</td>\n",
       "      <td>['Fraud']</td>\n",
       "      <td>0</td>\n",
       "      <td>IFX</td>\n",
       "      <td>EMV Smart Cards Market Expected to Reach $ 14 ...</td>\n",
       "      <td>pandemic positively impacted emv smart card pa...</td>\n",
       "      <td>['pandemic', 'positively', 'impacted', 'emv', ...</td>\n",
       "      <td>['pandemic positively impacted emv smart card ...</td>\n",
       "      <td>[('pandemic', 'JJ'), ('positively', 'RB'), ('i...</td>\n",
       "      <td>[[('pandemic', 'JJ'), ('positively', 'RB'), ('...</td>\n",
       "      <td>41.33</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>0.403326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company  datatype        date         domain   \n",
       "1027                 Airbus      tech  2021-03-05      aithority  \\\n",
       "891                  Airbus   general  2022-07-28   businesswire   \n",
       "8063                    RWE   general  2022-06-15    energyvoice   \n",
       "5612      Heidelberg Cement  business  2022-02-11    worldcement   \n",
       "3797          Deutsche Bank       NaN  2021-12-15            wsj   \n",
       "7053                Porsche       esg  2022-05-06       esgtoday   \n",
       "4185          Deutsche Bank  business  2021-11-09           cnbc   \n",
       "6380                    MTU   general  2021-01-01  irishexaminer   \n",
       "7070                Porsche   general  2021-09-15   businesswire   \n",
       "6158  Infineon Technologies   general  2022-09-19  globenewswire   \n",
       "\n",
       "                                             esg_topics  internal symbol   \n",
       "1027                                     ['Compliance']         0    AIR  \\\n",
       "891                                   ['CarbonDioxide']         0    AIR   \n",
       "8063  ['RenewableEnergy', 'GreenHydrogen', 'Recruiti...         0    RWE   \n",
       "5612  ['ClimateChange', 'ValueChain', 'Governance', ...         0    HEI   \n",
       "3797             ['EMobility', 'Governance', 'Vaccine']         0    DBK   \n",
       "7053                         ['EMobility', 'LowCarbon']         0   PAH3   \n",
       "4185  ['GreatResignation', 'Recruiting', 'Environment']         0    DBK   \n",
       "6380                ['HumanCapital', 'GenderDiversity']         0    MTX   \n",
       "7070                                ['CustomerService']         0   PAH3   \n",
       "6158                                          ['Fraud']         0    IFX   \n",
       "\n",
       "                                                  title   \n",
       "1027  Mirakl Listed as a Representative Vendor in Ga...  \\\n",
       "891   Aviation Capital Group Announces Delivery of O...   \n",
       "8063  BP to boost hydrogen team as energy giant seek...   \n",
       "5612  CDP celebrates HeidelbergCement as ‘ Supplier ...   \n",
       "3797  How Bad Can Inflation Be? Turkey Offers a Warn...   \n",
       "7053  Porsche Leads $ 400 Million Capital Raise for ...   \n",
       "4185  Switching jobs can lead to higher pay. Here's ...   \n",
       "6380  A new dawn as CIT and ITT become Munster Techn...   \n",
       "7070               XPEL Introduces ULTIMATE PLUS™ BLACK   \n",
       "6158  EMV Smart Cards Market Expected to Reach $ 14 ...   \n",
       "\n",
       "                                        cleaned_content   \n",
       "1027  mirakl industry first advanced enterprise mark...  \\\n",
       "891   photo business wire aviation capital group ann...   \n",
       "8063  bp lon bp beef hydrogen team according news re...   \n",
       "5612  instruction enable javascript web browser foll...   \n",
       "3797  turkey rate currently offer warning soaring in...   \n",
       "7053  series funding round led luxury sport car manu...   \n",
       "4185  forget negotiating current employer sometimes ...   \n",
       "6380  academic council member attendee cit itt pictu...   \n",
       "7070  gloss black film selfhealing property xpel ult...   \n",
       "6158  pandemic positively impacted emv smart card pa...   \n",
       "\n",
       "                                            word_tokens   \n",
       "1027  ['mirakl', 'industry', 'first', 'advanced', 'e...  \\\n",
       "891   ['photo', 'business', 'wire', 'aviation', 'cap...   \n",
       "8063  ['bp', 'lon', 'bp', 'beef', 'hydrogen', 'team'...   \n",
       "5612  ['instruction', 'enable', 'javascript', 'web',...   \n",
       "3797  ['turkey', 'rate', 'currently', 'offer', 'warn...   \n",
       "7053  ['series', 'funding', 'round', 'led', 'luxury'...   \n",
       "4185  ['forget', 'negotiating', 'current', 'employer...   \n",
       "6380  ['academic', 'council', 'member', 'attendee', ...   \n",
       "7070  ['gloss', 'black', 'film', 'selfhealing', 'pro...   \n",
       "6158  ['pandemic', 'positively', 'impacted', 'emv', ...   \n",
       "\n",
       "                                        sentence_tokens   \n",
       "1027  ['mirakl industry first advanced enterprise ma...  \\\n",
       "891   ['photo business wire aviation capital group a...   \n",
       "8063  ['bp lon bp beef hydrogen team according news ...   \n",
       "5612  ['', 'instructions enable javascript web brows...   \n",
       "3797  ['turkey rate currently offers warning', 'soar...   \n",
       "7053  ['series funding round led luxury sports car m...   \n",
       "4185  ['', 'forget negotiating current employer', 's...   \n",
       "6380  ['academic council members attendees cit itt p...   \n",
       "7070  ['gloss black film selfhealing properties xpel...   \n",
       "6158  ['pandemic positively impacted emv smart card ...   \n",
       "\n",
       "                                 pos_tagged_word_tokens   \n",
       "1027  [('mirakl', 'NN'), ('industry', 'NN'), ('first...  \\\n",
       "891   [('photo', 'NN'), ('business', 'NN'), ('wire',...   \n",
       "8063  [('bp', 'NN'), ('lon', 'NN'), ('bp', 'NN'), ('...   \n",
       "5612  [('instruction', 'NN'), ('enable', 'JJ'), ('ja...   \n",
       "3797  [('turkey', 'JJ'), ('rate', 'NN'), ('currently...   \n",
       "7053  [('series', 'NN'), ('funding', 'NN'), ('round'...   \n",
       "4185  [('forget', 'VB'), ('negotiating', 'VBG'), ('c...   \n",
       "6380  [('academic', 'JJ'), ('council', 'NN'), ('memb...   \n",
       "7070  [('gloss', 'NN'), ('black', 'JJ'), ('film', 'N...   \n",
       "6158  [('pandemic', 'JJ'), ('positively', 'RB'), ('i...   \n",
       "\n",
       "                             pos_tagged_sentence_tokens  market_cap_in_usd_b   \n",
       "1027  [[('mirakl', 'NN'), ('industry', 'NN'), ('firs...                96.87  \\\n",
       "891   [[('photo', 'NN'), ('business', 'NN'), ('wire'...                96.87   \n",
       "8063  [[('bp', 'NN'), ('lon', 'NN'), ('bp', 'NN'), (...                29.97   \n",
       "5612  [[], [('enable', 'JJ'), ('javascript', 'NN'), ...                11.91   \n",
       "3797  [[('turkey', 'JJ'), ('rate', 'NN'), ('currentl...                24.97   \n",
       "7053  [[('series', 'NN'), ('funding', 'NN'), ('round...                16.65   \n",
       "4185  [[], [('forget', 'VB'), ('negotiating', 'VBG')...                24.97   \n",
       "6380  [[('academic', 'JJ'), ('council', 'NN'), ('cit...                12.24   \n",
       "7070  [[('gloss', 'NN'), ('black', 'JJ'), ('film', '...                16.65   \n",
       "6158  [[('pandemic', 'JJ'), ('positively', 'RB'), ('...                41.33   \n",
       "\n",
       "                      sector               industry  sentiment_value  \n",
       "1027             Industrials    Aerospace & Defense         0.390390  \n",
       "891              Industrials    Aerospace & Defense         0.359367  \n",
       "8063               Utilities  Utilities—Diversified         0.086800  \n",
       "5612         Basic Materials     Building Materials         0.399864  \n",
       "3797              Financials                  Banks         0.129217  \n",
       "7053  Consumer Discretionary     Auto Manufacturers         0.238658  \n",
       "4185              Financials                  Banks         0.328927  \n",
       "6380             Industrials    Aerospace & Defense         0.376223  \n",
       "7070  Consumer Discretionary     Auto Manufacturers         0.227162  \n",
       "6158              Technology         Semiconductors         0.403326  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data after full preprocessing\n",
    "enriched_cleaned_data.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
