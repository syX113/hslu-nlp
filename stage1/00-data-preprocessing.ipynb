{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import ast\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import contractions\n",
    "from langdetect import detect \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# Download a spacy model, can also be adjusted (medium = en_core_web_sm, large = en_core_web_lg)\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_csv('../data/esg_documents_for_dax_companies.csv', delimiter = '|', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beiersdorf AG</td>\n",
       "      <td>Sustainability Highlight Report CARE BEYOND SK...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'GHGEmission', 'ProductLiabilit...</td>\n",
       "      <td>1</td>\n",
       "      <td>BEI</td>\n",
       "      <td>BeiersdorfAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>Corporate Responsibility Report 2021 2 Content...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DTE</td>\n",
       "      <td>DeutscheTelekomAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vonovia SE</td>\n",
       "      <td>VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'DataSecurity', 'Vaccine', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>VNA</td>\n",
       "      <td>VonoviaSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merck KGaA</td>\n",
       "      <td>Sustainability Report 2021 TABLE OF CONTENTS S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'DataMisuse', 'DrugResistance...</td>\n",
       "      <td>1</td>\n",
       "      <td>MRK</td>\n",
       "      <td>MerckKGaA Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTU</td>\n",
       "      <td>Our ideas and concepts FOR A SUSTAINABLE FUTUR...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'AirQuality'...</td>\n",
       "      <td>1</td>\n",
       "      <td>MTX</td>\n",
       "      <td>MTUAeroEngines Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>#StandWithUkraine Sustainability Report 2021 C...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>E.ONSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RWE AG</td>\n",
       "      <td>Focus on tomorrow. Sustainability Report 2021 ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'Iso50001', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>RWE</td>\n",
       "      <td>RWEAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Annual Report 2021 HeidelbergCement at a glanc...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Vaccine', 'DataSecurity',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Annual Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Company Strategy &amp; Business &amp; Product &amp; Produc...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'Corruption', 'Whistleblowing',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>Sustainability 1 Siemens 2 Our 3 Governance – ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'EmployeeTurnover...</td>\n",
       "      <td>1</td>\n",
       "      <td>SIE</td>\n",
       "      <td>SiemensAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company                                            content   \n",
       "0         Beiersdorf AG  Sustainability Highlight Report CARE BEYOND SK...  \\\n",
       "1   Deutsche Telekom AG  Corporate Responsibility Report 2021 2 Content...   \n",
       "2            Vonovia SE  VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...   \n",
       "3            Merck KGaA  Sustainability Report 2021 TABLE OF CONTENTS S...   \n",
       "4                   MTU  Our ideas and concepts FOR A SUSTAINABLE FUTUR...   \n",
       "5                E ONSE  #StandWithUkraine Sustainability Report 2021 C...   \n",
       "6                RWE AG  Focus on tomorrow. Sustainability Report 2021 ...   \n",
       "7  Heidelberg Cement AG  Annual Report 2021 HeidelbergCement at a glanc...   \n",
       "8  Heidelberg Cement AG  Company Strategy & Business & Product & Produc...   \n",
       "9            Siemens AG  Sustainability 1 Siemens 2 Our 3 Governance – ...   \n",
       "\n",
       "                datatype        date domain   \n",
       "0  sustainability_report  2021-03-31    NaN  \\\n",
       "1  sustainability_report  2021-03-31    NaN   \n",
       "2  sustainability_report  2021-03-31    NaN   \n",
       "3  sustainability_report  2021-03-31    NaN   \n",
       "4  sustainability_report  2020-03-31    NaN   \n",
       "5  sustainability_report  2021-03-31    NaN   \n",
       "6  sustainability_report  2021-03-31    NaN   \n",
       "7          annual_report  2021-03-31    NaN   \n",
       "8  sustainability_report  2020-03-31    NaN   \n",
       "9  sustainability_report  2020-03-31    NaN   \n",
       "\n",
       "                                          esg_topics  internal symbol   \n",
       "0  ['CleanWater', 'GHGEmission', 'ProductLiabilit...         1    BEI  \\\n",
       "1  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1    DTE   \n",
       "2  ['Whistleblowing', 'DataSecurity', 'Vaccine', ...         1    VNA   \n",
       "3  ['DataSecurity', 'DataMisuse', 'DrugResistance...         1    MRK   \n",
       "4  ['WorkLifeBalance', 'Corruption', 'AirQuality'...         1    MTX   \n",
       "5  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1   EOAN   \n",
       "6  ['WorkLifeBalance', 'Corruption', 'Iso50001', ...         1    RWE   \n",
       "7  ['WorkLifeBalance', 'Vaccine', 'DataSecurity',...         1    HEI   \n",
       "8  ['CleanWater', 'Corruption', 'Whistleblowing',...         1    HEI   \n",
       "9  ['DataSecurity', 'Iso50001', 'EmployeeTurnover...         1    SIE   \n",
       "\n",
       "                                           title  url  \n",
       "0        BeiersdorfAG Sustainability Report 2021  NaN  \n",
       "1   DeutscheTelekomAG Sustainability Report 2021  NaN  \n",
       "2           VonoviaSE Sustainability Report 2021  NaN  \n",
       "3           MerckKGaA Sustainability Report 2021  NaN  \n",
       "4      MTUAeroEngines Sustainability Report 2020  NaN  \n",
       "5              E.ONSE Sustainability Report 2021  NaN  \n",
       "6               RWEAG Sustainability Report 2021  NaN  \n",
       "7          HeidelbergCementAG Annual Report 2021  NaN  \n",
       "8  HeidelbergCementAG Sustainability Report 2020  NaN  \n",
       "9           SiemensAG Sustainability Report 2020  NaN  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded data and reset index\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column descriptions**\n",
    "- symbol: stock symbol of the company\n",
    "- company: company name\n",
    "- date: publication date of document\n",
    "- title: document title\n",
    "- content: document content\n",
    "- datatype: document type\n",
    "- internal: is this a report by company (1) or a third-party document (0)\n",
    "- domain (optional): Web domain where the document was published\n",
    "- url (optional): URL where the document can be accessed\n",
    "- esg_topics (optional): ESG topics extracted from the data using our internal NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11188, 10)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape (row and column amount)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company       object\n",
       "content       object\n",
       "datatype      object\n",
       "date          object\n",
       "domain        object\n",
       "esg_topics    object\n",
       "internal       int64\n",
       "symbol        object\n",
       "title         object\n",
       "url           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "raw_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small checkpoint function to save intermediary processing steps and enhance development\n",
    "def csv_checkpoint(df, filename='checkpoint'):\n",
    "\n",
    "    if not os.path.exists('../data/checkpoints/'):\n",
    "        os.makedirs('../data/checkpoints/')\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(f'../data/checkpoints/{filename}.csv', index=False, sep = '|')\n",
    "    print(f'Saved DataFrame to {filename}.csv')\n",
    "    \n",
    "    # Load CSV back into DataFrame\n",
    "    df = pd.read_csv(f'../data/checkpoints/{filename}.csv', delimiter = '|')\n",
    "    print(f'Loaded DataFrame from {filename}.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As initial data cleaning steps, the following is conducted:\n",
    "- Rows with missing \"content\" were dropped to prevent any missing data-related issues. Missing data can create gaps in the data and lead to errors or distortions in the analysis.\n",
    "- The \"URL\" column was removed as the relevant information was available in the \"domain\" column. Removing redundant columns simplifies the data set and makes it easier to work with\n",
    "- Duplicate entries were identified and removed, resulting in a cleaner and more concise dataset. Duplicates can distort the data and lead to biased analysis. \n",
    "- Language checking was conducted and all rows with non-English content were dropped to ensure consistent language. Language inconsistencies can create bias in the data and lead to inaccurate conclusions. Therefore, it is important to ensure that the data is consistent in language to prevent linguistic biases.\n",
    "- \"Date\" is formatted as a date and wrong dates, e.g. \"bayer-03-31\" are replaced with a default date (2023-03-31).\n",
    "- Remove company name parts like \"AG\" for clarity\n",
    "- The \"sample\" method was used to check the data for representativeness and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_cleaned_data = raw_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"url\" column, since the most relevant information from an analysis perspective is already in the \"domain\" column (e.g. the source of the report)\n",
    "general_cleaned_data = general_cleaned_data.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 6\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and delete them\n",
    "duplicates = general_cleaned_data[general_cleaned_data.duplicated()]\n",
    "print(f'Duplicated rows: {len(duplicates)}')\n",
    "general_cleaned_data = general_cleaned_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted amount of rows with language other ehan English: 106\n"
     ]
    }
   ],
   "source": [
    "# Check for other languange than English\n",
    "general_cleaned_data['language'] = general_cleaned_data['content'].apply(lambda x: detect(x))\n",
    "not_english = len(general_cleaned_data) - len(general_cleaned_data.loc[general_cleaned_data['language'] == 'en'])\n",
    "\n",
    "# Drop rows with other languange, since other languanges influences to quality of the later analysis\n",
    "general_cleaned_data = general_cleaned_data.loc[general_cleaned_data['language'] == 'en']\n",
    "\n",
    "print(f'Deleted amount of rows with language other ehan English: {not_english}')\n",
    "general_cleaned_data.drop(['language'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly formatted dates:\n",
      "Row 13: p.DE-03-31\n",
      "Row 18: p.DE-03-31\n",
      "Row 20: bayer-03-31\n",
      "Row 22: p.DE-03-31\n",
      "Row 25: p.DE-03-31\n",
      "Row 26: p.DE-03-31\n",
      "Row 31: p.DE-03-31\n",
      "Row 32: p.DE-03-31\n",
      "Row 33: p.DE-03-31\n",
      "Row 37: p.DE-03-31\n",
      "Row 41: p.DE-03-31\n",
      "Row 50: p.DE-03-31\n",
      "Row 78: p.DE-03-31\n",
      "Row 80: p.DE-03-31\n",
      "Row 86: p.DE-03-31\n",
      "Row 87: p.DE-03-31\n",
      "Row 88: p.DE-03-31\n"
     ]
    }
   ],
   "source": [
    "# Correct the dates to ISO standard\n",
    "def find_incorrect_dates(data):\n",
    "    incorrect_dates = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            pd.to_datetime(row['date'], format='%Y-%m-%d', errors='raise')\n",
    "        except ValueError:\n",
    "            incorrect_dates.append((index, row['date']))\n",
    "\n",
    "    return incorrect_dates\n",
    "\n",
    "incorrect_date_rows = find_incorrect_dates(general_cleaned_data)\n",
    "print(\"Incorrectly formatted dates:\")\n",
    "for index, date in incorrect_date_rows:\n",
    "    print(f\"Row {index}: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the wrong formatted dates and set default date\n",
    "def correct_date_format(data):\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce').fillna('2022-03-31')\n",
    "    return data\n",
    "\n",
    "general_cleaned_data = correct_date_format(general_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace company name parts like \"AG\" to have a cleaner name\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' AG', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' SE', '')\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].str.replace(' KGaA', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9861</th>\n",
       "      <td>Siemens Healthineers</td>\n",
       "      <td>October 23, 2022 — Meeting with a medical phys...</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>SHL</td>\n",
       "      <td>Medical Physicist Consults with Patients can H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7166</th>\n",
       "      <td>Porsche</td>\n",
       "      <td>Vuzix Corporation, a leading supplier of Smart...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-05-20</td>\n",
       "      <td>aithority</td>\n",
       "      <td>['Mentoring', 'Environment']</td>\n",
       "      <td>0</td>\n",
       "      <td>PAH3</td>\n",
       "      <td>Vuzix, Softfoundry and Porsche China to Host i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Adidas</td>\n",
       "      <td>ADVERTISEMENT On Tuesday morning, adidas anno...</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-10-25</td>\n",
       "      <td>sneakernews</td>\n",
       "      <td>['Antisemitism', 'Diversity', 'HateSpeech']</td>\n",
       "      <td>0</td>\n",
       "      <td>ADS</td>\n",
       "      <td>adidas Breaks Up With Kanye West, Keeps Yeezy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>Daimler</td>\n",
       "      <td>A forensic scientist has given evidence at the...</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>irishexaminer</td>\n",
       "      <td>['Racism', 'HumanRights']</td>\n",
       "      <td>0</td>\n",
       "      <td>DAI</td>\n",
       "      <td>Scientist gives evidence about drugs found aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7845</th>\n",
       "      <td>RWE</td>\n",
       "      <td>Hi, what are you looking for? By Published Tho...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>digitaljournal</td>\n",
       "      <td>['ClimateChange', 'CarbonDioxide', 'Coal', 'Fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>RWE</td>\n",
       "      <td>'Hambi stays! ': Thousands join German forest ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   company                                            content   \n",
       "9861  Siemens Healthineers  October 23, 2022 — Meeting with a medical phys...  \\\n",
       "7166               Porsche  Vuzix Corporation, a leading supplier of Smart...   \n",
       "203                 Adidas   ADVERTISEMENT On Tuesday morning, adidas anno...   \n",
       "3635               Daimler  A forensic scientist has given evidence at the...   \n",
       "7845                   RWE  Hi, what are you looking for? By Published Tho...   \n",
       "\n",
       "     datatype       date          domain   \n",
       "9861  general 2022-10-23       itnonline  \\\n",
       "7166     tech 2021-05-20       aithority   \n",
       "203   general 2022-10-25     sneakernews   \n",
       "3635  general 2022-02-10   irishexaminer   \n",
       "7845     tech 2021-03-29  digitaljournal   \n",
       "\n",
       "                                             esg_topics  internal symbol   \n",
       "9861                                         ['Social']         0    SHL  \\\n",
       "7166                       ['Mentoring', 'Environment']         0   PAH3   \n",
       "203         ['Antisemitism', 'Diversity', 'HateSpeech']         0    ADS   \n",
       "3635                          ['Racism', 'HumanRights']         0    DAI   \n",
       "7845  ['ClimateChange', 'CarbonDioxide', 'Coal', 'Fo...         0    RWE   \n",
       "\n",
       "                                                  title  \n",
       "9861  Medical Physicist Consults with Patients can H...  \n",
       "7166  Vuzix, Softfoundry and Porsche China to Host i...  \n",
       "203   adidas Breaks Up With Kanye West, Keeps Yeezy ...  \n",
       "3635  Scientist gives evidence about drugs found aft...  \n",
       "7845  'Hambi stays! ': Thousands join German forest ...  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the date with some samples\n",
    "general_cleaned_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of \"Muenchener Rueckversicherungs Gesellschaft AGin Muenchen\" to something more readable\n",
    "general_cleaned_data['company'] = general_cleaned_data['company'].replace('Muenchener Rueckversicherungs Gesellschaftin Muenchen', 'Munich R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to general_cleaned_data.csv\n",
      "Loaded DataFrame from general_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "general_cleaned_data = csv_checkpoint(general_cleaned_data, 'general_cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning & Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"content\" column, containing the text of the reports, undergoes a series of cleaning, normalization, and preprocessing steps to ensure accurate and efficient analysis. These steps include:\n",
    "\n",
    "- **String conversion**: Converting the input to a string format ensures consistency and compatibility during subsequent processing tasks.\n",
    "- **Lowercase conversion**: Transforming all text to lowercase serves as a simple normalization step, reducing the complexity and variability of the input data.\n",
    "- **Unicode decoding**: Removing diacritics (e.g., accented characters) and normalizing the text encoding mitigates potential discrepancies arising from different encoding formats.\n",
    "- **URL and email address removal**: Eliminating URLs and email addresses reduces noise in the dataset, as these elements do not contribute valuable information for the analysis.\n",
    "- **Extra whitespace removal**: Eradicating extra whitespaces improves text analysis and tokenization by ensuring that only meaningful spaces are retained.\n",
    "- **Contact detail removal**: Excluding phone numbers, contact person strings, and social media references further minimizes noise in the dataset, honing the focus on relevant text.\n",
    "- **Table of contents removal**: Discarding the table of contents enhances the data quality by eliminating repetitive and non-essential information.\n",
    "- **Named entity removal**: Employing the spaCy model to remove human names and other named entities optimizes the text for analysis and modeling by concentrating on pertinent content.\n",
    "- **Abbreviation expansion**: Utilizing the contractions library and custom functions with regular expressions, common and uncommon abbreviations are expanded to improve text interpretation.\n",
    "- **Special character elimination**: Excluding all special characters, except punctuation, refines the input data. Retaining punctuation is necessary for accurate sentence tokenization and removed after sentence tokenization..\n",
    "- **Tokenization and lemmatization**: Tokenizing words and sentences, and subsequently lemmatizing words using the WordNetLemmatizer from nltk, streamlines the text and reduces morphological variations.\n",
    "- **Stopword removal**: Customizing the nltk stopwords list by adding or removing specific stopwords enables more precise and tailored text analysis.\n",
    "- **Part-of-speech (POS) tagging**: Assigning POS tags to words and sentences enhances the text representation by providing additional linguistic information, which may be beneficial for subsequent analysis and modeling tasks.\n",
    "- **Sentiment Analysis**: Basic sentiment value calculation to get first insights im terms of the sentiments in the reports within the EDA.\n",
    "\n",
    "Spellchecking was tested with TextBlob and PySpellChecker but deliverd not useful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = general_cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name entities removed: 94745\n"
     ]
    }
   ],
   "source": [
    "# Since the spacy model shows better results on the \"raw\" text, the named entity removal is conducted before all normalization and cleaning steps\n",
    "spacy_model = spacy.load('en_core_web_md')\n",
    "spacy_model.max_length = 1800000 # Increase max text length\n",
    "\n",
    "def remove_named_entities(text):\n",
    "    doc = spacy_model(text)\n",
    "    \n",
    "    named_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\"]:\n",
    "            named_entities.add(ent.text)\n",
    "    \n",
    "    named_entities_count = len(named_entities)\n",
    "    \n",
    "    for named_entity in named_entities:\n",
    "        text = text.replace(named_entity, '')\n",
    "    \n",
    "    return text, named_entities_count\n",
    "\n",
    "# Assuming cleaned_data is a pandas DataFrame with a 'content' column\n",
    "cleaned_data['cleaned_content'], name_entity_count = zip(*cleaned_data['content'].apply(remove_named_entities))\n",
    "print(\"Name entities removed:\", sum(name_entity_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs removed: 7492\n",
      "Mail addresses removed: 434\n",
      "Extra whitespaces removed: 149075\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    urls = re.findall(r'http\\S+|www\\S+|https\\S+', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE), len(urls)\n",
    "\n",
    "def remove_emails(text):\n",
    "    mail_addresses = re.findall(r'\\S+@\\S+\\s?', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'\\S+@\\S+\\s?', '', text, flags=re.MULTILINE), len(mail_addresses)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    extra_spaces = re.findall(r'\\s{2,}', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip(), len(extra_spaces)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].astype(str) # Convert all texts to string\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: x.lower()) # Convert all texts to lower-case\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: unidecode(x, errors=\"preserve\")) # Remove diacritics / accented characters and unicode normalization\n",
    "cleaned_data['cleaned_content'], url_count = zip(*cleaned_data['cleaned_content'].apply(remove_urls)) # Remove URLs from texts\n",
    "cleaned_data['cleaned_content'], email_count = zip(*cleaned_data['cleaned_content'].apply(remove_emails)) # Remove e-mail addresses from texts\n",
    "cleaned_data['cleaned_content'], extra_space_count = zip(*cleaned_data['cleaned_content'].apply(remove_extra_whitespace)) # Remove extra whitespaces from texts\n",
    "\n",
    "print(\"URLs removed:\", sum(url_count))\n",
    "print(\"Mail addresses removed:\", sum(email_count))\n",
    "print(\"Extra whitespaces removed:\", sum(extra_space_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact information removed: 44237\n",
      "TOCs removed: 9603\n"
     ]
    }
   ],
   "source": [
    "def remove_contact_details(text):\n",
    "    # Remove phone numbers\n",
    "    phone_regex = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
    "    phone_count = len(re.findall(phone_regex, text))\n",
    "    text = re.sub(phone_regex, '', text)\n",
    "\n",
    "    # Remove common contact-related phrases\n",
    "    contact_phrases_regex = r'\\b(?:Contact Person|Phone|Tel|Fax|Mobile|E?mail|Skype|Twitter|Facebook|LinkedIn|Website):\\b'\n",
    "    contact_phrases_count = len(re.findall(contact_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(contact_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    total_count = phone_count + contact_phrases_count\n",
    "    return text, total_count\n",
    "\n",
    "def remove_table_of_contents(text):\n",
    "    # Remove common table of contents phrases\n",
    "    toc_phrases_regex = r'\\b(?:Table of Contents|Contents)\\b'\n",
    "    toc_phrases_count = len(re.findall(toc_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(toc_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove content with numbering like \"1. Introduction\", \"1.1. Background\", \"A. Overview\", etc.\n",
    "    toc_entries_regex = r'(^|\\n)\\s*\\w+(\\.\\w+)*\\s+\\w+([\\w\\s]+)?'\n",
    "    toc_entries_count = len(re.findall(toc_entries_regex, text))\n",
    "    text = re.sub(toc_entries_regex, '', text)\n",
    "\n",
    "    total_count = toc_phrases_count + toc_entries_count\n",
    "    return text, total_count\n",
    "\n",
    "cleaned_data['cleaned_content'], contact_count = zip(*cleaned_data['cleaned_content'].apply(remove_contact_details))\n",
    "cleaned_data['cleaned_content'], toc_count = zip(*cleaned_data['cleaned_content'].apply(remove_table_of_contents))\n",
    "print(\"Contact information removed:\", sum(contact_count))\n",
    "print(\"TOCs removed:\", sum(toc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = []\n",
    "    for word in text.split():\n",
    "        expanded_text.append(contractions.fix(word))\n",
    "    expanded_text = ' '.join(expanded_text)\n",
    "    return contractions.fix(expanded_text)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded custom abbreviations: 0\n"
     ]
    }
   ],
   "source": [
    "# Expand custom abbreviations which are not captured by \"contractions\"\n",
    "# Basic idea from: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# Compile the regular expressions only once for efficiency\n",
    "specific_patterns = [\n",
    "    (re.compile(r\"won['’]t\"), \"will not\"),\n",
    "    (re.compile(r\"can['’]t\"), \"can not\"),\n",
    "]\n",
    "\n",
    "def decontracted(phrase):\n",
    "    count = 0\n",
    "\n",
    "    # Replace specific patterns\n",
    "    for pattern, replacement in specific_patterns:\n",
    "        matches = len(pattern.findall(phrase))\n",
    "        count += matches\n",
    "        phrase = pattern.sub(replacement, phrase)\n",
    "\n",
    "    return phrase, count\n",
    "\n",
    "# Apply the function to expand abbreviations\n",
    "cleaned_data['cleaned_content'], abbreviation_counts = zip(*cleaned_data['cleaned_content'].apply(decontracted))\n",
    "print(\"Expanded custom abbreviations:\", sum(abbreviation_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters excl. punctuation removed: 1388695\n"
     ]
    }
   ],
   "source": [
    "# Remove special characters excl. punctuation since this is needed by the sentence tokenization\n",
    "def remove_non_alphanumeric(text, remove_punctuation=False):\n",
    "    if remove_punctuation:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s.,!?\\'\"]'\n",
    "    \n",
    "    special_chars = re.findall(pattern, text)\n",
    "    return re.sub(pattern, '', text), len(special_chars)\n",
    "\n",
    "cleaned_data['cleaned_content'], special_char_count = zip(*cleaned_data['cleaned_content'].apply(remove_non_alphanumeric, remove_punctuation=False))\n",
    "print(\"Special characters excl. punctuation removed:\", sum(special_char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word token amount: 16966134\n"
     ]
    }
   ],
   "source": [
    "def tokenize_words(text):\n",
    "    # Remove numbers, digits, and punctuation\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['word_tokens'], word_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_words))\n",
    "print(\"Generated word token amount:\", sum(word_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence token amount: 687795\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(text):\n",
    "    # Tokenize sentences\n",
    "    tokens = sent_tokenize(text)\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['sentence_tokens'], sentence_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_sentences))\n",
    "print(\"Generated sentence token amount:\", sum(sentence_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stopwords in word tokens 6817319\n",
      "Removed stopwords in sentence tokens 9628193\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_word_tokens(tokens, custom_stopwords):\n",
    "    # Remove stopwords and one-character tokens from tokens\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens\n",
    "        if token.lower() not in custom_stopwords and len(token) > 1\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens, len(tokens) - len(filtered_tokens)\n",
    "\n",
    "def remove_stopwords_from_sentence_tokens(sentences_list, custom_stopwords):\n",
    "    filtered_sentences_list = []\n",
    "    total_removed_items_count = 0\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        # Tokenize the sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords, one-character tokens, digits, numbers, and special characters (excluding whitespace) from word tokens\n",
    "        filtered_word_tokens = [\n",
    "            re.sub(rf\"[{re.escape(string.punctuation)}]\", '', token) for token in word_tokens\n",
    "            if token.lower() not in custom_stopwords\n",
    "            and len(token) > 1\n",
    "            and not re.search(r'\\d', token)\n",
    "            and not re.search(r'\\W', token)\n",
    "        ]\n",
    "\n",
    "        # Reconstruct the sentence without the removed words and special characters\n",
    "        filtered_sentence = ' '.join(filtered_word_tokens)\n",
    "        removed_items_count = len(word_tokens) - len(filtered_word_tokens)\n",
    "        filtered_sentences_list.append(filtered_sentence)\n",
    "        total_removed_items_count += removed_items_count\n",
    "\n",
    "    return filtered_sentences_list, total_removed_items_count\n",
    "\n",
    "# Define custom stopwords to add or remove (the extra stopwords were identified by the TFIDF based wordcloud)\n",
    "custom_stopwords = {\n",
    "    'add': ['said','company','companies','year','billion','million','siemens','linde','rwe','volkswagen','symrise','porsche','sap','adidas','puma','airbus','bmw','hannover','mtu','heiderbergcement','qiagen','benz','continental','bayer','fresenius','wa', 'ha', 'eur'],\n",
    "    'remove': [''] # Currently not needed\n",
    "}\n",
    "\n",
    "# Combine stopwords to filter the content of the reports\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "all_stopwords |= set(custom_stopwords['add'])\n",
    "all_stopwords -= set(custom_stopwords['remove'])\n",
    "\n",
    "cleaned_data['word_tokens'], stopword_count_words = zip(*cleaned_data['word_tokens'].apply(remove_stopwords_from_word_tokens, custom_stopwords=all_stopwords))\n",
    "cleaned_data['sentence_tokens'], stopword_count_sentences = zip(*cleaned_data['sentence_tokens'].apply(remove_stopwords_from_sentence_tokens, custom_stopwords=all_stopwords))\n",
    "\n",
    "print(\"Removed stopwords in word tokens\", sum(stopword_count_words))\n",
    "print(\"Removed stopwords in sentence tokens\", sum(stopword_count_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the cleaned content based on the cleaned word tokens\n",
    "cleaned_data['cleaned_content'] = cleaned_data['word_tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_tokens(word_tokens, sentence_list):\n",
    "    # POS tagging for word tokens\n",
    "    pos_tagged_word_tokens = nltk.pos_tag(word_tokens)\n",
    "\n",
    "    # Create a dictionary to map word tokens to their POS tags, this reduces the effort to call nltk.pos_tag twice\n",
    "    pos_tags_dict = dict(pos_tagged_word_tokens)\n",
    "\n",
    "    # POS tagging for sentence tokens\n",
    "    pos_tagged_sentence_list = []\n",
    "    for sentence in sentence_list:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = [(token, pos_tags_dict[token]) for token in tokenized_sentence if token in pos_tags_dict]\n",
    "        pos_tagged_sentence_list.append(pos_tagged_sentence)\n",
    "\n",
    "    return pos_tagged_word_tokens, pos_tagged_sentence_list\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = cleaned_data.apply(lambda row: pos_tagging_tokens(row['word_tokens'], row['sentence_tokens']), axis=1)\n",
    "cleaned_data['pos_tagged_word_tokens'], cleaned_data['pos_tagged_sentence_tokens'] = zip(*pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to cleaned_data.csv\n",
      "Loaded DataFrame from cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "cleaned_data = csv_checkpoint(cleaned_data, 'cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several additional information could be helpful in the further analysis, which are not included in the dataset. Therefore a small scraper is used to enrich the the dataset with the sector, industry and market capitalization of the DAX companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>symbol</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linde plc</td>\n",
       "      <td>LIN</td>\n",
       "      <td>156.93</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Specialty Chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAP SE</td>\n",
       "      <td>SAP</td>\n",
       "      <td>121.03</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software—Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>SIE</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>DTE</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Airbus SE</td>\n",
       "      <td>AIR</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company_name symbol  market_cap_in_usd_b         country   \n",
       "0            Linde plc    LIN               156.93  United Kingdom  \\\n",
       "1               SAP SE    SAP               121.03         Germany   \n",
       "2           Siemens AG    SIE               110.13         Germany   \n",
       "3  Deutsche Telekom AG    DTE               101.78         Germany   \n",
       "4            Airbus SE    AIR                96.87     Netherlands   \n",
       "\n",
       "                   sector                        industry  \n",
       "0         Basic Materials             Specialty Chemicals  \n",
       "1              Technology            Software—Application  \n",
       "2             Industrials  Specialty Industrial Machinery  \n",
       "3  Communication Services                Telecom Services  \n",
       "4             Industrials             Aerospace & Defense  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://disfold.com/stock-index/dax/companies/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "scraped_data = []\n",
    "for row in table.find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    scraped_data.append(cols)\n",
    "\n",
    "def clean_scraped_data(data):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in data:\n",
    "        # Remove empty rows\n",
    "        if len(row) > 0:\n",
    "            # Remove the '$' and ',' signs from the market cap and convert it to float\n",
    "            market_cap = float(row[3].replace('$', '').replace(',', '').replace('B', ''))\n",
    "            cleaned_data.append([row[1], row[2], market_cap, row[4], row[5], row[6]])\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_data, columns=['company_name', 'symbol', 'market_cap_in_usd_b', 'country', 'sector', 'industry'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "company_enrichments = clean_scraped_data(scraped_data)\n",
    "company_enrichments.to_csv('../data/dax_company_sectors.csv', index=False)\n",
    "company_enrichments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the ticker symbols to prevent NaN and ensure correct join conditions\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('SRT3', 'SRT')\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('HEN3', 'HNK')\n",
    "company_enrichments.loc[company_enrichments['company_name'] == 'Mercedes-Benz Group AG', 'symbol'] = 'DAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['symbol'] = cleaned_data['symbol'].astype(pd.StringDtype())\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].astype(pd.StringDtype())\n",
    "\n",
    "# Merge the cleaned data with the enrichment\n",
    "enriched_cleaned_data = pd.merge(cleaned_data, company_enrichments, how='left', on='symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>company_name</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Sustainability Report 2020 We face up to futur...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'Vaccine', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Sustainability Repo...</td>\n",
       "      <td>somewhat different approach purpose value refl...</td>\n",
       "      <td>['somewhat', 'different', 'approach', 'purpose...</td>\n",
       "      <td>['somewhat different', 'approach', 'purpose va...</td>\n",
       "      <td>[('somewhat', 'RB'), ('different', 'JJ'), ('ap...</td>\n",
       "      <td>[[('somewhat', 'RB'), ('different', 'JJ')], [(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Hannover R</td>\n",
       "      <td>Annual Report An overview Gross premium E 01 i...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Vaccine', 'Monopolization', 'Corruption', 'G...</td>\n",
       "      <td>1</td>\n",
       "      <td>HNR1</td>\n",
       "      <td>HannoverRückversicherungAG Annual Report 2021</td>\n",
       "      <td>group net income policyholder surplus book vue...</td>\n",
       "      <td>['group', 'net', 'income', 'policyholder', 'su...</td>\n",
       "      <td>['group net income policyholders', 'surplus bo...</td>\n",
       "      <td>[('group', 'NN'), ('net', 'JJ'), ('income', 'N...</td>\n",
       "      <td>[[('group', 'NN'), ('net', 'JJ'), ('income', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company                                            content   \n",
       "48  Hannover R  Sustainability Report 2020 We face up to futur...  \\\n",
       "76  Hannover R  Annual Report An overview Gross premium E 01 i...   \n",
       "\n",
       "                 datatype        date domain   \n",
       "48  sustainability_report  2020-03-31    NaN  \\\n",
       "76          annual_report  2021-03-31    NaN   \n",
       "\n",
       "                                           esg_topics  internal symbol   \n",
       "48  ['Whistleblowing', 'Vaccine', 'Corruption', 'G...         1   HNR1  \\\n",
       "76  ['Vaccine', 'Monopolization', 'Corruption', 'G...         1   HNR1   \n",
       "\n",
       "                                                title   \n",
       "48  HannoverRückversicherungAG Sustainability Repo...  \\\n",
       "76      HannoverRückversicherungAG Annual Report 2021   \n",
       "\n",
       "                                      cleaned_content   \n",
       "48  somewhat different approach purpose value refl...  \\\n",
       "76  group net income policyholder surplus book vue...   \n",
       "\n",
       "                                          word_tokens   \n",
       "48  ['somewhat', 'different', 'approach', 'purpose...  \\\n",
       "76  ['group', 'net', 'income', 'policyholder', 'su...   \n",
       "\n",
       "                                      sentence_tokens   \n",
       "48  ['somewhat different', 'approach', 'purpose va...  \\\n",
       "76  ['group net income policyholders', 'surplus bo...   \n",
       "\n",
       "                               pos_tagged_word_tokens   \n",
       "48  [('somewhat', 'RB'), ('different', 'JJ'), ('ap...  \\\n",
       "76  [('group', 'NN'), ('net', 'JJ'), ('income', 'N...   \n",
       "\n",
       "                           pos_tagged_sentence_tokens company_name   \n",
       "48  [[('somewhat', 'RB'), ('different', 'JJ')], [(...          NaN  \\\n",
       "76  [[('group', 'NN'), ('net', 'JJ'), ('income', '...          NaN   \n",
       "\n",
       "    market_cap_in_usd_b country sector industry  \n",
       "48                  NaN     NaN    NaN      NaN  \n",
       "76                  NaN     NaN    NaN      NaN  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_cleaned_data[enriched_cleaned_data['industry'].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hannover R AG cannot be matched, since it is not present in the scraped data. Since there are only 2 records this is negligible and will be fixed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'sector'] = 'Financials'\n",
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R', 'industry'] = 'Insurance—Reinsurance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns/data\n",
    "enriched_cleaned_data = enriched_cleaned_data.drop(columns=['content', 'company_name', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7539</th>\n",
       "      <td>Qiagen</td>\n",
       "      <td>thinktank</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>thelancet</td>\n",
       "      <td>['GenderDiversity', 'Vaccine', 'Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>QIA</td>\n",
       "      <td>SARS-CoV-2 antibody-positivity protects agains...</td>\n",
       "      <td>sarscov2 documented raising public health conc...</td>\n",
       "      <td>['sarscov2', 'documented', 'raising', 'public'...</td>\n",
       "      <td>['documented raising public health concerns', ...</td>\n",
       "      <td>[('sarscov2', 'NN'), ('documented', 'VBD'), ('...</td>\n",
       "      <td>[[('documented', 'VBD'), ('raising', 'VBG'), (...</td>\n",
       "      <td>11.38</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>Puma</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['HumanCapital', 'Social', 'GenderDiversity', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>PUM</td>\n",
       "      <td>Puma: SOCIAL JUSTICE ADVOCATE AND MUSICIAN DEO...</td>\n",
       "      <td>want work always inspire meaningful opportunit...</td>\n",
       "      <td>['want', 'work', 'always', 'inspire', 'meaning...</td>\n",
       "      <td>['want work always inspire meaningful', 'oppor...</td>\n",
       "      <td>[('want', 'JJ'), ('work', 'NN'), ('always', 'R...</td>\n",
       "      <td>[[('want', 'VBP'), ('work', 'NN'), ('always', ...</td>\n",
       "      <td>9.74</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Footwear &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>Siemens Healthineers</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>SHL</td>\n",
       "      <td>Revolutionary Technology Shortens Cardiac Scan...</td>\n",
       "      <td>imaging mechanism performance evaluation selfc...</td>\n",
       "      <td>['imaging', 'mechanism', 'performance', 'evalu...</td>\n",
       "      <td>['imaging mechanism performance evaluation sel...</td>\n",
       "      <td>[('imaging', 'VBG'), ('mechanism', 'NN'), ('pe...</td>\n",
       "      <td>[[('imaging', 'VBG'), ('mechanism', 'NN'), ('p...</td>\n",
       "      <td>55.11</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>business</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>business-review</td>\n",
       "      <td>['RenewableEnergy', 'Environment', 'Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>Top story of the year in telecom: 5G rollout</td>\n",
       "      <td>one industry developed rapidly last couple dec...</td>\n",
       "      <td>['one', 'industry', 'developed', 'rapidly', 'l...</td>\n",
       "      <td>['one industries developed rapidly last couple...</td>\n",
       "      <td>[('one', 'CD'), ('industry', 'NN'), ('develope...</td>\n",
       "      <td>[[('one', 'CD'), ('developed', 'VBD'), ('rapid...</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9166</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['RenewableEnergy']</td>\n",
       "      <td>0</td>\n",
       "      <td>SIE</td>\n",
       "      <td>Siemens Gamesa Renewable Energy S A: Segundo T...</td>\n",
       "      <td>disclaimer material prepared gamesa renewable ...</td>\n",
       "      <td>['disclaimer', 'material', 'prepared', 'gamesa...</td>\n",
       "      <td>['disclaimer material prepared gamesa renewabl...</td>\n",
       "      <td>[('disclaimer', 'JJ'), ('material', 'NN'), ('p...</td>\n",
       "      <td>[[('disclaimer', 'NN'), ('material', 'NN'), ('...</td>\n",
       "      <td>110.13</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Siemens Healthineers</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-10-03</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Social', 'GenderDiversity']</td>\n",
       "      <td>0</td>\n",
       "      <td>SHL</td>\n",
       "      <td>Latest Ultrasound Advances on Display at RSNA ...</td>\n",
       "      <td>december issue view chart acuson sequoia healt...</td>\n",
       "      <td>['december', 'issue', 'view', 'chart', 'acuson...</td>\n",
       "      <td>['december issue', 'view chart', 'acuson sequo...</td>\n",
       "      <td>[('december', 'JJ'), ('issue', 'NN'), ('view',...</td>\n",
       "      <td>[[('december', 'JJ'), ('issue', 'NN')], [('vie...</td>\n",
       "      <td>55.11</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10902</th>\n",
       "      <td>Zalando</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>finews</td>\n",
       "      <td>['HumanCapital', 'Recycling']</td>\n",
       "      <td>0</td>\n",
       "      <td>ZAL</td>\n",
       "      <td>CEO-Turned-Full-Time Dad Resurfaces at Swiss Bank</td>\n",
       "      <td>time caregiver three daughter back zurichbased...</td>\n",
       "      <td>['time', 'caregiver', 'three', 'daughter', 'ba...</td>\n",
       "      <td>['time caregiver three daughters back', 'zuric...</td>\n",
       "      <td>[('time', 'NN'), ('caregiver', 'NN'), ('three'...</td>\n",
       "      <td>[[('time', 'NN'), ('caregiver', 'NN'), ('three...</td>\n",
       "      <td>10.41</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Internet Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9838</th>\n",
       "      <td>Siemens Healthineers</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-10-03</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Social', 'PatientSafety']</td>\n",
       "      <td>0</td>\n",
       "      <td>SHL</td>\n",
       "      <td>Toshiba’ s Infinix 4DCT Seamlessly Integrates ...</td>\n",
       "      <td>industry first seamless integration interventi...</td>\n",
       "      <td>['industry', 'first', 'seamless', 'integration...</td>\n",
       "      <td>['industry first seamless integration interven...</td>\n",
       "      <td>[('industry', 'NN'), ('first', 'RB'), ('seamle...</td>\n",
       "      <td>[[('industry', 'NN'), ('first', 'JJ'), ('seaml...</td>\n",
       "      <td>55.11</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9762</th>\n",
       "      <td>Siemens Healthineers</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>itnonline</td>\n",
       "      <td>['Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>SHL</td>\n",
       "      <td>ASTRO 2022 Shines Spotlight on “ Cancer Breakt...</td>\n",
       "      <td>representative american society clinical oncol...</td>\n",
       "      <td>['representative', 'american', 'society', 'cli...</td>\n",
       "      <td>['representatives american society clinical on...</td>\n",
       "      <td>[('representative', 'JJ'), ('american', 'JJ'),...</td>\n",
       "      <td>[[('american', 'JJ'), ('society', 'NN'), ('cli...</td>\n",
       "      <td>55.11</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>Airbus</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-10-12</td>\n",
       "      <td>scitechdaily</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>AIR</td>\n",
       "      <td>A European Push to the Moon and Beyond</td>\n",
       "      <td>orion venture thousand mile beyond moon approx...</td>\n",
       "      <td>['orion', 'venture', 'thousand', 'mile', 'beyo...</td>\n",
       "      <td>['orion venture thousands miles beyond moon ap...</td>\n",
       "      <td>[('orion', 'NN'), ('venture', 'NN'), ('thousan...</td>\n",
       "      <td>[[('orion', 'NN'), ('venture', 'NN'), ('beyond...</td>\n",
       "      <td>96.87</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company   datatype        date           domain   \n",
       "7539                 Qiagen  thinktank  2021-04-27        thelancet  \\\n",
       "7453                   Puma   business  2021-12-28   marketscreener   \n",
       "9901   Siemens Healthineers    general  2022-06-15        itnonline   \n",
       "4735       Deutsche Telekom   business  2023-02-22  business-review   \n",
       "9166                Siemens   business  2021-04-30   marketscreener   \n",
       "9994   Siemens Healthineers    general  2021-10-03        itnonline   \n",
       "10902               Zalando    general  2021-02-11           finews   \n",
       "9838   Siemens Healthineers    general  2021-10-03        itnonline   \n",
       "9762   Siemens Healthineers    general  2022-11-02        itnonline   \n",
       "737                  Airbus       tech  2021-10-12     scitechdaily   \n",
       "\n",
       "                                              esg_topics  internal symbol   \n",
       "7539            ['GenderDiversity', 'Vaccine', 'Social']         0    QIA  \\\n",
       "7453   ['HumanCapital', 'Social', 'GenderDiversity', ...         0    PUM   \n",
       "9901                                          ['Social']         0    SHL   \n",
       "4735        ['RenewableEnergy', 'Environment', 'Social']         0    DTE   \n",
       "9166                                 ['RenewableEnergy']         0    SIE   \n",
       "9994                       ['Social', 'GenderDiversity']         0    SHL   \n",
       "10902                      ['HumanCapital', 'Recycling']         0    ZAL   \n",
       "9838                         ['Social', 'PatientSafety']         0    SHL   \n",
       "9762                                          ['Social']         0    SHL   \n",
       "737                                                   []         0    AIR   \n",
       "\n",
       "                                                   title   \n",
       "7539   SARS-CoV-2 antibody-positivity protects agains...  \\\n",
       "7453   Puma: SOCIAL JUSTICE ADVOCATE AND MUSICIAN DEO...   \n",
       "9901   Revolutionary Technology Shortens Cardiac Scan...   \n",
       "4735        Top story of the year in telecom: 5G rollout   \n",
       "9166   Siemens Gamesa Renewable Energy S A: Segundo T...   \n",
       "9994   Latest Ultrasound Advances on Display at RSNA ...   \n",
       "10902  CEO-Turned-Full-Time Dad Resurfaces at Swiss Bank   \n",
       "9838   Toshiba’ s Infinix 4DCT Seamlessly Integrates ...   \n",
       "9762   ASTRO 2022 Shines Spotlight on “ Cancer Breakt...   \n",
       "737               A European Push to the Moon and Beyond   \n",
       "\n",
       "                                         cleaned_content   \n",
       "7539   sarscov2 documented raising public health conc...  \\\n",
       "7453   want work always inspire meaningful opportunit...   \n",
       "9901   imaging mechanism performance evaluation selfc...   \n",
       "4735   one industry developed rapidly last couple dec...   \n",
       "9166   disclaimer material prepared gamesa renewable ...   \n",
       "9994   december issue view chart acuson sequoia healt...   \n",
       "10902  time caregiver three daughter back zurichbased...   \n",
       "9838   industry first seamless integration interventi...   \n",
       "9762   representative american society clinical oncol...   \n",
       "737    orion venture thousand mile beyond moon approx...   \n",
       "\n",
       "                                             word_tokens   \n",
       "7539   ['sarscov2', 'documented', 'raising', 'public'...  \\\n",
       "7453   ['want', 'work', 'always', 'inspire', 'meaning...   \n",
       "9901   ['imaging', 'mechanism', 'performance', 'evalu...   \n",
       "4735   ['one', 'industry', 'developed', 'rapidly', 'l...   \n",
       "9166   ['disclaimer', 'material', 'prepared', 'gamesa...   \n",
       "9994   ['december', 'issue', 'view', 'chart', 'acuson...   \n",
       "10902  ['time', 'caregiver', 'three', 'daughter', 'ba...   \n",
       "9838   ['industry', 'first', 'seamless', 'integration...   \n",
       "9762   ['representative', 'american', 'society', 'cli...   \n",
       "737    ['orion', 'venture', 'thousand', 'mile', 'beyo...   \n",
       "\n",
       "                                         sentence_tokens   \n",
       "7539   ['documented raising public health concerns', ...  \\\n",
       "7453   ['want work always inspire meaningful', 'oppor...   \n",
       "9901   ['imaging mechanism performance evaluation sel...   \n",
       "4735   ['one industries developed rapidly last couple...   \n",
       "9166   ['disclaimer material prepared gamesa renewabl...   \n",
       "9994   ['december issue', 'view chart', 'acuson sequo...   \n",
       "10902  ['time caregiver three daughters back', 'zuric...   \n",
       "9838   ['industry first seamless integration interven...   \n",
       "9762   ['representatives american society clinical on...   \n",
       "737    ['orion venture thousands miles beyond moon ap...   \n",
       "\n",
       "                                  pos_tagged_word_tokens   \n",
       "7539   [('sarscov2', 'NN'), ('documented', 'VBD'), ('...  \\\n",
       "7453   [('want', 'JJ'), ('work', 'NN'), ('always', 'R...   \n",
       "9901   [('imaging', 'VBG'), ('mechanism', 'NN'), ('pe...   \n",
       "4735   [('one', 'CD'), ('industry', 'NN'), ('develope...   \n",
       "9166   [('disclaimer', 'JJ'), ('material', 'NN'), ('p...   \n",
       "9994   [('december', 'JJ'), ('issue', 'NN'), ('view',...   \n",
       "10902  [('time', 'NN'), ('caregiver', 'NN'), ('three'...   \n",
       "9838   [('industry', 'NN'), ('first', 'RB'), ('seamle...   \n",
       "9762   [('representative', 'JJ'), ('american', 'JJ'),...   \n",
       "737    [('orion', 'NN'), ('venture', 'NN'), ('thousan...   \n",
       "\n",
       "                              pos_tagged_sentence_tokens  market_cap_in_usd_b   \n",
       "7539   [[('documented', 'VBD'), ('raising', 'VBG'), (...                11.38  \\\n",
       "7453   [[('want', 'VBP'), ('work', 'NN'), ('always', ...                 9.74   \n",
       "9901   [[('imaging', 'VBG'), ('mechanism', 'NN'), ('p...                55.11   \n",
       "4735   [[('one', 'CD'), ('developed', 'VBD'), ('rapid...               101.78   \n",
       "9166   [[('disclaimer', 'NN'), ('material', 'NN'), ('...               110.13   \n",
       "9994   [[('december', 'JJ'), ('issue', 'NN')], [('vie...                55.11   \n",
       "10902  [[('time', 'NN'), ('caregiver', 'NN'), ('three...                10.41   \n",
       "9838   [[('industry', 'NN'), ('first', 'JJ'), ('seaml...                55.11   \n",
       "9762   [[('american', 'JJ'), ('society', 'NN'), ('cli...                55.11   \n",
       "737    [[('orion', 'NN'), ('venture', 'NN'), ('beyond...                96.87   \n",
       "\n",
       "                       sector                        industry  \n",
       "7539               Healthcare          Diagnostics & Research  \n",
       "7453   Consumer Discretionary          Footwear & Accessories  \n",
       "9901               Healthcare          Diagnostics & Research  \n",
       "4735   Communication Services                Telecom Services  \n",
       "9166              Industrials  Specialty Industrial Machinery  \n",
       "9994               Healthcare          Diagnostics & Research  \n",
       "10902  Consumer Discretionary                 Internet Retail  \n",
       "9838               Healthcare          Diagnostics & Research  \n",
       "9762               Healthcare          Diagnostics & Research  \n",
       "737               Industrials             Aerospace & Defense  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check final dataframe\n",
    "enriched_cleaned_data.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Sentiment Value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last preprocessing step, the sentiment is calculated with the (quite basic) SentimentIntensityAnalyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute the sentiment of the report\n",
    "def get_sentiment_score(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "enriched_cleaned_data['sentiment_value'] = enriched_cleaned_data['cleaned_content'].apply(get_sentiment_score) # Sentence tokens +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>sentiment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>AkzoNobelNV</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>chemistryworld</td>\n",
       "      <td>['AnimalTesting']</td>\n",
       "      <td>0</td>\n",
       "      <td>BAS</td>\n",
       "      <td>Non-animal test for skin sensitisation gets OE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>48.10</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-11-09</td>\n",
       "      <td>law360</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>Former Deutsche Bank Exec Says Discrimination ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-08-20</td>\n",
       "      <td>globalcapital</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>Deutsche Bank debuts green label in Formosa fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>RWE</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['RenewableEnergy']</td>\n",
       "      <td>0</td>\n",
       "      <td>RWE</td>\n",
       "      <td>Germany's RWE Expands US Green Power Portfolio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>29.97</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Utilities—Diversified</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            company  datatype        date          domain   \n",
       "1574    AkzoNobelNV   general  2021-09-09  chemistryworld  \\\n",
       "3881  Deutsche Bank   general  2022-11-09          law360   \n",
       "3995  Deutsche Bank  business  2021-08-20   globalcapital   \n",
       "7858            RWE   general  2022-10-03     energyintel   \n",
       "\n",
       "               esg_topics  internal symbol   \n",
       "1574    ['AnimalTesting']         0    BAS  \\\n",
       "3881                   []         0    DBK   \n",
       "3995                   []         0    DBK   \n",
       "7858  ['RenewableEnergy']         0    RWE   \n",
       "\n",
       "                                                  title cleaned_content   \n",
       "1574  Non-animal test for skin sensitisation gets OE...             NaN  \\\n",
       "3881  Former Deutsche Bank Exec Says Discrimination ...             NaN   \n",
       "3995  Deutsche Bank debuts green label in Formosa fo...             NaN   \n",
       "7858     Germany's RWE Expands US Green Power Portfolio             NaN   \n",
       "\n",
       "     word_tokens sentence_tokens pos_tagged_word_tokens   \n",
       "1574          []              []                     []  \\\n",
       "3881          []            ['']                     []   \n",
       "3995          []            ['']                     []   \n",
       "7858          []            ['']                     []   \n",
       "\n",
       "     pos_tagged_sentence_tokens  market_cap_in_usd_b           sector   \n",
       "1574                         []                48.10  Basic Materials  \\\n",
       "3881                       [[]]                24.97       Financials   \n",
       "3995                       [[]]                24.97       Financials   \n",
       "7858                       [[]]                29.97        Utilities   \n",
       "\n",
       "                   industry  sentiment_value  \n",
       "1574              Chemicals              0.0  \n",
       "3881                  Banks              0.0  \n",
       "3995                  Banks              0.0  \n",
       "7858  Utilities—Diversified              0.0  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NaN values\n",
    "enriched_cleaned_data[enriched_cleaned_data['cleaned_content'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN columns\n",
    "enriched_cleaned_data = enriched_cleaned_data.dropna(subset=['cleaned_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to enriched_cleaned_data.csv\n",
      "Loaded DataFrame from enriched_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file for further analysis\n",
    "enriched_cleaned_data = csv_checkpoint(enriched_cleaned_data, 'enriched_cleaned_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>pos_tagged_word_tokens</th>\n",
       "      <th>pos_tagged_sentence_tokens</th>\n",
       "      <th>market_cap_in_usd_b</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>sentiment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>Deutsche Post</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-06-18</td>\n",
       "      <td>supplychainbrain</td>\n",
       "      <td>['Environment', 'Social', 'Biofuel', 'LowCarbo...</td>\n",
       "      <td>0</td>\n",
       "      <td>DPW</td>\n",
       "      <td>DHL Adds Sustainable Marine Fuel Option for Fu...</td>\n",
       "      <td>air ocean freight specialist deutsche post dhl...</td>\n",
       "      <td>['air', 'ocean', 'freight', 'specialist', 'deu...</td>\n",
       "      <td>['air ocean freight specialist deutsche post d...</td>\n",
       "      <td>[('air', 'NN'), ('ocean', 'JJ'), ('freight', '...</td>\n",
       "      <td>[[('air', 'NN'), ('ocean', 'JJ'), ('freight', ...</td>\n",
       "      <td>45.40</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Integrated Freight &amp; Logistics</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>Covestro</td>\n",
       "      <td>thinktank</td>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>spglobal</td>\n",
       "      <td>['Solvents']</td>\n",
       "      <td>0</td>\n",
       "      <td>1COV</td>\n",
       "      <td>Urethane Surface Coatings - Chemical Economics...</td>\n",
       "      <td>analytics expertise sign product service cente...</td>\n",
       "      <td>['analytics', 'expertise', 'sign', 'product', ...</td>\n",
       "      <td>['analytics expertise', 'sign product service ...</td>\n",
       "      <td>[('analytics', 'NNS'), ('expertise', 'VBP'), (...</td>\n",
       "      <td>[[('analytics', 'NNS'), ('expertise', 'VBP')],...</td>\n",
       "      <td>8.40</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Specialty Chemicals</td>\n",
       "      <td>0.9970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Adidas</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-07-21</td>\n",
       "      <td>teenvogue</td>\n",
       "      <td>['Privacy', 'GenderDiversity']</td>\n",
       "      <td>0</td>\n",
       "      <td>ADS</td>\n",
       "      <td>Olympics 2020 Athletes to Know: Rock Climber B...</td>\n",
       "      <td>teen vogue caught brightest team usa star head...</td>\n",
       "      <td>['teen', 'vogue', 'caught', 'brightest', 'team...</td>\n",
       "      <td>['teen vogue caught brightest team usa stars h...</td>\n",
       "      <td>[('teen', 'JJ'), ('vogue', 'NN'), ('caught', '...</td>\n",
       "      <td>[[('teen', 'JJ'), ('vogue', 'NN'), ('caught', ...</td>\n",
       "      <td>25.77</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Footwear &amp; Accessories</td>\n",
       "      <td>0.9993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-07-20</td>\n",
       "      <td>finextra</td>\n",
       "      <td>['GenderDiversity', 'HumanCapital']</td>\n",
       "      <td>0</td>\n",
       "      <td>DBK</td>\n",
       "      <td>LedgerEdge appoints Michelle Neal as CEO of US...</td>\n",
       "      <td>rolling global corporate bond trading platform...</td>\n",
       "      <td>['rolling', 'global', 'corporate', 'bond', 'tr...</td>\n",
       "      <td>['rolling global corporate bond trading platfo...</td>\n",
       "      <td>[('rolling', 'VBG'), ('global', 'JJ'), ('corpo...</td>\n",
       "      <td>[[('rolling', 'VBG'), ('global', 'JJ'), ('corp...</td>\n",
       "      <td>24.97</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.9840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>BMW</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>digitaljournal</td>\n",
       "      <td>['HealthRisk', 'Pollution', 'Hydropower', 'Tox...</td>\n",
       "      <td>0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>Indonesia scrubbing the 'world's dirtiest river '</td>\n",
       "      <td>hi looking published scabies indonesian rice f...</td>\n",
       "      <td>['hi', 'looking', 'published', 'scabies', 'ind...</td>\n",
       "      <td>['hi looking', 'published scabies indonesian r...</td>\n",
       "      <td>[('hi', 'NN'), ('looking', 'VBG'), ('published...</td>\n",
       "      <td>[[('hi', 'NN'), ('looking', 'VBG')], [('publis...</td>\n",
       "      <td>60.24</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>-0.9972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Adidas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-09-28</td>\n",
       "      <td>vegconomist</td>\n",
       "      <td>['Overconsumption', 'AnimalWelfare', 'Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>ADS</td>\n",
       "      <td>Kering Ends Use of Fur at All Its Fashion Hous...</td>\n",
       "      <td>none fashion house use real animal fur include...</td>\n",
       "      <td>['none', 'fashion', 'house', 'use', 'real', 'a...</td>\n",
       "      <td>['none fashion houses use real animal fur', 'i...</td>\n",
       "      <td>[('none', 'NN'), ('fashion', 'NN'), ('house', ...</td>\n",
       "      <td>[[('none', 'NN'), ('fashion', 'NN'), ('use', '...</td>\n",
       "      <td>25.77</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Footwear &amp; Accessories</td>\n",
       "      <td>0.9186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>AkzoNobelNV</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>business-review</td>\n",
       "      <td>['Social', 'HumanCapital']</td>\n",
       "      <td>0</td>\n",
       "      <td>BAS</td>\n",
       "      <td>BASF and its employees donate an additional € ...</td>\n",
       "      <td>much basf employee donated basf stiftung basf ...</td>\n",
       "      <td>['much', 'basf', 'employee', 'donated', 'basf'...</td>\n",
       "      <td>['much basf employees donated basf stiftung ba...</td>\n",
       "      <td>[('much', 'JJ'), ('basf', 'NN'), ('employee', ...</td>\n",
       "      <td>[[('much', 'JJ'), ('basf', 'NN'), ('donated', ...</td>\n",
       "      <td>48.10</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>0.9950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5070</th>\n",
       "      <td>Deutsche Telekom</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-08-11</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>Good news keeps on coming</td>\n",
       "      <td>july producer price rose le expected yearonyea...</td>\n",
       "      <td>['july', 'producer', 'price', 'rose', 'le', 'e...</td>\n",
       "      <td>['us july producer prices rose less expected y...</td>\n",
       "      <td>[('july', 'RB'), ('producer', 'NN'), ('price',...</td>\n",
       "      <td>[[('july', 'JJ'), ('producer', 'NN'), ('rose',...</td>\n",
       "      <td>101.78</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Telecom Services</td>\n",
       "      <td>0.8957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10380</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>swissinfo</td>\n",
       "      <td>['Social', 'Privacy', 'CarbonDioxide', 'LowCar...</td>\n",
       "      <td>0</td>\n",
       "      <td>VOW3</td>\n",
       "      <td>Predicting future mobility, and remembering a ...</td>\n",
       "      <td>selfdriving car also way mature enough meet cl...</td>\n",
       "      <td>['selfdriving', 'car', 'also', 'way', 'mature'...</td>\n",
       "      <td>['selfdriving cars also way', 'mature enough m...</td>\n",
       "      <td>[('selfdriving', 'VBG'), ('car', 'NN'), ('also...</td>\n",
       "      <td>[[('selfdriving', 'VBG'), ('also', 'RB'), ('wa...</td>\n",
       "      <td>75.05</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>0.9868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>business</td>\n",
       "      <td>2022-09-02</td>\n",
       "      <td>marketscreener</td>\n",
       "      <td>['LowCarbon']</td>\n",
       "      <td>0</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>Excelerate Energy Signs Term Sheet to Deploy a...</td>\n",
       "      <td>inc nyse ee excelerate announced august engie ...</td>\n",
       "      <td>['inc', 'nyse', 'ee', 'excelerate', 'announced...</td>\n",
       "      <td>['nyse ee announced august engie signed term s...</td>\n",
       "      <td>[('inc', 'NN'), ('nyse', 'CC'), ('ee', 'JJ'), ...</td>\n",
       "      <td>[[('nyse', 'CC'), ('ee', 'JJ'), ('announced', ...</td>\n",
       "      <td>27.70</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Utilities—Diversified</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company   datatype        date            domain   \n",
       "4458      Deutsche Post       tech  2021-06-18  supplychainbrain  \\\n",
       "3216           Covestro  thinktank  2022-11-28          spglobal   \n",
       "178              Adidas    general  2021-07-21         teenvogue   \n",
       "3964      Deutsche Bank       tech  2021-07-20          finextra   \n",
       "1937                BMW       tech  2021-03-30    digitaljournal   \n",
       "580              Adidas        NaN  2021-09-28       vegconomist   \n",
       "1383        AkzoNobelNV   business  2022-04-27   business-review   \n",
       "5070   Deutsche Telekom   business  2022-08-11    marketscreener   \n",
       "10380        Volkswagen    general  2021-04-30         swissinfo   \n",
       "5191             E ONSE   business  2022-09-02    marketscreener   \n",
       "\n",
       "                                              esg_topics  internal symbol   \n",
       "4458   ['Environment', 'Social', 'Biofuel', 'LowCarbo...         0    DPW  \\\n",
       "3216                                        ['Solvents']         0   1COV   \n",
       "178                       ['Privacy', 'GenderDiversity']         0    ADS   \n",
       "3964                 ['GenderDiversity', 'HumanCapital']         0    DBK   \n",
       "1937   ['HealthRisk', 'Pollution', 'Hydropower', 'Tox...         0    BMW   \n",
       "580       ['Overconsumption', 'AnimalWelfare', 'Social']         0    ADS   \n",
       "1383                          ['Social', 'HumanCapital']         0    BAS   \n",
       "5070                                                  []         0    DTE   \n",
       "10380  ['Social', 'Privacy', 'CarbonDioxide', 'LowCar...         0   VOW3   \n",
       "5191                                       ['LowCarbon']         0   EOAN   \n",
       "\n",
       "                                                   title   \n",
       "4458   DHL Adds Sustainable Marine Fuel Option for Fu...  \\\n",
       "3216   Urethane Surface Coatings - Chemical Economics...   \n",
       "178    Olympics 2020 Athletes to Know: Rock Climber B...   \n",
       "3964   LedgerEdge appoints Michelle Neal as CEO of US...   \n",
       "1937   Indonesia scrubbing the 'world's dirtiest river '   \n",
       "580    Kering Ends Use of Fur at All Its Fashion Hous...   \n",
       "1383   BASF and its employees donate an additional € ...   \n",
       "5070                           Good news keeps on coming   \n",
       "10380  Predicting future mobility, and remembering a ...   \n",
       "5191   Excelerate Energy Signs Term Sheet to Deploy a...   \n",
       "\n",
       "                                         cleaned_content   \n",
       "4458   air ocean freight specialist deutsche post dhl...  \\\n",
       "3216   analytics expertise sign product service cente...   \n",
       "178    teen vogue caught brightest team usa star head...   \n",
       "3964   rolling global corporate bond trading platform...   \n",
       "1937   hi looking published scabies indonesian rice f...   \n",
       "580    none fashion house use real animal fur include...   \n",
       "1383   much basf employee donated basf stiftung basf ...   \n",
       "5070   july producer price rose le expected yearonyea...   \n",
       "10380  selfdriving car also way mature enough meet cl...   \n",
       "5191   inc nyse ee excelerate announced august engie ...   \n",
       "\n",
       "                                             word_tokens   \n",
       "4458   ['air', 'ocean', 'freight', 'specialist', 'deu...  \\\n",
       "3216   ['analytics', 'expertise', 'sign', 'product', ...   \n",
       "178    ['teen', 'vogue', 'caught', 'brightest', 'team...   \n",
       "3964   ['rolling', 'global', 'corporate', 'bond', 'tr...   \n",
       "1937   ['hi', 'looking', 'published', 'scabies', 'ind...   \n",
       "580    ['none', 'fashion', 'house', 'use', 'real', 'a...   \n",
       "1383   ['much', 'basf', 'employee', 'donated', 'basf'...   \n",
       "5070   ['july', 'producer', 'price', 'rose', 'le', 'e...   \n",
       "10380  ['selfdriving', 'car', 'also', 'way', 'mature'...   \n",
       "5191   ['inc', 'nyse', 'ee', 'excelerate', 'announced...   \n",
       "\n",
       "                                         sentence_tokens   \n",
       "4458   ['air ocean freight specialist deutsche post d...  \\\n",
       "3216   ['analytics expertise', 'sign product service ...   \n",
       "178    ['teen vogue caught brightest team usa stars h...   \n",
       "3964   ['rolling global corporate bond trading platfo...   \n",
       "1937   ['hi looking', 'published scabies indonesian r...   \n",
       "580    ['none fashion houses use real animal fur', 'i...   \n",
       "1383   ['much basf employees donated basf stiftung ba...   \n",
       "5070   ['us july producer prices rose less expected y...   \n",
       "10380  ['selfdriving cars also way', 'mature enough m...   \n",
       "5191   ['nyse ee announced august engie signed term s...   \n",
       "\n",
       "                                  pos_tagged_word_tokens   \n",
       "4458   [('air', 'NN'), ('ocean', 'JJ'), ('freight', '...  \\\n",
       "3216   [('analytics', 'NNS'), ('expertise', 'VBP'), (...   \n",
       "178    [('teen', 'JJ'), ('vogue', 'NN'), ('caught', '...   \n",
       "3964   [('rolling', 'VBG'), ('global', 'JJ'), ('corpo...   \n",
       "1937   [('hi', 'NN'), ('looking', 'VBG'), ('published...   \n",
       "580    [('none', 'NN'), ('fashion', 'NN'), ('house', ...   \n",
       "1383   [('much', 'JJ'), ('basf', 'NN'), ('employee', ...   \n",
       "5070   [('july', 'RB'), ('producer', 'NN'), ('price',...   \n",
       "10380  [('selfdriving', 'VBG'), ('car', 'NN'), ('also...   \n",
       "5191   [('inc', 'NN'), ('nyse', 'CC'), ('ee', 'JJ'), ...   \n",
       "\n",
       "                              pos_tagged_sentence_tokens  market_cap_in_usd_b   \n",
       "4458   [[('air', 'NN'), ('ocean', 'JJ'), ('freight', ...                45.40  \\\n",
       "3216   [[('analytics', 'NNS'), ('expertise', 'VBP')],...                 8.40   \n",
       "178    [[('teen', 'JJ'), ('vogue', 'NN'), ('caught', ...                25.77   \n",
       "3964   [[('rolling', 'VBG'), ('global', 'JJ'), ('corp...                24.97   \n",
       "1937   [[('hi', 'NN'), ('looking', 'VBG')], [('publis...                60.24   \n",
       "580    [[('none', 'NN'), ('fashion', 'NN'), ('use', '...                25.77   \n",
       "1383   [[('much', 'JJ'), ('basf', 'NN'), ('donated', ...                48.10   \n",
       "5070   [[('july', 'JJ'), ('producer', 'NN'), ('rose',...               101.78   \n",
       "10380  [[('selfdriving', 'VBG'), ('also', 'RB'), ('wa...                75.05   \n",
       "5191   [[('nyse', 'CC'), ('ee', 'JJ'), ('announced', ...                27.70   \n",
       "\n",
       "                       sector                        industry  sentiment_value  \n",
       "4458              Industrials  Integrated Freight & Logistics           0.9864  \n",
       "3216          Basic Materials             Specialty Chemicals           0.9970  \n",
       "178    Consumer Discretionary          Footwear & Accessories           0.9993  \n",
       "3964               Financials                           Banks           0.9840  \n",
       "1937   Consumer Discretionary              Auto Manufacturers          -0.9972  \n",
       "580    Consumer Discretionary          Footwear & Accessories           0.9186  \n",
       "1383          Basic Materials                       Chemicals           0.9950  \n",
       "5070   Communication Services                Telecom Services           0.8957  \n",
       "10380  Consumer Discretionary              Auto Manufacturers           0.9868  \n",
       "5191                Utilities           Utilities—Diversified           0.9864  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data after full preprocessing\n",
    "enriched_cleaned_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
