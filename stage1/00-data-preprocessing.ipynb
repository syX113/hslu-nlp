{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import contractions\n",
    "from langdetect import detect \n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tim/.pyenv/versions/3.10.10/envs/venv-nlp/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# Download a spacy model, can also be adjusted (medium = en_core_web_md, large = en_core_web_lg)\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_csv('../data/esg_documents_for_dax_companies.csv', delimiter = '|', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beiersdorf AG</td>\n",
       "      <td>Sustainability Highlight Report CARE BEYOND SK...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'GHGEmission', 'ProductLiabilit...</td>\n",
       "      <td>1</td>\n",
       "      <td>BEI</td>\n",
       "      <td>BeiersdorfAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>Corporate Responsibility Report 2021 2 Content...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DTE</td>\n",
       "      <td>DeutscheTelekomAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vonovia SE</td>\n",
       "      <td>VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Whistleblowing', 'DataSecurity', 'Vaccine', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>VNA</td>\n",
       "      <td>VonoviaSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merck KGaA</td>\n",
       "      <td>Sustainability Report 2021 TABLE OF CONTENTS S...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'DataMisuse', 'DrugResistance...</td>\n",
       "      <td>1</td>\n",
       "      <td>MRK</td>\n",
       "      <td>MerckKGaA Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTU</td>\n",
       "      <td>Our ideas and concepts FOR A SUSTAINABLE FUTUR...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'AirQuality'...</td>\n",
       "      <td>1</td>\n",
       "      <td>MTX</td>\n",
       "      <td>MTUAeroEngines Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E ONSE</td>\n",
       "      <td>#StandWithUkraine Sustainability Report 2021 C...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'GlobalWarming', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>EOAN</td>\n",
       "      <td>E.ONSE Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RWE AG</td>\n",
       "      <td>Focus on tomorrow. Sustainability Report 2021 ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Corruption', 'Iso50001', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>RWE</td>\n",
       "      <td>RWEAG Sustainability Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Annual Report 2021 HeidelbergCement at a glanc...</td>\n",
       "      <td>annual_report</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WorkLifeBalance', 'Vaccine', 'DataSecurity',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Annual Report 2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heidelberg Cement AG</td>\n",
       "      <td>Company Strategy &amp; Business &amp; Product &amp; Produc...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['CleanWater', 'Corruption', 'Whistleblowing',...</td>\n",
       "      <td>1</td>\n",
       "      <td>HEI</td>\n",
       "      <td>HeidelbergCementAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>Sustainability 1 Siemens 2 Our 3 Governance – ...</td>\n",
       "      <td>sustainability_report</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['DataSecurity', 'Iso50001', 'EmployeeTurnover...</td>\n",
       "      <td>1</td>\n",
       "      <td>SIE</td>\n",
       "      <td>SiemensAG Sustainability Report 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company                                            content   \n",
       "0         Beiersdorf AG  Sustainability Highlight Report CARE BEYOND SK...  \\\n",
       "1   Deutsche Telekom AG  Corporate Responsibility Report 2021 2 Content...   \n",
       "2            Vonovia SE  VONOVIA SE SUSTAINABILITY REPORT 2021 =For a S...   \n",
       "3            Merck KGaA  Sustainability Report 2021 TABLE OF CONTENTS S...   \n",
       "4                   MTU  Our ideas and concepts FOR A SUSTAINABLE FUTUR...   \n",
       "5                E ONSE  #StandWithUkraine Sustainability Report 2021 C...   \n",
       "6                RWE AG  Focus on tomorrow. Sustainability Report 2021 ...   \n",
       "7  Heidelberg Cement AG  Annual Report 2021 HeidelbergCement at a glanc...   \n",
       "8  Heidelberg Cement AG  Company Strategy & Business & Product & Produc...   \n",
       "9            Siemens AG  Sustainability 1 Siemens 2 Our 3 Governance – ...   \n",
       "\n",
       "                datatype        date domain   \n",
       "0  sustainability_report  2021-03-31    NaN  \\\n",
       "1  sustainability_report  2021-03-31    NaN   \n",
       "2  sustainability_report  2021-03-31    NaN   \n",
       "3  sustainability_report  2021-03-31    NaN   \n",
       "4  sustainability_report  2020-03-31    NaN   \n",
       "5  sustainability_report  2021-03-31    NaN   \n",
       "6  sustainability_report  2021-03-31    NaN   \n",
       "7          annual_report  2021-03-31    NaN   \n",
       "8  sustainability_report  2020-03-31    NaN   \n",
       "9  sustainability_report  2020-03-31    NaN   \n",
       "\n",
       "                                          esg_topics  internal symbol   \n",
       "0  ['CleanWater', 'GHGEmission', 'ProductLiabilit...         1    BEI  \\\n",
       "1  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1    DTE   \n",
       "2  ['Whistleblowing', 'DataSecurity', 'Vaccine', ...         1    VNA   \n",
       "3  ['DataSecurity', 'DataMisuse', 'DrugResistance...         1    MRK   \n",
       "4  ['WorkLifeBalance', 'Corruption', 'AirQuality'...         1    MTX   \n",
       "5  ['DataSecurity', 'Iso50001', 'GlobalWarming', ...         1   EOAN   \n",
       "6  ['WorkLifeBalance', 'Corruption', 'Iso50001', ...         1    RWE   \n",
       "7  ['WorkLifeBalance', 'Vaccine', 'DataSecurity',...         1    HEI   \n",
       "8  ['CleanWater', 'Corruption', 'Whistleblowing',...         1    HEI   \n",
       "9  ['DataSecurity', 'Iso50001', 'EmployeeTurnover...         1    SIE   \n",
       "\n",
       "                                           title  url  \n",
       "0        BeiersdorfAG Sustainability Report 2021  NaN  \n",
       "1   DeutscheTelekomAG Sustainability Report 2021  NaN  \n",
       "2           VonoviaSE Sustainability Report 2021  NaN  \n",
       "3           MerckKGaA Sustainability Report 2021  NaN  \n",
       "4      MTUAeroEngines Sustainability Report 2020  NaN  \n",
       "5              E.ONSE Sustainability Report 2021  NaN  \n",
       "6               RWEAG Sustainability Report 2021  NaN  \n",
       "7          HeidelbergCementAG Annual Report 2021  NaN  \n",
       "8  HeidelbergCementAG Sustainability Report 2020  NaN  \n",
       "9           SiemensAG Sustainability Report 2020  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded data and reset index\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column descriptions**\n",
    "- symbol: stock symbol of the company\n",
    "- company: company name\n",
    "- date: publication date of document\n",
    "- title: document title\n",
    "- content: document content\n",
    "- datatype: document type\n",
    "- internal: is this a report by company (1) or a third-party document (0)\n",
    "- domain (optional): Web domain where the document was published\n",
    "- url (optional): URL where the document can be accessed\n",
    "- esg_topics (optional): ESG topics extracted from the data using our internal NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11188, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape (row and column amount)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company       object\n",
       "content       object\n",
       "datatype      object\n",
       "date          object\n",
       "domain        object\n",
       "esg_topics    object\n",
       "internal       int64\n",
       "symbol        object\n",
       "title         object\n",
       "url           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "raw_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small checkpoint function to save intermediary processing steps and enhance development\n",
    "def csv_checkpoint(df, filename='checkpoint'):\n",
    "\n",
    "    if not os.path.exists('../data/checkpoints/'):\n",
    "        os.makedirs('../data/checkpoints/')\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(f'../data/checkpoints/{filename}.csv', index=False, sep = '|')\n",
    "    print(f'Saved DataFrame to {filename}.csv')\n",
    "    \n",
    "    # Load CSV back into DataFrame\n",
    "    df = pd.read_csv(f'../data/checkpoints/{filename}.csv', delimiter = '|')\n",
    "    print(f'Loaded DataFrame from {filename}.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As initial data cleaning steps, the following is conducted:\n",
    "- Rows with missing \"content\" were dropped to prevent any missing data-related issues. Missing data can create gaps in the data and lead to errors or distortions in the analysis.\n",
    "- The \"URL\" column was removed as the relevant information was available in the \"domain\" column. Removing redundant columns simplifies the data set and makes it easier to work with\n",
    "- Duplicate entries were identified and removed, resulting in a cleaner and more concise dataset. Duplicates can distort the data and lead to biased analysis. \n",
    "- Language checking was conducted and all rows with non-English content were dropped to ensure consistent language. Language inconsistencies can create bias in the data and lead to inaccurate conclusions. Therefore, it is important to ensure that the data is consistent in language to prevent linguistic biases.\n",
    "- \"Date\" is formatted as a date and wrong dates, e.g. \"bayer-03-31\" are replaced with a default date (2023-03-31).\n",
    "- The \"sample\" method was used to check the data for representativeness and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_cleaned_data = raw_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"url\" column, since the most relevant information from an analysis perspective is already in the \"domain\" column (e.g. the source of the report)\n",
    "general_cleaned_data = general_cleaned_data.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 6\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and delete them\n",
    "duplicates = general_cleaned_data[general_cleaned_data.duplicated()]\n",
    "print(f'Duplicated rows: {len(duplicates)}')\n",
    "general_cleaned_data = general_cleaned_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted amount of rows with language other ehan English: 107\n"
     ]
    }
   ],
   "source": [
    "# Check for other languange than English\n",
    "general_cleaned_data['language'] = general_cleaned_data['content'].apply(lambda x: detect(x))\n",
    "not_english = len(general_cleaned_data) - len(general_cleaned_data.loc[general_cleaned_data['language'] == 'en'])\n",
    "\n",
    "# Drop rows with other languange, since other languanges influences to quality of the later analysis\n",
    "general_cleaned_data = general_cleaned_data.loc[general_cleaned_data['language'] == 'en']\n",
    "\n",
    "print(f'Deleted amount of rows with language other ehan English: {not_english}')\n",
    "general_cleaned_data.drop(['language'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly formatted dates:\n",
      "Row 13: p.DE-03-31\n",
      "Row 18: p.DE-03-31\n",
      "Row 20: bayer-03-31\n",
      "Row 22: p.DE-03-31\n",
      "Row 25: p.DE-03-31\n",
      "Row 26: p.DE-03-31\n",
      "Row 31: p.DE-03-31\n",
      "Row 32: p.DE-03-31\n",
      "Row 33: p.DE-03-31\n",
      "Row 37: p.DE-03-31\n",
      "Row 41: p.DE-03-31\n",
      "Row 50: p.DE-03-31\n",
      "Row 78: p.DE-03-31\n",
      "Row 80: p.DE-03-31\n",
      "Row 86: p.DE-03-31\n",
      "Row 87: p.DE-03-31\n",
      "Row 88: p.DE-03-31\n"
     ]
    }
   ],
   "source": [
    "# Correct the dates to ISO standard\n",
    "def find_incorrect_dates(data):\n",
    "    incorrect_dates = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            pd.to_datetime(row['date'], format='%Y-%m-%d', errors='raise')\n",
    "        except ValueError:\n",
    "            incorrect_dates.append((index, row['date']))\n",
    "\n",
    "    return incorrect_dates\n",
    "\n",
    "incorrect_date_rows = find_incorrect_dates(general_cleaned_data)\n",
    "print(\"Incorrectly formatted dates:\")\n",
    "for index, date in incorrect_date_rows:\n",
    "    print(f\"Row {index}: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the wrong formatted dates and set default date\n",
    "def correct_date_format(data):\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce').fillna('2022-03-31')\n",
    "    return data\n",
    "\n",
    "cleaned_data = correct_date_format(general_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content, e.g. no report\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>datatype</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>esg_topics</th>\n",
       "      <th>internal</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>Airbus SE</td>\n",
       "      <td>Philippine Airlines ( PAL) plans to come out o...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-10-16</td>\n",
       "      <td>simpleflying</td>\n",
       "      <td>['Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Philippine Airlines Plans Long-Haul Route Shakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>Adidas AG</td>\n",
       "      <td>Every product on this page was chosen by a Har...</td>\n",
       "      <td>general</td>\n",
       "      <td>2021-02-14</td>\n",
       "      <td>harpersbazaar</td>\n",
       "      <td>['Social']</td>\n",
       "      <td>0</td>\n",
       "      <td>ADS</td>\n",
       "      <td>Beyoncé﻿ Wears a Skintight, PVC Bodysuit from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>Daimler AG</td>\n",
       "      <td>Hi, what are you looking for? Rising artist Ca...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>digitaljournal</td>\n",
       "      <td>['FossilFuels', 'HumanCapital']</td>\n",
       "      <td>0</td>\n",
       "      <td>DAI</td>\n",
       "      <td>Caleb Polaha talks about his new song 'The Who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>Beiersdorf AG</td>\n",
       "      <td>These materials are part of the specialty chem...</td>\n",
       "      <td>general</td>\n",
       "      <td>2022-12-06</td>\n",
       "      <td>globenewswire</td>\n",
       "      <td>['EMobility', 'RussianFederation', 'Petroleum']</td>\n",
       "      <td>0</td>\n",
       "      <td>BEI</td>\n",
       "      <td>Coatings Adhesives Sealants and Elastomers Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5052</th>\n",
       "      <td>Deutsche Telekom AG</td>\n",
       "      <td>EU consumers spend more than $ 2.8 billion ann...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>zdnet</td>\n",
       "      <td>['GDPR', 'DataSecurity', 'Environment', 'Priva...</td>\n",
       "      <td>0</td>\n",
       "      <td>DTE</td>\n",
       "      <td>EU wants USB-C to become standard charging por...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  company                                            content   \n",
       "770             Airbus SE  Philippine Airlines ( PAL) plans to come out o...  \\\n",
       "357             Adidas AG  Every product on this page was chosen by a Har...   \n",
       "3781           Daimler AG  Hi, what are you looking for? Rising artist Ca...   \n",
       "2903        Beiersdorf AG  These materials are part of the specialty chem...   \n",
       "5052  Deutsche Telekom AG  EU consumers spend more than $ 2.8 billion ann...   \n",
       "\n",
       "     datatype       date          domain   \n",
       "770      tech 2021-10-16    simpleflying  \\\n",
       "357   general 2021-02-14   harpersbazaar   \n",
       "3781     tech 2023-03-13  digitaljournal   \n",
       "2903  general 2022-12-06   globenewswire   \n",
       "5052     tech 2021-09-23           zdnet   \n",
       "\n",
       "                                             esg_topics  internal symbol   \n",
       "770                                          ['Social']         0    AIR  \\\n",
       "357                                          ['Social']         0    ADS   \n",
       "3781                    ['FossilFuels', 'HumanCapital']         0    DAI   \n",
       "2903    ['EMobility', 'RussianFederation', 'Petroleum']         0    BEI   \n",
       "5052  ['GDPR', 'DataSecurity', 'Environment', 'Priva...         0    DTE   \n",
       "\n",
       "                                                  title  \n",
       "770   Philippine Airlines Plans Long-Haul Route Shakeup  \n",
       "357   Beyoncé﻿ Wears a Skintight, PVC Bodysuit from ...  \n",
       "3781  Caleb Polaha talks about his new song 'The Who...  \n",
       "2903  Coatings Adhesives Sealants and Elastomers Mar...  \n",
       "5052  EU wants USB-C to become standard charging por...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the date with some samples\n",
    "general_cleaned_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to general_cleaned_data.csv\n",
      "Loaded DataFrame from general_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint file\n",
    "general_cleaned_data = csv_checkpoint(general_cleaned_data, 'general_cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning & Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"content\" column, containing the text of the reports, undergoes a series of cleaning, normalization, and preprocessing steps to ensure accurate and efficient analysis. These steps include:\n",
    "\n",
    "- **String conversion**: Converting the input to a string format ensures consistency and compatibility during subsequent processing tasks.\n",
    "- **Lowercase conversion**: Transforming all text to lowercase serves as a simple normalization step, reducing the complexity and variability of the input data.\n",
    "- **Unicode decoding**: Removing diacritics (e.g., accented characters) and normalizing the text encoding mitigates potential discrepancies arising from different encoding formats.\n",
    "- **URL and email address removal**: Eliminating URLs and email addresses reduces noise in the dataset, as these elements do not contribute valuable information for the analysis.\n",
    "- **Extra whitespace removal**: Eradicating extra whitespaces improves text analysis and tokenization by ensuring that only meaningful spaces are retained.\n",
    "- **Contact detail removal**: Excluding phone numbers, contact person strings, and social media references further minimizes noise in the dataset, honing the focus on relevant text.\n",
    "- **Table of contents removal**: Discarding the table of contents enhances the data quality by eliminating repetitive and non-essential information.\n",
    "- **Named entity removal**: Employing the spaCy model to remove human names and other named entities optimizes the text for analysis and modeling by concentrating on pertinent content.\n",
    "- **Abbreviation expansion**: Utilizing the contractions library and custom functions with regular expressions, common and uncommon abbreviations are expanded to improve text interpretation.\n",
    "- **Special character elimination**: Excluding all special characters, except punctuation, refines the input data. Retaining punctuation is necessary for accurate sentence tokenization and removed after sentence tokenization..\n",
    "- **Tokenization and lemmatization**: Tokenizing words and sentences, and subsequently lemmatizing words using the WordNetLemmatizer from nltk, streamlines the text and reduces morphological variations.\n",
    "- **Stopword removal**: Customizing the nltk stopwords list by adding or removing specific stopwords enables more precise and tailored text analysis.\n",
    "- **Part-of-speech (POS) tagging**: Assigning POS tags to words and sentences enhances the text representation by providing additional linguistic information, which may be beneficial for subsequent analysis and modeling tasks.  \n",
    "\n",
    "Spellchecking was tested with TextBlob and PySpellChecker but deliverd not useful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = general_cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the spacy model shows better results on the \"raw\" text, the named entity removal is conducted before all normalization and cleaning steps\n",
    "spacy_model = spacy.load('en_core_web_md')\n",
    "spacy_model.max_length = 1800000 # Increase max text length\n",
    "\n",
    "def remove_named_entities(text):\n",
    "    doc = spacy_model(text)\n",
    "    \n",
    "    named_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\"]:\n",
    "            named_entities.add(ent.text)\n",
    "    \n",
    "    named_entities_count = len(named_entities)\n",
    "    \n",
    "    for named_entity in named_entities:\n",
    "        text = text.replace(named_entity, '')\n",
    "    \n",
    "    return text, named_entities_count\n",
    "\n",
    "# Assuming cleaned_data is a pandas DataFrame with a 'content' column\n",
    "cleaned_data['cleaned_content'], name_entity_count = zip(*cleaned_data['content'].apply(remove_named_entities))\n",
    "print(\"Name entities removed:\", sum(name_entity_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    urls = re.findall(r'http\\S+|www\\S+|https\\S+', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE), len(urls)\n",
    "\n",
    "def remove_emails(text):\n",
    "    mail_addresses = re.findall(r'\\S+@\\S+\\s?', text, flags=re.MULTILINE)\n",
    "    return re.sub(r'\\S+@\\S+\\s?', '', text, flags=re.MULTILINE), len(mail_addresses)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    extra_spaces = re.findall(r'\\s{2,}', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip(), len(extra_spaces)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].astype(str) # Convert all texts to string\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: x.lower()) # Convert all texts to lower-case\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(lambda x: unidecode(x, errors=\"preserve\")) # Remove diacritics / accented characters and unicode normalization\n",
    "cleaned_data['cleaned_content'], url_count = zip(*cleaned_data['cleaned_content'].apply(remove_urls)) # Remove URLs from texts\n",
    "cleaned_data['cleaned_content'], email_count = zip(*cleaned_data['cleaned_content'].apply(remove_emails)) # Remove e-mail addresses from texts\n",
    "cleaned_data['cleaned_content'], extra_space_count = zip(*cleaned_data['cleaned_content'].apply(remove_extra_whitespace)) # Remove extra whitespaces from texts\n",
    "\n",
    "print(\"URLs removed:\", sum(url_count))\n",
    "print(\"Mail addresses removed:\", sum(email_count))\n",
    "print(\"Extra whitespaces removed:\", sum(extra_space_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contact_details(text):\n",
    "    # Remove phone numbers\n",
    "    phone_regex = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
    "    phone_count = len(re.findall(phone_regex, text))\n",
    "    text = re.sub(phone_regex, '', text)\n",
    "\n",
    "    # Remove common contact-related phrases\n",
    "    contact_phrases_regex = r'\\b(?:Contact Person|Phone|Tel|Fax|Mobile|E?mail|Skype|Twitter|Facebook|LinkedIn|Website):\\b'\n",
    "    contact_phrases_count = len(re.findall(contact_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(contact_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    total_count = phone_count + contact_phrases_count\n",
    "    return text, total_count\n",
    "\n",
    "def remove_table_of_contents(text):\n",
    "    # Remove common table of contents phrases\n",
    "    toc_phrases_regex = r'\\b(?:Table of Contents|Contents)\\b'\n",
    "    toc_phrases_count = len(re.findall(toc_phrases_regex, text, flags=re.IGNORECASE))\n",
    "    text = re.sub(toc_phrases_regex, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove content with numbering like \"1. Introduction\", \"1.1. Background\", \"A. Overview\", etc.\n",
    "    toc_entries_regex = r'(^|\\n)\\s*\\w+(\\.\\w+)*\\s+\\w+([\\w\\s]+)?'\n",
    "    toc_entries_count = len(re.findall(toc_entries_regex, text))\n",
    "    text = re.sub(toc_entries_regex, '', text)\n",
    "\n",
    "    total_count = toc_phrases_count + toc_entries_count\n",
    "    return text, total_count\n",
    "\n",
    "cleaned_data['cleaned_content'], contact_count = zip(*cleaned_data['cleaned_content'].apply(remove_contact_details))\n",
    "cleaned_data['cleaned_content'], toc_count = zip(*cleaned_data['cleaned_content'].apply(remove_table_of_contents))\n",
    "print(\"Contact information removed:\", sum(contact_count))\n",
    "print(\"TOCs removed:\", sum(toc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = []\n",
    "    for word in text.split():\n",
    "        expanded_text.append(contractions.fix(word))\n",
    "    expanded_text = ' '.join(expanded_text)\n",
    "    return contractions.fix(expanded_text)\n",
    "\n",
    "cleaned_data['cleaned_content'] = cleaned_data['cleaned_content'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand custom abbreviations which are not captured by \"contractions\"\n",
    "# Basic idea from: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# Compile the regular expressions only once for efficiency\n",
    "specific_patterns = [\n",
    "    (re.compile(r\"won['’]t\"), \"will not\"),\n",
    "    (re.compile(r\"can['’]t\"), \"can not\"),\n",
    "]\n",
    "\n",
    "def decontracted(phrase):\n",
    "    count = 0\n",
    "\n",
    "    # Replace specific patterns\n",
    "    for pattern, replacement in specific_patterns:\n",
    "        matches = len(pattern.findall(phrase))\n",
    "        count += matches\n",
    "        phrase = pattern.sub(replacement, phrase)\n",
    "\n",
    "    return phrase, count\n",
    "\n",
    "# Apply the function to expand abbreviations\n",
    "cleaned_data['cleaned_content'], abbreviation_counts = zip(*cleaned_data['cleaned_content'].apply(decontracted))\n",
    "print(\"Expanded custom abbreviations:\", sum(abbreviation_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters excl. punctuation since this is needed by the sentence tokenization\n",
    "def remove_non_alphanumeric(text):\n",
    "    special_chars = re.findall(r'[^a-zA-Z0-9\\s.,!?\\'\"]', text)\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"]', ' ', text), len(special_chars)\n",
    "\n",
    "cleaned_data['cleaned_content'], special_char_count = zip(*cleaned_data['cleaned_content'].apply(remove_non_alphanumeric))\n",
    "print(\"Special characters excl. punctuation removed:\", sum(special_char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    # Remove numbers, digits, and punctuation\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['word_tokens'], word_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_words))\n",
    "print(\"Generated word token amount:\", sum(word_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    # Tokenize sentences\n",
    "    tokens = sent_tokenize(text)\n",
    "    \n",
    "    return tokens, len(tokens)\n",
    "\n",
    "cleaned_data['sentence_tokens'], sentence_token_count = zip(*cleaned_data['cleaned_content'].apply(tokenize_sentences))\n",
    "print(\"Generated sentence token amount:\", sum(sentence_token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_word_tokens(tokens, custom_stopwords):\n",
    "\n",
    "    # Remove stopwords from tokens\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in custom_stopwords]\n",
    "    \n",
    "    return filtered_tokens, len(tokens) - len(filtered_tokens)\n",
    "\n",
    "def remove_stopwords_from_sentence_tokens(sentences_list, custom_stopwords):\n",
    "    filtered_sentences_list = []\n",
    "    total_removed_items_count = 0\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        # Tokenize the sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords, digits, numbers, dates, and punctuation from word tokens\n",
    "        filtered_word_tokens = [\n",
    "            token for token in word_tokens\n",
    "            if token.lower() not in custom_stopwords\n",
    "            and not re.search(r'\\d', token)  # Remove tokens containing digits\n",
    "            ]\n",
    "\n",
    "        # Remove remaining special characters from sentences, i.e. punctuation\n",
    "        filtered_word_tokens = [\n",
    "            re.sub(rf\"[{re.escape(string.punctuation)}]\", '', token) for token in filtered_word_tokens\n",
    "            ]\n",
    "\n",
    "        # Reconstruct the sentence without the removed words and special characters\n",
    "        filtered_sentence = ' '.join(filtered_word_tokens)\n",
    "        removed_items_count = len(word_tokens) - len(filtered_word_tokens)\n",
    "\n",
    "        filtered_sentences_list.append(filtered_sentence)\n",
    "        total_removed_items_count += removed_items_count\n",
    "\n",
    "    return filtered_sentences_list, total_removed_items_count\n",
    "\n",
    "# Define custom stopwords to add or remove\n",
    "custom_stopwords = {\n",
    "    'add': ['said','company','companies','year','billion','million','siemens','linde','rwe','volkswagen','symrise','porsche','sap','adidas','puma','airbus','bmw','hannover','mtu','heiderbergcement','qiagen','benz','continental','bayer','fresenius'],\n",
    "    'remove': ['stopword_to_remove1', 'stopword_to_remove2']\n",
    "}\n",
    "\n",
    "# Combine stopwords to filter the content of the reports\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "all_stopwords |= set(custom_stopwords['add'])\n",
    "all_stopwords -= set(custom_stopwords['remove'])\n",
    "\n",
    "cleaned_data['word_tokens'], stopword_count_words = zip(*cleaned_data['word_tokens'].apply(remove_stopwords_from_word_tokens, custom_stopwords=all_stopwords))\n",
    "cleaned_data['sentence_tokens'], stopword_count_sentences = zip(*cleaned_data['sentence_tokens'].apply(remove_stopwords_from_sentence_tokens, custom_stopwords=all_stopwords))\n",
    "\n",
    "print(\"Removed stopwords in word tokens\", sum(stopword_count_words))\n",
    "print(\"Removed stopwords in sentence tokens\", sum(stopword_count_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_tokens(word_tokens, sentence_list):\n",
    "    # POS tagging for word tokens\n",
    "    pos_tagged_word_tokens = nltk.pos_tag(word_tokens)\n",
    "\n",
    "    # Create a dictionary to map word tokens to their POS tags, this reduces the effort to call nltk.pos_tag twice\n",
    "    pos_tags_dict = dict(pos_tagged_word_tokens)\n",
    "\n",
    "    # POS tagging for sentence tokens\n",
    "    pos_tagged_sentence_list = []\n",
    "    for sentence in sentence_list:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = [(token, pos_tags_dict[token]) for token in tokenized_sentence if token in pos_tags_dict]\n",
    "        pos_tagged_sentence_list.append(pos_tagged_sentence)\n",
    "\n",
    "    return pos_tagged_word_tokens, pos_tagged_sentence_list\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = cleaned_data.apply(lambda row: pos_tagging_tokens(row['word_tokens'], row['sentence_tokens']), axis=1)\n",
    "cleaned_data['pos_tagged_word_tokens'], cleaned_data['pos_tagged_sentence_tokens'] = zip(*pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint file\n",
    "cleaned_data = csv_checkpoint(cleaned_data, 'cleaned_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several additional information could be helpful in the further analysis, which are not included in the dataset. Therefore a small scraper is used to enrich the the dataset with the sector, industry and market capitalization of the DAX companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://disfold.com/stock-index/dax/companies/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "scraped_data = []\n",
    "for row in table.find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    scraped_data.append(cols)\n",
    "\n",
    "def clean_scraped_data(data):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in data:\n",
    "        # Remove empty rows\n",
    "        if len(row) > 0:\n",
    "            # Remove the '$' and ',' signs from the market cap and convert it to float\n",
    "            market_cap = float(row[3].replace('$', '').replace(',', '').replace('B', ''))\n",
    "            cleaned_data.append([row[1], row[2], market_cap, row[4], row[5], row[6]])\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_data, columns=['company_name', 'symbol', 'market_cap_in_usd_b', 'country', 'sector', 'industry'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "company_enrichments = clean_scraped_data(scraped_data)\n",
    "company_enrichments.to_csv('../data/dax_company_sectors.csv', index=False)\n",
    "company_enrichments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the ticker symbols to prevent NaN and ensure correct join conditions\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('SRT3', 'SRT')\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].replace('HEN3', 'HNK')\n",
    "company_enrichments.loc[company_enrichments['company_name'] == 'Mercedes-Benz Group AG', 'symbol'] = 'DAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['symbol'] = cleaned_data['symbol'].astype(pd.StringDtype())\n",
    "company_enrichments['symbol'] = company_enrichments['symbol'].astype(pd.StringDtype())\n",
    "\n",
    "# Merge the cleaned data with the enrichment\n",
    "enriched_cleaned_data = pd.merge(cleaned_data, company_enrichments, how='left', on='symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_cleaned_data[enriched_cleaned_data['industry'].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hannover R AG cannot be matched, since it is not present in the scraped data. Since there are only 2 records this is negligible and will be fixed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R AG', 'sector'] = 'Financials'\n",
    "enriched_cleaned_data.loc[enriched_cleaned_data['company'] == 'Hannover R AG', 'industry'] = 'Insurance—Reinsurance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns/data\n",
    "enriched_cleaned_data = enriched_cleaned_data.drop(columns=['content', 'company_name', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final dataframe\n",
    "enriched_cleaned_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the processing, some rows (2) have no content anymore. These are dropped.\n",
    "enriched_cleaned_data[enriched_cleaned_data['cleaned_content'].isna()]\n",
    "general_cleaned_data = general_cleaned_data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint file for further analysis\n",
    "enriched_cleaned_data = csv_checkpoint(enriched_cleaned_data, 'enriched_cleaned_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
